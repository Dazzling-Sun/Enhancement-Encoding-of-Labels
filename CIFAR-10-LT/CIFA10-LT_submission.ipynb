{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6eHm7W0Kc4Fu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 09:34:34.684471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "# from tensorflow.python.keras.engine import data_adapter\n",
    "# from tensorflow.python.eager import backprop\n",
    "from functools import partial\n",
    "# from tensorflow.python.keras.engine.training import _minimize\n",
    "# import tensorflow_addons as tfa\n",
    "import math\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Uz7ssaYixpuN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 09:34:36.762676: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-12 09:34:36.763637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-05-12 09:34:36.859885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:48:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\n",
      "2022-05-12 09:34:36.859927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-12 09:34:36.862099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-12 09:34:36.862173: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-12 09:34:36.864120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-12 09:34:36.864434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-12 09:34:36.866321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-12 09:34:36.867358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-12 09:34:36.871455: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-12 09:34:36.881455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lup2j6RUelce"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 09:34:36.894309: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-12 09:34:36.905051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:48:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\n",
      "2022-05-12 09:34:36.905101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-12 09:34:36.905146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-12 09:34:36.905157: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-12 09:34:36.905167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-12 09:34:36.905177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-12 09:34:36.905187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-12 09:34:36.905197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-12 09:34:36.905208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-12 09:34:36.911174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-12 09:34:36.911219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-12 09:34:37.588669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-12 09:34:37.588699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-05-12 09:34:37.588705: I tensorflow/core/common_r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function decode at 0x7f1d6f0f9ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function decode at 0x7f1d6f0f9ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "untime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-05-12 09:34:37.600515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30130 MB memory) -> physical GPU (device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:48:00.0, compute capability: 7.0)\n",
      "2022-05-12 09:34:37.600844: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-12 09:34:37.649978: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-05-12 09:34:37.650414: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2095170000 Hz\n"
     ]
    }
   ],
   "source": [
    "n_class = 10\n",
    "train_set_size = 12406\n",
    "test_set_size = 10000\n",
    "valid_size_per_class = 5\n",
    "\n",
    "valid_set_size = valid_size_per_class * n_class\n",
    "\n",
    "\n",
    "\n",
    "def decode(instance):\n",
    "    feature_spec = {\n",
    "    'image/encoded': tf.io.FixedLenFeature((), tf.string),\n",
    "    'image/class/label': tf.io.FixedLenFeature((), tf.int64)\n",
    "    }\n",
    "    instance = tf.io.parse_example(instance, feature_spec)\n",
    "    image = tf.io.decode_raw(instance[\"image/encoded\"], tf.uint8)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [32, 32, 3])\n",
    "    label = instance[\"image/class/label\"]\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "traing_dateset = tf.data.TFRecordDataset(\"./data/cifar10-lt_train.tfrecord\").map(decode)\n",
    "\n",
    "train_x, train_y = np.zeros([train_set_size, 32, 32, 3]), np.zeros([train_set_size])\n",
    "for i,instance in enumerate(traing_dateset.as_numpy_iterator()):\n",
    "    train_x[i] = instance[0]\n",
    "    train_y[i] = instance[1]\n",
    "\n",
    "valid_x, valid_y = np.zeros([valid_set_size,32,32,3]), np.zeros([valid_set_size])\n",
    "\n",
    "train_set_size_no_valid = train_set_size - valid_set_size\n",
    "train_x_no_valid = train_x\n",
    "train_y_no_valid = train_y\n",
    "\n",
    "train_size_no_valid_per_class = np.zeros([n_class])\n",
    "\n",
    "for i in range(n_class):\n",
    "    indices_of_instances = np.where(train_y_no_valid == i)[0]\n",
    "    train_size_no_valid_per_class[i] = len(indices_of_instances) - valid_size_per_class\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(indices_of_instances)\n",
    "    \n",
    "    indices_of_valid_instances = indices_of_instances[ : valid_size_per_class]\n",
    "    valid_x[i * valid_size_per_class : (i+1) * valid_size_per_class] = \\\n",
    "        train_x_no_valid[indices_of_valid_instances]\n",
    "    \n",
    "    valid_y[i * valid_size_per_class : (i+1) * valid_size_per_class] = \\\n",
    "        train_y_no_valid[indices_of_valid_instances]\n",
    "    \n",
    "    train_x_no_valid = np.delete(train_x_no_valid, indices_of_valid_instances, 0)\n",
    "    train_y_no_valid = np.delete(train_y_no_valid, indices_of_valid_instances, 0)\n",
    "\n",
    "    \n",
    "test_dateset = tf.data.TFRecordDataset(\"./data/cifar10_test.tfrecord\").map(decode)\n",
    "\n",
    "test_x, test_y = np.zeros([test_set_size, 32, 32, 3]), np.zeros([test_set_size])\n",
    "for i,instance in enumerate(test_dateset.as_numpy_iterator()):\n",
    "    test_x[i] = instance[0]\n",
    "    test_y[i] = instance[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 303661208\n",
      "test set size: 245760152\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "print('training set size:',train_x_no_valid.__sizeof__())\n",
    "print('test set size:',test_x.__sizeof__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q5nA5eZEfHXn"
   },
   "outputs": [],
   "source": [
    "normalize_x = lambda x: x/255 - 0.5\n",
    "\n",
    "train_x_no_valid = tf.constant(normalize_x(train_x_no_valid))\n",
    "test_x = tf.constant(normalize_x(test_x))\n",
    "valid_x = tf.constant(normalize_x(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qqabQ0IpfIlM"
   },
   "outputs": [],
   "source": [
    "train_y_no_valid = tf.constant(keras.utils.to_categorical(train_y_no_valid, n_class))\n",
    "test_y = tf.constant(keras.utils.to_categorical(test_y, n_class))\n",
    "valid_y = tf.constant(keras.utils.to_categorical(valid_y, n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(train_size_no_valid_per_class)), train_size_no_valid_per_class)\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.xlabel('Sorted Index of Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 4\n",
    "\n",
    "# @trackable.no_automatic_dependency_tracking \n",
    "def train_generator(epochs, batch_size, encoding_matrix):\n",
    "    def preprocess_data(x, y):\n",
    "        x = tf.image.resize_with_crop_or_pad(x, 32 + padding, 32 + padding)\n",
    "        x = tf.image.random_crop(x, [32, 32, 3])\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        y = tf.matmul(train_y_no_valid, encoding_matrix)\n",
    "    \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_x_no_valid, y)\n",
    "        ).map(\n",
    "            preprocess_data\n",
    "        ).shuffle(\n",
    "            train_set_size, epoch\n",
    "        ).batch(\n",
    "            batch_size\n",
    "        ).prefetch(\n",
    "            tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "        for batch in train_dataset:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal loss \n",
    "def focal(gamma=2.):\n",
    "    def focal_loss (y_true, y_pred):\n",
    "        ep = 1e-7\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = y_pred + (1. - 2.*y_pred) * ep\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        return tf.reduce_sum(-y_true*((1-y_pred)**gamma)*tf.math.log(y_pred), axis=1)\n",
    "    \n",
    "    return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_key = 'ce'\n",
    "focal_gamma = 1.\n",
    "loss_fn_dic = {'focal':focal(focal_gamma), \n",
    "               'ce':keras.losses.categorical_crossentropy,\n",
    "               'mse':keras.losses.mean_squared_error}\n",
    "loss_fn = loss_fn_dic[loss_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NizAKxPTxpuQ"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9CiBEzRDxpuQ"
   },
   "outputs": [],
   "source": [
    "evaluation_batch_size = 5000\n",
    "\n",
    "class UpdateEncodingMatrixAndGetMetrics_EEC (keras.callbacks.Callback):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def __init__(self, \n",
    "                 beginEncodingEpoch,endEncodingEpoch, \n",
    "                 codingEnhancementRate, mu, \n",
    "                 trainSet_x, trainSet_y, \n",
    "                 validSet_x, validSet_y,\n",
    "                 trainMetrics, validMetrics,\n",
    "                 keyWord):\n",
    "        super().__init__()\n",
    "        self.beginEncodingEpoch = beginEncodingEpoch\n",
    "        self.endEncodingEpoch = endEncodingEpoch\n",
    "        self.codingEnhancementRate = tf.constant(codingEnhancementRate)\n",
    "        self.trainSet_x = trainSet_x\n",
    "        self.trainSet_y = trainSet_y\n",
    "        self.validSet_x = validSet_x\n",
    "        self.validSet_y = validSet_y\n",
    "        self.trainMetrics = trainMetrics\n",
    "        self.validMetrics = validMetrics\n",
    "        self.mu = mu\n",
    "        self.keyWord = keyWord\n",
    "\n",
    "    \n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _confusion_matrix(self, labels, preds):\n",
    "        pred_classes = tf.one_hot(tf.argmax(preds, axis=1), 10)\n",
    "        cm=tf.matmul(labels, pred_classes, transpose_a=True)\n",
    "        cm=cm/tf.reduce_sum(cm, axis=1, keepdims=True)\n",
    "        return cm\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _soft_confusion_matrix(self, labels, preds):\n",
    "        scm=tf.matmul(labels, preds, transpose_a=True)\n",
    "        scm=scm/tf.reduce_sum(scm, axis=1, keepdims=True)\n",
    "        return scm\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def _make_encoding_matrix(self, confusionMatrix):\n",
    "        coSenMatrix =  confusionMatrix - tf.eye(10)\n",
    "        encodingMatrix = - self.codingEnhancementRate \\\n",
    "                            * coSenMatrix \\\n",
    "                            + tf.eye(10)\n",
    "        \n",
    "        return encodingMatrix\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def on_train_begin(self, logs):\n",
    "        if self.endEncodingEpoch <= 0 or self.beginEncodingEpoch > 0:\n",
    "            encodingMatrix = tf.eye(10)\n",
    "        else:\n",
    "            y_h = tf.constant(self.model.predict(self.validSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "            y = self.validSet_y\n",
    "            scm = self._soft_confusion_matrix(y, y_h)\n",
    "            encodingMatrix = ((1-self.mu) * encoding_matrix +\n",
    "                              self.mu * self._make_encoding_matrix(scm))\n",
    "        K.set_value(encoding_matrix, encodingMatrix)\n",
    "\n",
    "   \n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        print('\\n'+self.keyWord)\n",
    "        \n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        y_h = tf.constant(self.model.predict(self.trainSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.trainSet_y\n",
    "        self.trainMetrics[0][epoch] = self._soft_confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[1][epoch] = self._confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[2][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.trainMetrics[3][epoch] = tf.reduce_mean(keras.metrics.categorical_accuracy(y, y_h)).numpy()\n",
    "        self.trainMetrics[4][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "\n",
    "        print('\\nTest on train set:',\n",
    "              'loss=', self.trainMetrics[2][epoch],\n",
    "              'acc=', self.trainMetrics[3][epoch],\n",
    "              'auc=', self.trainMetrics[4][epoch] )\n",
    "\n",
    "        y_h = tf.constant(self.model.predict(self.validSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.validSet_y\n",
    "        scm = self._soft_confusion_matrix(y, y_h)\n",
    "        self.validMetrics[0][epoch] = scm.numpy()\n",
    "        self.validMetrics[1][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.validMetrics[2][epoch] = scm.numpy().diagonal().mean()\n",
    "        self.validMetrics[3][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "\n",
    "        print('Test on valid set:',\n",
    "            'loss=', self.validMetrics[1][epoch],\n",
    "            'acc=', self.validMetrics[2][epoch],\n",
    "            'auc=', self.validMetrics[3][epoch] )\n",
    "\n",
    "        if epoch+1 < self.endEncodingEpoch and epoch+1>=self.beginEncodingEpoch:\n",
    "            new_encoding_matrix = (1-self.mu) * encoding_matrix + \\\n",
    "                              self.mu * self._make_encoding_matrix(scm)\n",
    "            K.set_value(encoding_matrix, new_encoding_matrix)\n",
    "\n",
    "        elif epoch+1 == self.endEncodingEpoch:\n",
    "            K.set_value(encoding_matrix, tf.eye(10))\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_train_end(self, logs):\n",
    "        K.set_value(encoding_matrix, tf.eye(10))\n",
    "        print('*'*100)\n",
    "        print('*'*100)\n",
    "        print('*'*100)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "class UpdateEncodingMatrixAndGetMetrics_Base (UpdateEncodingMatrixAndGetMetrics_EEC):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def __init__(self, \n",
    "                 trainSet_x, trainSet_y, \n",
    "                 validSet_x, validSet_y,\n",
    "                 trainMetrics, validMetrics,\n",
    "                 keyWord):\n",
    "        super(UpdateEncodingMatrixAndGetMetrics_EEC, self).__init__()\n",
    "        self.trainSet_x = trainSet_x\n",
    "        self.trainSet_y = trainSet_y\n",
    "        self.validSet_x = validSet_x\n",
    "        self.validSet_y = validSet_y\n",
    "        self.trainMetrics = trainMetrics\n",
    "        self.validMetrics = validMetrics\n",
    "        self.keyWord = keyWord\n",
    "\n",
    "   \n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def _make_encoding_matrix(self):\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def on_train_begin(self, logs):\n",
    "        super(UpdateEncodingMatrixAndGetMetrics_EEC, self).on_train_begin(logs)\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        y_h = tf.constant(self.model.predict(self.trainSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.trainSet_y\n",
    "        self.trainMetrics[0][epoch] = self._soft_confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[1][epoch] = self._confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[2][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.trainMetrics[3][epoch] = tf.reduce_mean(keras.metrics.categorical_accuracy(y, y_h)).numpy()\n",
    "        self.trainMetrics[4][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "\n",
    "        print('\\nTest on train set:',\n",
    "            'loss=', self.trainMetrics[2][epoch],\n",
    "            'acc=', self.trainMetrics[3][epoch],\n",
    "            'auc=', self.trainMetrics[4][epoch] )\n",
    "\n",
    "        y_h = tf.constant(self.model.predict(self.validSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.validSet_y\n",
    "        scm = self._soft_confusion_matrix(y, y_h)\n",
    "        self.validMetrics[0][epoch] = scm.numpy()\n",
    "        self.validMetrics[1][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.validMetrics[2][epoch] = scm.numpy().diagonal().mean()\n",
    "        self.validMetrics[3][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "        \n",
    "        print('Test on valid set:',\n",
    "            'loss=', self.validMetrics[1][epoch],\n",
    "            'acc=', self.validMetrics[2][epoch],\n",
    "            'auc=', self.validMetrics[3][epoch] )\n",
    "\n",
    "        print()\n",
    "\n",
    "        \n",
    "        \n",
    "#################################################################################################################        \n",
    "class UpdateEncodingMatrixAndGetMetrics_Reweight (UpdateEncodingMatrixAndGetMetrics_EEC):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _make_encoding_matrix(self, confusionMatrix):\n",
    "        CoSenMatrix =  confusionMatrix - tf.eye(10)\n",
    "        encodingMatrix = - self.codingEnhancementRate \\\n",
    "                            * CoSenMatrix \\\n",
    "                            + tf.eye(10)\n",
    "        encodingMatrix = encodingMatrix * tf.eye(10)\n",
    "        return encodingMatrix        \n",
    "\n",
    "#################################################################################################################        \n",
    "class UpdateEncodingMatrixAndGetMetrics_CoSen (UpdateEncodingMatrixAndGetMetrics_EEC):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _make_encoding_matrix(self, confusionMatrix):\n",
    "        CoSenMatrix =  (1 - tf.eye(10))*(confusionMatrix - tf.eye(10))\n",
    "        encodingMatrix = - self.codingEnhancementRate \\\n",
    "                            * CoSenMatrix \\\n",
    "                            + tf.eye(10)\n",
    "        return encodingMatrix     \n",
    "    \n",
    "##################################################################################################################\n",
    "class AdjustLR (keras.callbacks.Callback):\n",
    "    def __init__(self, schedule, base_learning_rate = None):\n",
    "        super().__init__()\n",
    "        self._schedule = schedule\n",
    "        self._base_learning_rate = base_learning_rate\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        warmup_lr_multiplier, warmup_end_epoch = self._schedule[0]\n",
    "        learning_rate = (\n",
    "            self._base_learning_rate * warmup_lr_multiplier * (epoch+1) /\n",
    "            (warmup_end_epoch+1))\n",
    "        for mult, start_epoch in self._schedule:\n",
    "            learning_rate = tf.where(epoch >= start_epoch,\n",
    "                                   self._base_learning_rate * mult, learning_rate)\n",
    "\n",
    "        K.set_value(self.model.optimizer.learning_rate, learning_rate)\n",
    "        print(self.model.optimizer.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu-nz9Bm3695"
   },
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d (ZeroPadding2 (None, 38, 38, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 36, 36, 16)        432       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 36, 36, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 36, 36, 16)        0         \n",
      "_________________________________________________________________\n",
      "residual_unit (ResidualUnit) (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_1 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_2 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_3 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_4 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_5 (ResidualUni (None, 18, 18, 32)        11776     \n",
      "_________________________________________________________________\n",
      "residual_unit_6 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_7 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_8 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_9 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_10 (ResidualUn (None, 9, 9, 64)          46080     \n",
      "_________________________________________________________________\n",
      "residual_unit_11 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "residual_unit_12 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "residual_unit_13 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "residual_unit_14 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 304,226\n",
      "Trainable params: 300,494\n",
      "Non-trainable params: 3,732\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "he_normal = keras.initializers.HeUniform()\n",
    "glorot_normal = keras.initializers.GlorotNormal()\n",
    "he_uniform = keras.initializers.HeUniform()\n",
    "glorot_uniform = keras.initializers.GlorotUniform()\n",
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"SAME\", use_bias=False, \n",
    "                        kernel_initializer=he_normal)\n",
    "DefaultBN = partial(keras.layers.BatchNormalization, \n",
    "                      momentum=0.9,\n",
    "                      epsilon=1e-5)\n",
    "\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters,kernel_size=1,strides=strides),\n",
    "            DefaultBN(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            DefaultBN(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters,kernel_size=1),\n",
    "            DefaultBN()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                DefaultBN()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=(32, 32, 3)))\n",
    "model.add(keras.layers.ZeroPadding2D(padding=(3, 3)))\n",
    "model.add(DefaultConv2D(16,padding='valid'))\n",
    "model.add(DefaultBN())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "prev_filters = 16\n",
    "for filters in [16] * 5 + [32] * 5 + [64] * 5:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model.add(DefaultBN())\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, kernel_initializer=keras.initializers.RandomNormal(stddev=0.01)))\n",
    "model.add(DefaultBN())\n",
    "model.add(keras.layers.Softmax())\n",
    "model.summary()\n",
    "initialWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = lambda learning_rate=0.01:keras.optimizers.SGD(learning_rate = learning_rate, \n",
    "                                                          momentum=0.9,  \n",
    "                                                          nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 09:34:47.325507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-12 09:34:47.585567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 7s 26ms/step - loss: 2.4380\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = Optimizer()\n",
    "model.compile(optimizer=optimizer, loss = loss_fn)\n",
    "model.set_weights(initialWeights)\n",
    "        \n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.prev_loss = 0\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(batch_loss)\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)        \n",
    "        \n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=100):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), 5.0])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    \n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, train_x_no_valid, train_y_no_valid, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-P5P0ZLDxpuW",
    "outputId": "168357d3-92d5-4f6f-eaba-92ef84ecea7c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lrDic = {'focal':0.5,\n",
    "         'ce':0.1,\n",
    "         'mse':5.0\n",
    "        }\n",
    "lr = lrDic[loss_key]\n",
    "codingEnhancementRateDic = {'focal':0.5,\n",
    "                            'ce':1.0,\n",
    "                            'mse':0.5\n",
    "                           }\n",
    "codingEnhancementRate = codingEnhancementRateDic[loss_key]\n",
    "\n",
    "lr_schedule_dic = {'focal':[(1.0, 5), (0.1, 200), (0.01, 300)],\n",
    "                   'ce':[(1.0, 5), (0.1, 200), (0.01, 250)],\n",
    "                   'mse':[(1.0, 5), (0.1, 200), (0.01, 300)]}\n",
    "\n",
    "lr_schedule = lr_schedule_dic[loss_key]\n",
    "\n",
    "mu = 0.1 if loss_key == \"mse\" else 0.5  #update rate\n",
    "\n",
    "epochs_dic = {'focal':400,\n",
    "          'ce':300,\n",
    "          'mse':400} \n",
    "epochs = epochs_dic[loss_key]\n",
    "endEncodingEpoch = epochs\n",
    "beginEncodingEpoch = 0\n",
    "batch_size = 128\n",
    "steps = math.ceil(train_set_size_no_valid / batch_size) \n",
    "print(steps)\n",
    "\n",
    "if 'softConfusionMatrix_train' not in locals() or len(softConfusionMatrix_train[0]) != epochs:\n",
    "    softConfusionMatrix_train = np.zeros((4, epochs, n_class, n_class))\n",
    "    confusionMatrix_train = np.zeros((4, epochs, n_class, n_class))\n",
    "    loss_train=np.zeros((4,epochs))\n",
    "    acc_train=np.zeros((4,epochs))\n",
    "    auc_train=np.zeros((4,epochs))\n",
    "    \n",
    "    softConfusionMatrix_valid = np.zeros((4, epochs, n_class, n_class))\n",
    "    loss_valid=np.zeros((4,epochs))\n",
    "    acc_valid=np.zeros((4,epochs))\n",
    "    auc_valid=np.zeros((4,epochs))\n",
    "\n",
    "model_weights = [] \n",
    "\n",
    "\n",
    "# Pretrain-------------------------------------------------------------------------\n",
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(initialWeights)\n",
    "optimizer = Optimizer(lr)\n",
    "model.compile(optimizer=optimizer, loss = loss_fn)\n",
    "\n",
    "metric_idx = 0\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_Base(train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Pretrain')\n",
    "                        \n",
    "_ = model.fit(trainGen, epochs = beginEncodingEpoch, verbose = 2,\n",
    "                steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "\n",
    "pretrainedWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 5s - loss: 1.5992\n",
      "\n",
      "Test on train set: loss= 1.3125081062316895 acc= 0.5598899126052856 auc= 0.7653441615563052\n",
      "Test on valid set: loss= 2.571068048477173 acc= 0.17567987740039825 auc= 0.6906666666666668\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 2s - loss: 1.1320\n",
      "\n",
      "Test on train set: loss= 1.0398727655410767 acc= 0.6413078904151917 auc= 0.8458192785546592\n",
      "Test on valid set: loss= 2.6005914211273193 acc= 0.1961887776851654 auc= 0.7466666666666667\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 3s - loss: 1.0027\n",
      "\n",
      "Test on train set: loss= 0.9316748380661011 acc= 0.6816121935844421 auc= 0.8620920689513417\n",
      "Test on valid set: loss= 2.4466681480407715 acc= 0.2507422864437103 auc= 0.7897777777777778\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 3s - loss: 0.9205\n",
      "\n",
      "Test on train set: loss= 0.9130818843841553 acc= 0.688086748123169 auc= 0.8798869306237392\n",
      "Test on valid set: loss= 2.499021530151367 acc= 0.23089583218097687 auc= 0.7817777777777778\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 3s - loss: 0.8835\n",
      "\n",
      "Test on train set: loss= 0.8672336339950562 acc= 0.7056490778923035 auc= 0.8699773289841826\n",
      "Test on valid set: loss= 2.5633325576782227 acc= 0.2747177481651306 auc= 0.8035555555555556\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.8304\n",
      "\n",
      "Test on train set: loss= 0.9991324543952942 acc= 0.6900291442871094 auc= 0.8808156999676628\n",
      "Test on valid set: loss= 2.6375226974487305 acc= 0.2754778265953064 auc= 0.8151111111111111\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.7863\n",
      "\n",
      "Test on train set: loss= 0.7528521418571472 acc= 0.7448203563690186 auc= 0.9167385827651335\n",
      "Test on valid set: loss= 2.1296262741088867 acc= 0.31574928760528564 auc= 0.8724444444444444\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.7419\n",
      "\n",
      "Test on train set: loss= 0.7882838249206543 acc= 0.7376173734664917 auc= 0.9064413242008236\n",
      "Test on valid set: loss= 2.498058557510376 acc= 0.3056352436542511 auc= 0.8133333333333332\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6983\n",
      "\n",
      "Test on train set: loss= 0.680226743221283 acc= 0.7630301117897034 auc= 0.9350988414211783\n",
      "Test on valid set: loss= 1.9411125183105469 acc= 0.33872076869010925 auc= 0.8804444444444444\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6670\n",
      "\n",
      "Test on train set: loss= 0.6406318545341492 acc= 0.7822920083999634 auc= 0.9409776032508151\n",
      "Test on valid set: loss= 2.058218002319336 acc= 0.3564864695072174 auc= 0.8928888888888888\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6331\n",
      "\n",
      "Test on train set: loss= 0.7508836388587952 acc= 0.7447394132614136 auc= 0.9404488131825962\n",
      "Test on valid set: loss= 1.9487862586975098 acc= 0.3683771789073944 auc= 0.8960000000000001\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6081\n",
      "\n",
      "Test on train set: loss= 0.5937050580978394 acc= 0.7948365211486816 auc= 0.9513893640920326\n",
      "Test on valid set: loss= 1.9524799585342407 acc= 0.4057385325431824 auc= 0.8866666666666667\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5787\n",
      "\n",
      "Test on train set: loss= 0.6722673177719116 acc= 0.7787309885025024 auc= 0.9430598682301884\n",
      "Test on valid set: loss= 2.3234550952911377 acc= 0.346577525138855 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5520\n",
      "\n",
      "Test on train set: loss= 0.5407740473747253 acc= 0.8127225637435913 auc= 0.963366364314185\n",
      "Test on valid set: loss= 1.8406341075897217 acc= 0.4367621839046478 auc= 0.916\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5312\n",
      "\n",
      "Test on train set: loss= 0.4927828013896942 acc= 0.8314988613128662 auc= 0.9663568818683819\n",
      "Test on valid set: loss= 1.6167974472045898 acc= 0.4663931429386139 auc= 0.9142222222222222\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5186\n",
      "\n",
      "Test on train set: loss= 0.5074111819267273 acc= 0.8258336186408997 auc= 0.9580245250669103\n",
      "Test on valid set: loss= 1.970912218093872 acc= 0.4341875910758972 auc= 0.9168888888888889\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.4916\n",
      "\n",
      "Test on train set: loss= 0.4438305199146271 acc= 0.8467950820922852 auc= 0.9744817545114677\n",
      "Test on valid set: loss= 1.6191811561584473 acc= 0.470231294631958 auc= 0.9119999999999999\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4767\n",
      "\n",
      "Test on train set: loss= 0.47141188383102417 acc= 0.8342505693435669 auc= 0.9750067592225442\n",
      "Test on valid set: loss= 1.5964370965957642 acc= 0.4899035394191742 auc= 0.9195555555555556\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4686\n",
      "\n",
      "Test on train set: loss= 0.48284533619880676 acc= 0.8371641039848328 auc= 0.9715321228692236\n",
      "Test on valid set: loss= 1.8844586610794067 acc= 0.41662758588790894 auc= 0.8915555555555557\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4434\n",
      "\n",
      "Test on train set: loss= 0.43072476983070374 acc= 0.8490611910820007 auc= 0.9785382648463685\n",
      "Test on valid set: loss= 1.7116857767105103 acc= 0.4507046341896057 auc= 0.9044444444444444\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4386\n",
      "\n",
      "Test on train set: loss= 0.511603057384491 acc= 0.8240531086921692 auc= 0.9751906930527955\n",
      "Test on valid set: loss= 1.7867019176483154 acc= 0.47720804810523987 auc= 0.9333333333333332\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4143\n",
      "\n",
      "Test on train set: loss= 0.5173822045326233 acc= 0.82720947265625 auc= 0.975530598273199\n",
      "Test on valid set: loss= 2.0311484336853027 acc= 0.4366142153739929 auc= 0.9079999999999998\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4108\n",
      "\n",
      "Test on train set: loss= 0.5095158815383911 acc= 0.8297992944717407 auc= 0.9736140913207055\n",
      "Test on valid set: loss= 1.711551547050476 acc= 0.4863014221191406 auc= 0.9462222222222223\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3971\n",
      "\n",
      "Test on train set: loss= 0.35280439257621765 acc= 0.8822434544563293 auc= 0.9828424468868185\n",
      "Test on valid set: loss= 1.5952093601226807 acc= 0.5048839449882507 auc= 0.9284444444444444\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3794\n",
      "\n",
      "Test on train set: loss= 0.40571507811546326 acc= 0.8629815578460693 auc= 0.9820666551184736\n",
      "Test on valid set: loss= 1.6372342109680176 acc= 0.5131344199180603 auc= 0.9142222222222223\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3688\n",
      "\n",
      "Test on train set: loss= 0.41545432806015015 acc= 0.8547264337539673 auc= 0.9832377919433373\n",
      "Test on valid set: loss= 1.4328571557998657 acc= 0.5449942350387573 auc= 0.9346666666666668\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3570\n",
      "\n",
      "Test on train set: loss= 0.4244655966758728 acc= 0.850517988204956 auc= 0.9848484690314525\n",
      "Test on valid set: loss= 1.6920870542526245 acc= 0.468252569437027 auc= 0.9311111111111112\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3487\n",
      "\n",
      "Test on train set: loss= 0.37370413541793823 acc= 0.8677565455436707 auc= 0.9865206585794452\n",
      "Test on valid set: loss= 1.6896867752075195 acc= 0.5008612275123596 auc= 0.9293333333333335\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3450\n",
      "\n",
      "Test on train set: loss= 0.436500608921051 acc= 0.8486565351486206 auc= 0.9851261206294852\n",
      "Test on valid set: loss= 1.4618312120437622 acc= 0.5365872979164124 auc= 0.9373333333333335\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3367\n",
      "\n",
      "Test on train set: loss= 0.2946573495864868 acc= 0.8993201851844788 auc= 0.9898221703860328\n",
      "Test on valid set: loss= 1.619564414024353 acc= 0.5028111338615417 auc= 0.9297777777777778\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3162\n",
      "\n",
      "Test on train set: loss= 0.34592536091804504 acc= 0.8779540061950684 auc= 0.989073008255156\n",
      "Test on valid set: loss= 1.6837536096572876 acc= 0.48911619186401367 auc= 0.9222222222222222\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3057\n",
      "\n",
      "Test on train set: loss= 0.2893625497817993 acc= 0.8991582989692688 auc= 0.991501566111974\n",
      "Test on valid set: loss= 1.5125612020492554 acc= 0.5309363007545471 auc= 0.9382222222222223\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3003\n",
      "\n",
      "Test on train set: loss= 0.30400213599205017 acc= 0.8945451378822327 auc= 0.991932858179297\n",
      "Test on valid set: loss= 1.4115054607391357 acc= 0.5725752711296082 auc= 0.944\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2989\n",
      "\n",
      "Test on train set: loss= 0.31363415718078613 acc= 0.8910650610923767 auc= 0.9900562103591458\n",
      "Test on valid set: loss= 1.3531494140625 acc= 0.5890412926673889 auc= 0.9440000000000002\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2861\n",
      "\n",
      "Test on train set: loss= 0.3080044984817505 acc= 0.8904176354408264 auc= 0.989912294305927\n",
      "Test on valid set: loss= 1.7793605327606201 acc= 0.512180507183075 auc= 0.9284444444444444\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2800\n",
      "\n",
      "Test on train set: loss= 0.3359830975532532 acc= 0.8832955360412598 auc= 0.9906837980868314\n",
      "Test on valid set: loss= 1.619482159614563 acc= 0.5411491990089417 auc= 0.9386666666666666\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2774\n",
      "\n",
      "Test on train set: loss= 0.3022136986255646 acc= 0.8977015018463135 auc= 0.9916539826615306\n",
      "Test on valid set: loss= 1.4122470617294312 acc= 0.5876885652542114 auc= 0.9386666666666666\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2603\n",
      "\n",
      "Test on train set: loss= 0.21132181584835052 acc= 0.9252185225486755 auc= 0.9949259499596621\n",
      "Test on valid set: loss= 1.6102609634399414 acc= 0.5288635492324829 auc= 0.9337777777777777\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2638\n",
      "\n",
      "Test on train set: loss= 0.24074088037014008 acc= 0.9135642647743225 auc= 0.9942209969079933\n",
      "Test on valid set: loss= 1.7123427391052246 acc= 0.5736544132232666 auc= 0.9351111111111111\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2504\n",
      "\n",
      "Test on train set: loss= 0.3241117000579834 acc= 0.885076105594635 auc= 0.9921154437316749\n",
      "Test on valid set: loss= 1.7961519956588745 acc= 0.5336395502090454 auc= 0.9275555555555556\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2406\n",
      "\n",
      "Test on train set: loss= 0.2032010853290558 acc= 0.926918089389801 auc= 0.995425813937746\n",
      "Test on valid set: loss= 1.3380545377731323 acc= 0.605864405632019 auc= 0.9493333333333334\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2313\n",
      "\n",
      "Test on train set: loss= 0.27469968795776367 acc= 0.9040951728820801 auc= 0.9939439496226049\n",
      "Test on valid set: loss= 1.2752809524536133 acc= 0.5810176134109497 auc= 0.9466666666666667\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2268\n",
      "\n",
      "Test on train set: loss= 0.23088598251342773 acc= 0.9163969159126282 auc= 0.9951241799327193\n",
      "Test on valid set: loss= 1.6592686176300049 acc= 0.5378706455230713 auc= 0.9457777777777778\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2377\n",
      "\n",
      "Test on train set: loss= 0.2588934600353241 acc= 0.9083845615386963 auc= 0.9945203056179718\n",
      "Test on valid set: loss= 1.4441272020339966 acc= 0.5853594541549683 auc= 0.9377777777777778\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2241\n",
      "\n",
      "Test on train set: loss= 0.23920103907585144 acc= 0.9131596088409424 auc= 0.9953134542758446\n",
      "Test on valid set: loss= 1.4736196994781494 acc= 0.5611113905906677 auc= 0.9493333333333334\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2214\n",
      "\n",
      "Test on train set: loss= 0.22573141753673553 acc= 0.9204435348510742 auc= 0.995380136623037\n",
      "Test on valid set: loss= 1.6364989280700684 acc= 0.5626208782196045 auc= 0.9408888888888889\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2092\n",
      "\n",
      "Test on train set: loss= 0.26162242889404297 acc= 0.9083036780357361 auc= 0.9964694676572847\n",
      "Test on valid set: loss= 1.5291136503219604 acc= 0.5664600729942322 auc= 0.9457777777777778\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1947\n",
      "\n",
      "Test on train set: loss= 0.2103622555732727 acc= 0.9276464581489563 auc= 0.9962404248129848\n",
      "Test on valid set: loss= 1.439974308013916 acc= 0.6147540807723999 auc= 0.9560000000000001\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2020\n",
      "\n",
      "Test on train set: loss= 0.21985779702663422 acc= 0.9205244183540344 auc= 0.9962693745445634\n",
      "Test on valid set: loss= 1.416233777999878 acc= 0.5874832272529602 auc= 0.9497777777777777\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1976\n",
      "\n",
      "Test on train set: loss= 0.1856873780488968 acc= 0.9366299510002136 auc= 0.996291939574362\n",
      "Test on valid set: loss= 1.6256905794143677 acc= 0.569494366645813 auc= 0.9346666666666668\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1946\n",
      "\n",
      "Test on train set: loss= 0.19318372011184692 acc= 0.9283748865127563 auc= 0.996911474990873\n",
      "Test on valid set: loss= 1.8487204313278198 acc= 0.5569411516189575 auc= 0.9431111111111111\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1821\n",
      "\n",
      "Test on train set: loss= 0.22194524109363556 acc= 0.919391393661499 auc= 0.9955609693444142\n",
      "Test on valid set: loss= 1.7920831441879272 acc= 0.5856667757034302 auc= 0.937777777777778\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1883\n",
      "\n",
      "Test on train set: loss= 0.21416929364204407 acc= 0.9264324903488159 auc= 0.9966688067610529\n",
      "Test on valid set: loss= 2.2299869060516357 acc= 0.5362387299537659 auc= 0.9400000000000001\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1823\n",
      "\n",
      "Test on train set: loss= 0.1934622824192047 acc= 0.9287795424461365 auc= 0.996983085421115\n",
      "Test on valid set: loss= 1.657955288887024 acc= 0.5526998043060303 auc= 0.9448888888888888\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1698\n",
      "\n",
      "Test on train set: loss= 0.2969578802585602 acc= 0.9003722667694092 auc= 0.995223714682733\n",
      "Test on valid set: loss= 1.9699759483337402 acc= 0.5304516553878784 auc= 0.923111111111111\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1591\n",
      "\n",
      "Test on train set: loss= 0.19248832762241364 acc= 0.9349303841590881 auc= 0.9969697836402329\n",
      "Test on valid set: loss= 2.2296345233917236 acc= 0.5489301681518555 auc= 0.9333333333333333\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1677\n",
      "\n",
      "Test on train set: loss= 0.15743590891361237 acc= 0.9436711072921753 auc= 0.9978394685716564\n",
      "Test on valid set: loss= 1.9896754026412964 acc= 0.5241661667823792 auc= 0.9333333333333333\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1551\n",
      "\n",
      "Test on train set: loss= 0.19965913891792297 acc= 0.9265943765640259 auc= 0.9970665075114702\n",
      "Test on valid set: loss= 1.9746496677398682 acc= 0.5450449585914612 auc= 0.939111111111111\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1526\n",
      "\n",
      "Test on train set: loss= 0.17836891114711761 acc= 0.9384914040565491 auc= 0.9971525802776651\n",
      "Test on valid set: loss= 2.199045419692993 acc= 0.5237230062484741 auc= 0.9239999999999998\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1560\n",
      "\n",
      "Test on train set: loss= 0.1599532812833786 acc= 0.9440757632255554 auc= 0.9976524995408441\n",
      "Test on valid set: loss= 1.9379733800888062 acc= 0.5181214213371277 auc= 0.9400000000000001\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1496\n",
      "\n",
      "Test on train set: loss= 0.27726486325263977 acc= 0.9065231680870056 auc= 0.9961826680050354\n",
      "Test on valid set: loss= 2.1268975734710693 acc= 0.5341017246246338 auc= 0.9186666666666665\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1392\n",
      "\n",
      "Test on train set: loss= 0.16625051200389862 acc= 0.94205242395401 auc= 0.997998300544032\n",
      "Test on valid set: loss= 1.7481213808059692 acc= 0.5549130439758301 auc= 0.9426666666666668\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1361\n",
      "\n",
      "Test on train set: loss= 0.18714353442192078 acc= 0.9336354732513428 auc= 0.9976531539730162\n",
      "Test on valid set: loss= 1.9798511266708374 acc= 0.5484017729759216 auc= 0.928888888888889\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1440\n",
      "\n",
      "Test on train set: loss= 0.15302057564258575 acc= 0.9458562731742859 auc= 0.9980816977690994\n",
      "Test on valid set: loss= 1.920780062675476 acc= 0.5535894632339478 auc= 0.9475555555555555\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1301\n",
      "\n",
      "Test on train set: loss= 0.13181239366531372 acc= 0.9514405727386475 auc= 0.9984385514157441\n",
      "Test on valid set: loss= 1.9553567171096802 acc= 0.5758047103881836 auc= 0.9266666666666667\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1257\n",
      "\n",
      "Test on train set: loss= 0.12632335722446442 acc= 0.954030454158783 auc= 0.9986888111122904\n",
      "Test on valid set: loss= 1.5676491260528564 acc= 0.5952251553535461 auc= 0.9400000000000001\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1358\n",
      "\n",
      "Test on train set: loss= 0.12109450250864029 acc= 0.9590482115745544 auc= 0.9989043294582534\n",
      "Test on valid set: loss= 1.4532967805862427 acc= 0.6306498050689697 auc= 0.9471111111111112\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1316\n",
      "\n",
      "Test on train set: loss= 0.17195658385753632 acc= 0.9377630352973938 auc= 0.9984198581914818\n",
      "Test on valid set: loss= 1.9831851720809937 acc= 0.5924855470657349 auc= 0.9306666666666666\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1264\n",
      "\n",
      "Test on train set: loss= 0.1436171531677246 acc= 0.950469434261322 auc= 0.9984602188736623\n",
      "Test on valid set: loss= 2.2817800045013428 acc= 0.58025062084198 auc= 0.9248888888888889\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1170\n",
      "\n",
      "Test on train set: loss= 0.11238736659288406 acc= 0.9588054418563843 auc= 0.9988783598353121\n",
      "Test on valid set: loss= 2.0632312297821045 acc= 0.5531483888626099 auc= 0.9435555555555555\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1108\n",
      "\n",
      "Test on train set: loss= 0.1457240879535675 acc= 0.9447231888771057 auc= 0.9987519553705144\n",
      "Test on valid set: loss= 2.1160781383514404 acc= 0.5689011812210083 auc= 0.9337777777777777\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1125\n",
      "\n",
      "Test on train set: loss= 0.1265946924686432 acc= 0.9549207091331482 auc= 0.9986352361670029\n",
      "Test on valid set: loss= 2.186493158340454 acc= 0.5923190712928772 auc= 0.9364444444444444\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1108\n",
      "\n",
      "Test on train set: loss= 0.09364917129278183 acc= 0.9667367935180664 auc= 0.9992124239559015\n",
      "Test on valid set: loss= 2.1746323108673096 acc= 0.5837432146072388 auc= 0.9395555555555555\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1189\n",
      "\n",
      "Test on train set: loss= 0.11806099861860275 acc= 0.9591291546821594 auc= 0.9987607310868286\n",
      "Test on valid set: loss= 1.929553508758545 acc= 0.5869432687759399 auc= 0.947111111111111\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1113\n",
      "\n",
      "Test on train set: loss= 0.08776480704545975 acc= 0.969731330871582 auc= 0.9991075921035335\n",
      "Test on valid set: loss= 2.0902488231658936 acc= 0.600390613079071 auc= 0.9186666666666665\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1144\n",
      "\n",
      "Test on train set: loss= 0.10023272037506104 acc= 0.9660084247589111 auc= 0.9991939960314429\n",
      "Test on valid set: loss= 1.855584740638733 acc= 0.5930286645889282 auc= 0.9346666666666665\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0984\n",
      "\n",
      "Test on train set: loss= 0.09325294941663742 acc= 0.9667367935180664 auc= 0.9993580372377521\n",
      "Test on valid set: loss= 2.041038751602173 acc= 0.5824634432792664 auc= 0.9279999999999999\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1057\n",
      "\n",
      "Test on train set: loss= 0.15036624670028687 acc= 0.9449660181999207 auc= 0.9984610641781014\n",
      "Test on valid set: loss= 2.149691104888916 acc= 0.57752525806427 auc= 0.944\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0917\n",
      "\n",
      "Test on train set: loss= 0.0945810005068779 acc= 0.9680317044258118 auc= 0.9992112330009336\n",
      "Test on valid set: loss= 1.6246758699417114 acc= 0.6549187898635864 auc= 0.9537777777777776\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1092\n",
      "\n",
      "Test on train set: loss= 0.13828814029693604 acc= 0.9490126371383667 auc= 0.9987699351789558\n",
      "Test on valid set: loss= 1.9309085607528687 acc= 0.6139189004898071 auc= 0.9497777777777777\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0958\n",
      "\n",
      "Test on train set: loss= 0.10591375082731247 acc= 0.9637423157691956 auc= 0.9991158276884157\n",
      "Test on valid set: loss= 1.9476488828659058 acc= 0.5809312462806702 auc= 0.9346666666666668\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0952\n",
      "\n",
      "Test on train set: loss= 0.1295587420463562 acc= 0.954030454158783 auc= 0.9986924463586115\n",
      "Test on valid set: loss= 2.0839126110076904 acc= 0.5822757482528687 auc= 0.9466666666666667\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1009\n",
      "\n",
      "Test on train set: loss= 0.14627394080162048 acc= 0.9483651518821716 auc= 0.9988253093679621\n",
      "Test on valid set: loss= 2.2198305130004883 acc= 0.580538809299469 auc= 0.9266666666666665\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0863\n",
      "\n",
      "Test on train set: loss= 0.07915026694536209 acc= 0.9716736674308777 auc= 0.9993791385206414\n",
      "Test on valid set: loss= 1.8880172967910767 acc= 0.6169813275337219 auc= 0.944\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0884\n",
      "\n",
      "Test on train set: loss= 0.07450636476278305 acc= 0.9726448655128479 auc= 0.9993688185675472\n",
      "Test on valid set: loss= 1.6160982847213745 acc= 0.608178973197937 auc= 0.9608888888888888\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0878\n",
      "\n",
      "Test on train set: loss= 0.08758118748664856 acc= 0.9707024693489075 auc= 0.9993807450824497\n",
      "Test on valid set: loss= 1.8709415197372437 acc= 0.5860125422477722 auc= 0.944888888888889\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0855\n",
      "\n",
      "Test on train set: loss= 0.10033807158470154 acc= 0.9651991128921509 auc= 0.9991804841800762\n",
      "Test on valid set: loss= 2.3449223041534424 acc= 0.5998381972312927 auc= 0.944888888888889\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0800\n",
      "\n",
      "Test on train set: loss= 0.09830810874700546 acc= 0.9628520607948303 auc= 0.9994616021908783\n",
      "Test on valid set: loss= 1.817266583442688 acc= 0.6270608901977539 auc= 0.9502222222222224\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0791\n",
      "\n",
      "Test on train set: loss= 0.0912591740489006 acc= 0.9682745337486267 auc= 0.999404030734205\n",
      "Test on valid set: loss= 2.0295326709747314 acc= 0.6081175208091736 auc= 0.9364444444444443\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0824\n",
      "\n",
      "Test on train set: loss= 0.07294159382581711 acc= 0.9740207195281982 auc= 0.9994838309916645\n",
      "Test on valid set: loss= 2.198812961578369 acc= 0.6002293825149536 auc= 0.9386666666666666\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0825\n",
      "\n",
      "Test on train set: loss= 0.053280264139175415 acc= 0.9810618162155151 auc= 0.9997326597311925\n",
      "Test on valid set: loss= 2.203240156173706 acc= 0.572510838508606 auc= 0.9231111111111112\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0753\n",
      "\n",
      "Test on train set: loss= 0.1260625571012497 acc= 0.9584817290306091 auc= 0.999111351993119\n",
      "Test on valid set: loss= 2.161073684692383 acc= 0.6155309677124023 auc= 0.9302222222222222\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0805\n",
      "\n",
      "Test on train set: loss= 0.06587613373994827 acc= 0.9763677716255188 auc= 0.9995933993124793\n",
      "Test on valid set: loss= 2.019779682159424 acc= 0.5863834619522095 auc= 0.9355555555555556\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0710\n",
      "\n",
      "Test on train set: loss= 0.06373768299818039 acc= 0.9781482815742493 auc= 0.9996699236276981\n",
      "Test on valid set: loss= 2.0359249114990234 acc= 0.6247156858444214 auc= 0.9271111111111111\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0776\n",
      "\n",
      "Test on train set: loss= 0.07981985062360764 acc= 0.9719164967536926 auc= 0.9992869305353059\n",
      "Test on valid set: loss= 2.213526964187622 acc= 0.6276910305023193 auc= 0.9333333333333333\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0781\n",
      "\n",
      "Test on train set: loss= 0.07559431344270706 acc= 0.9736970067024231 auc= 0.9995167393216144\n",
      "Test on valid set: loss= 1.9029043912887573 acc= 0.6068581342697144 auc= 0.9546666666666666\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0736\n",
      "\n",
      "Test on train set: loss= 0.10731448233127594 acc= 0.9626901745796204 auc= 0.9993586113371309\n",
      "Test on valid set: loss= 1.6015151739120483 acc= 0.6388018727302551 auc= 0.9613333333333335\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0652\n",
      "\n",
      "Test on train set: loss= 0.05482932925224304 acc= 0.9791194796562195 auc= 0.999782564789672\n",
      "Test on valid set: loss= 2.1164286136627197 acc= 0.5840637683868408 auc= 0.9497777777777779\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0643\n",
      "\n",
      "Test on train set: loss= 0.08241546899080276 acc= 0.9707024693489075 auc= 0.9995493065300491\n",
      "Test on valid set: loss= 2.136995553970337 acc= 0.5755404233932495 auc= 0.9542222222222222\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0628\n",
      "\n",
      "Test on train set: loss= 0.051485564559698105 acc= 0.9828423261642456 auc= 0.9997239382888292\n",
      "Test on valid set: loss= 1.8871371746063232 acc= 0.6288872957229614 auc= 0.9506666666666668\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0620\n",
      "\n",
      "Test on train set: loss= 0.05949604883790016 acc= 0.9787148237228394 auc= 0.9996685456580814\n",
      "Test on valid set: loss= 2.241786003112793 acc= 0.5700929164886475 auc= 0.9431111111111111\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0696\n",
      "\n",
      "Test on train set: loss= 0.0699547827243805 acc= 0.9748300313949585 auc= 0.999593906052852\n",
      "Test on valid set: loss= 1.9787359237670898 acc= 0.5847544074058533 auc= 0.9457777777777776\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0619\n",
      "\n",
      "Test on train set: loss= 0.04568734019994736 acc= 0.984865665435791 auc= 0.9997966265082783\n",
      "Test on valid set: loss= 1.8257238864898682 acc= 0.599543035030365 auc= 0.9444444444444444\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0599\n",
      "\n",
      "Test on train set: loss= 0.11994299292564392 acc= 0.9569439888000488 auc= 0.9994297673019492\n",
      "Test on valid set: loss= 1.9741079807281494 acc= 0.6268892288208008 auc= 0.9413333333333332\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0674\n",
      "\n",
      "Test on train set: loss= 0.08009226620197296 acc= 0.9715927243232727 auc= 0.9995297274478006\n",
      "Test on valid set: loss= 2.108139753341675 acc= 0.56363445520401 auc= 0.9466666666666667\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0575\n",
      "\n",
      "Test on train set: loss= 0.0408473014831543 acc= 0.9868889451026917 auc= 0.999802560473328\n",
      "Test on valid set: loss= 2.059109926223755 acc= 0.6038659811019897 auc= 0.9413333333333332\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0528\n",
      "\n",
      "Test on train set: loss= 0.04703289270401001 acc= 0.9827613830566406 auc= 0.999775156190662\n",
      "Test on valid set: loss= 2.7843401432037354 acc= 0.599492609500885 auc= 0.924\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0535\n",
      "\n",
      "Test on train set: loss= 0.04847898706793785 acc= 0.9834088683128357 auc= 0.9998685812237509\n",
      "Test on valid set: loss= 2.0541677474975586 acc= 0.6080722212791443 auc= 0.9435555555555556\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0588\n",
      "\n",
      "Test on train set: loss= 0.08532939106225967 acc= 0.9685173034667969 auc= 0.9995157860466394\n",
      "Test on valid set: loss= 2.4206793308258057 acc= 0.5573498010635376 auc= 0.9422222222222223\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0652\n",
      "\n",
      "Test on train set: loss= 0.0695180669426918 acc= 0.9745872616767883 auc= 0.9996386590918374\n",
      "Test on valid set: loss= 2.7147979736328125 acc= 0.5562812089920044 auc= 0.9386666666666665\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0539\n",
      "\n",
      "Test on train set: loss= 0.04716823250055313 acc= 0.9835707545280457 auc= 0.9998302144561739\n",
      "Test on valid set: loss= 2.3845834732055664 acc= 0.5709657669067383 auc= 0.9275555555555556\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0576\n",
      "\n",
      "Test on train set: loss= 0.05523398518562317 acc= 0.98073810338974 auc= 0.9997346185060193\n",
      "Test on valid set: loss= 2.4655258655548096 acc= 0.5742683410644531 auc= 0.9244444444444444\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0595\n",
      "\n",
      "Test on train set: loss= 0.07108999043703079 acc= 0.9758012294769287 auc= 0.9996177855783628\n",
      "Test on valid set: loss= 2.24973201751709 acc= 0.5666419267654419 auc= 0.9328888888888889\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0481\n",
      "\n",
      "Test on train set: loss= 0.047830015420913696 acc= 0.9834898114204407 auc= 0.9997560364045425\n",
      "Test on valid set: loss= 1.8418736457824707 acc= 0.5919610261917114 auc= 0.952888888888889\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0446\n",
      "\n",
      "Test on train set: loss= 0.05821102112531662 acc= 0.9800906181335449 auc= 0.9997624830339857\n",
      "Test on valid set: loss= 2.7879059314727783 acc= 0.5627661943435669 auc= 0.9257777777777777\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0480\n",
      "\n",
      "Test on train set: loss= 0.06286069750785828 acc= 0.977258026599884 auc= 0.9997395795980518\n",
      "Test on valid set: loss= 2.0393710136413574 acc= 0.5767925381660461 auc= 0.9444444444444444\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0492\n",
      "\n",
      "Test on train set: loss= 0.050944626331329346 acc= 0.9836516380310059 auc= 0.9998557967690788\n",
      "Test on valid set: loss= 2.198140859603882 acc= 0.5874265432357788 auc= 0.9453333333333335\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0480\n",
      "\n",
      "Test on train set: loss= 0.05001123249530792 acc= 0.9838944673538208 auc= 0.999854025206855\n",
      "Test on valid set: loss= 2.003138780593872 acc= 0.6122325658798218 auc= 0.9480000000000001\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0480\n",
      "\n",
      "Test on train set: loss= 0.040797170251607895 acc= 0.9863224625587463 auc= 0.9998551257620699\n",
      "Test on valid set: loss= 2.3332443237304688 acc= 0.6137278079986572 auc= 0.9408888888888891\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0384\n",
      "\n",
      "Test on train set: loss= 0.06299027800559998 acc= 0.9784719944000244 auc= 0.9998194804139509\n",
      "Test on valid set: loss= 2.142672061920166 acc= 0.6169986128807068 auc= 0.9453333333333334\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0503\n",
      "\n",
      "Test on train set: loss= 0.06252949684858322 acc= 0.9781482815742493 auc= 0.9997750952674609\n",
      "Test on valid set: loss= 2.1263320446014404 acc= 0.6882163882255554 auc= 0.9342222222222223\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0483\n",
      "\n",
      "Test on train set: loss= 0.03494628891348839 acc= 0.9889122843742371 auc= 0.9999012132316942\n",
      "Test on valid set: loss= 2.171382188796997 acc= 0.6412360072135925 auc= 0.9431111111111109\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0454\n",
      "\n",
      "Test on train set: loss= 0.05034437030553818 acc= 0.9821139574050903 auc= 0.9998035563816193\n",
      "Test on valid set: loss= 2.309048652648926 acc= 0.6151581406593323 auc= 0.9359999999999999\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0514\n",
      "\n",
      "Test on train set: loss= 0.04869631305336952 acc= 0.9837325811386108 auc= 0.9997562111931149\n",
      "Test on valid set: loss= 2.320136785507202 acc= 0.6225555539131165 auc= 0.9400000000000001\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0433\n",
      "\n",
      "Test on train set: loss= 0.04135425388813019 acc= 0.9861605763435364 auc= 0.9998290902772716\n",
      "Test on valid set: loss= 1.8590258359909058 acc= 0.6228157877922058 auc= 0.9426666666666665\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0511\n",
      "\n",
      "Test on train set: loss= 0.0439767986536026 acc= 0.9843800663948059 auc= 0.9998109787695805\n",
      "Test on valid set: loss= 2.444211721420288 acc= 0.6281248331069946 auc= 0.9408888888888889\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0452\n",
      "\n",
      "Test on train set: loss= 0.03682180866599083 acc= 0.9868080019950867 auc= 0.9999008270849293\n",
      "Test on valid set: loss= 2.2076258659362793 acc= 0.6016895174980164 auc= 0.9382222222222222\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0423\n",
      "\n",
      "Test on train set: loss= 0.07610290497541428 acc= 0.9742634892463684 auc= 0.9996036575740297\n",
      "Test on valid set: loss= 1.8498843908309937 acc= 0.6116957068443298 auc= 0.9417777777777777\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0437\n",
      "\n",
      "Test on train set: loss= 0.030677512288093567 acc= 0.9896406531333923 auc= 0.9999001138063404\n",
      "Test on valid set: loss= 2.013364553451538 acc= 0.5935319066047668 auc= 0.9568888888888889\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0445\n",
      "\n",
      "Test on train set: loss= 0.051674291491508484 acc= 0.98073810338974 auc= 0.999861721262074\n",
      "Test on valid set: loss= 2.556501626968384 acc= 0.6008095145225525 auc= 0.9413333333333334\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0477\n",
      "\n",
      "Test on train set: loss= 0.028141425922513008 acc= 0.9909355640411377 auc= 0.9999277001583126\n",
      "Test on valid set: loss= 2.2660465240478516 acc= 0.6032032370567322 auc= 0.9471111111111112\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0416\n",
      "\n",
      "Test on train set: loss= 0.03549100458621979 acc= 0.9866461753845215 auc= 0.9999182271042466\n",
      "Test on valid set: loss= 2.2641849517822266 acc= 0.6073300242424011 auc= 0.9431111111111111\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0402\n",
      "\n",
      "Test on train set: loss= 0.03031899966299534 acc= 0.9895597100257874 auc= 0.9999115255777372\n",
      "Test on valid set: loss= 2.3308870792388916 acc= 0.6155214309692383 auc= 0.9546666666666667\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0362\n",
      "\n",
      "Test on train set: loss= 0.03258919715881348 acc= 0.9889122843742371 auc= 0.9998880349467054\n",
      "Test on valid set: loss= 1.9560164213180542 acc= 0.6589992642402649 auc= 0.9697777777777776\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0363\n",
      "\n",
      "Test on train set: loss= 0.03703593090176582 acc= 0.9876982569694519 auc= 0.9998806026002542\n",
      "Test on valid set: loss= 2.3681693077087402 acc= 0.618239164352417 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0393\n",
      "\n",
      "Test on train set: loss= 0.03786466643214226 acc= 0.9866461753845215 auc= 0.9999035550864246\n",
      "Test on valid set: loss= 2.6579949855804443 acc= 0.616737961769104 auc= 0.932\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0350\n",
      "\n",
      "Test on train set: loss= 0.02414466254413128 acc= 0.9916639924049377 auc= 0.9999437079242938\n",
      "Test on valid set: loss= 2.493701457977295 acc= 0.5897685289382935 auc= 0.9453333333333334\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0340\n",
      "\n",
      "Test on train set: loss= 0.028950348496437073 acc= 0.9903690218925476 auc= 0.9999171615490134\n",
      "Test on valid set: loss= 2.2464826107025146 acc= 0.6274999976158142 auc= 0.9457777777777776\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0290\n",
      "\n",
      "Test on train set: loss= 0.03121509589254856 acc= 0.9903690218925476 auc= 0.9998788609696712\n",
      "Test on valid set: loss= 1.9691919088363647 acc= 0.6479565501213074 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0379\n",
      "\n",
      "Test on train set: loss= 0.03947661817073822 acc= 0.9865652322769165 auc= 0.9998904532299535\n",
      "Test on valid set: loss= 2.0062415599823 acc= 0.6403909921646118 auc= 0.9568888888888889\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0311\n",
      "\n",
      "Test on train set: loss= 0.019303787499666214 acc= 0.9938491582870483 auc= 0.9999734044936559\n",
      "Test on valid set: loss= 2.239530086517334 acc= 0.6311355829238892 auc= 0.944888888888889\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0276\n",
      "\n",
      "Test on train set: loss= 0.03335580602288246 acc= 0.9877792000770569 auc= 0.9999270772235833\n",
      "Test on valid set: loss= 2.008976459503174 acc= 0.660115659236908 auc= 0.9644444444444445\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0345\n",
      "\n",
      "Test on train set: loss= 0.04223441332578659 acc= 0.984784722328186 auc= 0.9998281421634058\n",
      "Test on valid set: loss= 2.6723849773406982 acc= 0.6339976191520691 auc= 0.9537777777777778\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0393\n",
      "\n",
      "Test on train set: loss= 0.03704052418470383 acc= 0.9855940341949463 auc= 0.9998691638865912\n",
      "Test on valid set: loss= 2.712355852127075 acc= 0.5840697884559631 auc= 0.9404444444444444\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0422\n",
      "\n",
      "Test on train set: loss= 0.02549162320792675 acc= 0.9910165071487427 auc= 0.9999398557007015\n",
      "Test on valid set: loss= 2.3806650638580322 acc= 0.5956829786300659 auc= 0.9480000000000001\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0270\n",
      "\n",
      "Test on train set: loss= 0.02234409749507904 acc= 0.9927160739898682 auc= 0.9999640280557648\n",
      "Test on valid set: loss= 1.8165631294250488 acc= 0.6528015732765198 auc= 0.9528888888888888\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0336\n",
      "\n",
      "Test on train set: loss= 0.025724483653903008 acc= 0.9912593364715576 auc= 0.9999478622645659\n",
      "Test on valid set: loss= 2.4225072860717773 acc= 0.6475870609283447 auc= 0.9493333333333333\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0358\n",
      "\n",
      "Test on train set: loss= 0.023116957396268845 acc= 0.9918258190155029 auc= 0.9999582284639376\n",
      "Test on valid set: loss= 2.436047315597534 acc= 0.6332475543022156 auc= 0.9444444444444443\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0276\n",
      "\n",
      "Test on train set: loss= 0.015633054077625275 acc= 0.9949012398719788 auc= 0.9999865129724264\n",
      "Test on valid set: loss= 2.0278425216674805 acc= 0.6462644934654236 auc= 0.9457777777777778\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0207\n",
      "\n",
      "Test on train set: loss= 0.017444849014282227 acc= 0.9940919280052185 auc= 0.9999730834121616\n",
      "Test on valid set: loss= 2.2656383514404297 acc= 0.6667546033859253 auc= 0.9497777777777779\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0302\n",
      "\n",
      "Test on train set: loss= 0.02298612892627716 acc= 0.9922304749488831 auc= 0.9999578569412803\n",
      "Test on valid set: loss= 1.9731698036193848 acc= 0.6034716367721558 auc= 0.9568888888888889\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0281\n",
      "\n",
      "Test on train set: loss= 0.028739620000123978 acc= 0.9892359972000122 auc= 0.9999418128592467\n",
      "Test on valid set: loss= 2.1335387229919434 acc= 0.6662516593933105 auc= 0.9497777777777777\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0284\n",
      "\n",
      "Test on train set: loss= 0.022279003635048866 acc= 0.9933635592460632 auc= 0.9999515186792571\n",
      "Test on valid set: loss= 2.1479461193084717 acc= 0.6399489641189575 auc= 0.9488888888888889\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0266\n",
      "\n",
      "Test on train set: loss= 0.024331919848918915 acc= 0.9913402199745178 auc= 0.9999576170143006\n",
      "Test on valid set: loss= 2.567258358001709 acc= 0.6033390164375305 auc= 0.9475555555555555\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0290\n",
      "\n",
      "Test on train set: loss= 0.029095768928527832 acc= 0.9901262521743774 auc= 0.9999216736381664\n",
      "Test on valid set: loss= 2.6473729610443115 acc= 0.6097219586372375 auc= 0.9297777777777776\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0266\n",
      "\n",
      "Test on train set: loss= 0.03594166785478592 acc= 0.9871317744255066 auc= 0.9999082503938161\n",
      "Test on valid set: loss= 2.537338972091675 acc= 0.5616247057914734 auc= 0.9497777777777777\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0258\n",
      "\n",
      "Test on train set: loss= 0.016106346622109413 acc= 0.9953868389129639 auc= 0.9999686308456035\n",
      "Test on valid set: loss= 2.6071159839630127 acc= 0.5750483870506287 auc= 0.9413333333333334\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0213\n",
      "\n",
      "Test on train set: loss= 0.027922719717025757 acc= 0.9908546209335327 auc= 0.9999352287334318\n",
      "Test on valid set: loss= 2.657679557800293 acc= 0.572801411151886 auc= 0.9444444444444444\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0258\n",
      "\n",
      "Test on train set: loss= 0.030587373301386833 acc= 0.9894787669181824 auc= 0.9999239569975622\n",
      "Test on valid set: loss= 2.392317295074463 acc= 0.5881158113479614 auc= 0.9462222222222222\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0285\n",
      "\n",
      "Test on train set: loss= 0.03909076377749443 acc= 0.9864842891693115 auc= 0.9998694643812277\n",
      "Test on valid set: loss= 2.9530258178710938 acc= 0.5883662700653076 auc= 0.9297777777777778\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0258\n",
      "\n",
      "Test on train set: loss= 0.02807512879371643 acc= 0.9900453090667725 auc= 0.9999458543203021\n",
      "Test on valid set: loss= 2.5601634979248047 acc= 0.6036192178726196 auc= 0.9431111111111111\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0220\n",
      "\n",
      "Test on train set: loss= 0.014109235256910324 acc= 0.9950631260871887 auc= 0.9999806998143932\n",
      "Test on valid set: loss= 2.896116018295288 acc= 0.5740330815315247 auc= 0.9404444444444445\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0183\n",
      "\n",
      "Test on train set: loss= 0.012673931196331978 acc= 0.995953381061554 auc= 0.9999792240141565\n",
      "Test on valid set: loss= 2.398221731185913 acc= 0.6246265769004822 auc= 0.9395555555555555\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0202\n",
      "\n",
      "Test on train set: loss= 0.010805363766849041 acc= 0.9973292350769043 auc= 0.9999898672118169\n",
      "Test on valid set: loss= 2.3343124389648438 acc= 0.6042207479476929 auc= 0.9444444444444444\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0270\n",
      "\n",
      "Test on train set: loss= 0.027532512322068214 acc= 0.9902881383895874 auc= 0.9999574841579417\n",
      "Test on valid set: loss= 2.5114288330078125 acc= 0.6341250538825989 auc= 0.9342222222222221\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0246\n",
      "\n",
      "Test on train set: loss= 0.024851635098457336 acc= 0.9909355640411377 auc= 0.9999526800347868\n",
      "Test on valid set: loss= 1.927769422531128 acc= 0.6556995511054993 auc= 0.9542222222222222\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0288\n",
      "\n",
      "Test on train set: loss= 0.027531251311302185 acc= 0.9898025393486023 auc= 0.9999254593715738\n",
      "Test on valid set: loss= 2.170988082885742 acc= 0.5875285863876343 auc= 0.9520000000000002\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0308\n",
      "\n",
      "Test on train set: loss= 0.03229158744215965 acc= 0.9881029725074768 auc= 0.9999500397773439\n",
      "Test on valid set: loss= 2.4181413650512695 acc= 0.5736467838287354 auc= 0.9373333333333334\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0326\n",
      "\n",
      "Test on train set: loss= 0.03137050196528435 acc= 0.9885885119438171 auc= 0.9999180666464538\n",
      "Test on valid set: loss= 1.8159271478652954 acc= 0.6661778092384338 auc= 0.9537777777777778\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0291\n",
      "\n",
      "Test on train set: loss= 0.02027263678610325 acc= 0.9938491582870483 auc= 0.9999657059186873\n",
      "Test on valid set: loss= 2.4927828311920166 acc= 0.601800262928009 auc= 0.9355555555555556\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0256\n",
      "\n",
      "Test on train set: loss= 0.02171146869659424 acc= 0.9927160739898682 auc= 0.9999687913939284\n",
      "Test on valid set: loss= 2.1517083644866943 acc= 0.653161883354187 auc= 0.9453333333333334\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0197\n",
      "\n",
      "Test on train set: loss= 0.014815469272434711 acc= 0.9953868389129639 auc= 0.9999907340357534\n",
      "Test on valid set: loss= 2.2983202934265137 acc= 0.6222999691963196 auc= 0.9435555555555556\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0177\n",
      "\n",
      "Test on train set: loss= 0.06466018408536911 acc= 0.9784719944000244 auc= 0.999804952242872\n",
      "Test on valid set: loss= 2.234436511993408 acc= 0.6194591522216797 auc= 0.9453333333333334\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0282\n",
      "\n",
      "Test on train set: loss= 0.022479452192783356 acc= 0.9936063289642334 auc= 0.9999493821421896\n",
      "Test on valid set: loss= 2.069542646408081 acc= 0.6082862019538879 auc= 0.9453333333333334\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0267\n",
      "\n",
      "Test on train set: loss= 0.017045913264155388 acc= 0.9946584701538086 auc= 0.9999706173488885\n",
      "Test on valid set: loss= 1.6193604469299316 acc= 0.6432424783706665 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0202\n",
      "\n",
      "Test on train set: loss= 0.01343290414661169 acc= 0.995872437953949 auc= 0.9999795028988654\n",
      "Test on valid set: loss= 1.9822862148284912 acc= 0.6439217329025269 auc= 0.9568888888888889\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0216\n",
      "\n",
      "Test on train set: loss= 0.016520163044333458 acc= 0.9942538142204285 auc= 0.9999706301278518\n",
      "Test on valid set: loss= 2.5993313789367676 acc= 0.6020643711090088 auc= 0.9488888888888889\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0201\n",
      "\n",
      "Test on train set: loss= 0.017446396872401237 acc= 0.9941728711128235 auc= 0.9999750921953379\n",
      "Test on valid set: loss= 2.1641852855682373 acc= 0.6096318364143372 auc= 0.952\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0178\n",
      "\n",
      "Test on train set: loss= 0.02046181447803974 acc= 0.9928779602050781 auc= 0.9999647859660368\n",
      "Test on valid set: loss= 2.2933003902435303 acc= 0.6253229975700378 auc= 0.9537777777777776\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0191\n",
      "\n",
      "Test on train set: loss= 0.02503190189599991 acc= 0.9915021061897278 auc= 0.9999565771545205\n",
      "Test on valid set: loss= 2.197237491607666 acc= 0.6392165422439575 auc= 0.9395555555555555\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0208\n",
      "\n",
      "Test on train set: loss= 0.014410392381250858 acc= 0.9956296682357788 auc= 0.9999884268657727\n",
      "Test on valid set: loss= 2.4961936473846436 acc= 0.6357129812240601 auc= 0.9355555555555556\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0251\n",
      "\n",
      "Test on train set: loss= 0.021300051361322403 acc= 0.9927160739898682 auc= 0.9999713230298527\n",
      "Test on valid set: loss= 2.4510843753814697 acc= 0.6010412573814392 auc= 0.9435555555555556\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0259\n",
      "\n",
      "Test on train set: loss= 0.028083793818950653 acc= 0.9910974502563477 auc= 0.9999265735846187\n",
      "Test on valid set: loss= 2.371511220932007 acc= 0.6242754459381104 auc= 0.9413333333333334\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0224\n",
      "\n",
      "Test on train set: loss= 0.014647813513875008 acc= 0.9953058958053589 auc= 0.999969467557365\n",
      "Test on valid set: loss= 2.227992534637451 acc= 0.5928939580917358 auc= 0.9484444444444445\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0203\n",
      "\n",
      "Test on train set: loss= 0.021323690190911293 acc= 0.992311418056488 auc= 0.9999678634114968\n",
      "Test on valid set: loss= 2.5643694400787354 acc= 0.594369113445282 auc= 0.9493333333333334\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0176\n",
      "\n",
      "Test on train set: loss= 0.012887129560112953 acc= 0.9956296682357788 auc= 0.9999910474669175\n",
      "Test on valid set: loss= 2.63577938079834 acc= 0.5996830463409424 auc= 0.9462222222222223\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0226\n",
      "\n",
      "Test on train set: loss= 0.028936710208654404 acc= 0.9891550540924072 auc= 0.9999372895227708\n",
      "Test on valid set: loss= 2.310817003250122 acc= 0.6475085020065308 auc= 0.9546666666666666\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0179\n",
      "\n",
      "Test on train set: loss= 0.015207274816930294 acc= 0.9950631260871887 auc= 0.999981295790662\n",
      "Test on valid set: loss= 2.937122106552124 acc= 0.5977975130081177 auc= 0.9391111111111112\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0159\n",
      "\n",
      "Test on train set: loss= 0.012499327771365643 acc= 0.9956296682357788 auc= 0.9999856789168262\n",
      "Test on valid set: loss= 2.54459547996521 acc= 0.6085627675056458 auc= 0.9373333333333335\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0153\n",
      "\n",
      "Test on train set: loss= 0.00881647877395153 acc= 0.9970055222511292 auc= 0.9999938443393365\n",
      "Test on valid set: loss= 2.6700658798217773 acc= 0.5711483955383301 auc= 0.9488888888888889\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0192\n",
      "\n",
      "Test on train set: loss= 0.023114560171961784 acc= 0.992473304271698 auc= 0.9999471055402556\n",
      "Test on valid set: loss= 2.5826902389526367 acc= 0.5856772065162659 auc= 0.9444444444444444\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0252\n",
      "\n",
      "Test on train set: loss= 0.01725710555911064 acc= 0.9940109848976135 auc= 0.9999693778159202\n",
      "Test on valid set: loss= 2.947798728942871 acc= 0.5980105400085449 auc= 0.9395555555555555\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0201\n",
      "\n",
      "Test on train set: loss= 0.0230331439524889 acc= 0.992311418056488 auc= 0.9999760886051117\n",
      "Test on valid set: loss= 2.8986616134643555 acc= 0.5913330316543579 auc= 0.9364444444444444\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0165\n",
      "\n",
      "Test on train set: loss= 0.019931280985474586 acc= 0.9939301013946533 auc= 0.9999788769755484\n",
      "Test on valid set: loss= 2.773728847503662 acc= 0.5866018533706665 auc= 0.9453333333333334\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0191\n",
      "\n",
      "Test on train set: loss= 0.02897428162395954 acc= 0.9891550540924072 auc= 0.9999686134560382\n",
      "Test on valid set: loss= 2.546729326248169 acc= 0.6099212169647217 auc= 0.9311111111111112\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0178\n",
      "\n",
      "Test on train set: loss= 0.008379487320780754 acc= 0.9971673488616943 auc= 0.999994949124136\n",
      "Test on valid set: loss= 2.5621285438537598 acc= 0.5873525142669678 auc= 0.9391111111111112\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0171\n",
      "\n",
      "Test on train set: loss= 0.014348517172038555 acc= 0.9951440691947937 auc= 0.9999842843934769\n",
      "Test on valid set: loss= 2.6841859817504883 acc= 0.5780207514762878 auc= 0.9537777777777778\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0149\n",
      "\n",
      "Test on train set: loss= 0.013186972588300705 acc= 0.9951440691947937 auc= 0.9999885971526942\n",
      "Test on valid set: loss= 2.935392379760742 acc= 0.5815448760986328 auc= 0.944\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0185\n",
      "\n",
      "Test on train set: loss= 0.012316043488681316 acc= 0.995953381061554 auc= 0.9999801193709755\n",
      "Test on valid set: loss= 2.5100600719451904 acc= 0.6025937795639038 auc= 0.9395555555555555\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0166\n",
      "\n",
      "Test on train set: loss= 0.015862327069044113 acc= 0.9948203563690186 auc= 0.9999826455417299\n",
      "Test on valid set: loss= 1.9238994121551514 acc= 0.6372150182723999 auc= 0.9604444444444444\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0188\n",
      "\n",
      "Test on train set: loss= 0.005139616318047047 acc= 0.9987050890922546 auc= 0.9999985301292351\n",
      "Test on valid set: loss= 1.896878957748413 acc= 0.6326679587364197 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0086\n",
      "\n",
      "Test on train set: loss= 0.003659170586615801 acc= 0.9992716312408447 auc= 0.9999996919680882\n",
      "Test on valid set: loss= 1.8361847400665283 acc= 0.641837477684021 auc= 0.963111111111111\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0072\n",
      "\n",
      "Test on train set: loss= 0.0025877468287944794 acc= 0.9997572302818298 auc= 0.9999999599769037\n",
      "Test on valid set: loss= 1.8665286302566528 acc= 0.639763355255127 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0059\n",
      "\n",
      "Test on train set: loss= 0.0021631093695759773 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.928739070892334 acc= 0.6405316591262817 auc= 0.9608888888888888\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0067\n",
      "\n",
      "Test on train set: loss= 0.0020754814613610506 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.9126886129379272 acc= 0.6450898051261902 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0042\n",
      "\n",
      "Test on train set: loss= 0.0017485045827925205 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9360026121139526 acc= 0.6458505988121033 auc= 0.9604444444444443\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0046\n",
      "\n",
      "Test on train set: loss= 0.0017961811972782016 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 2.054056406021118 acc= 0.635492205619812 auc= 0.9591111111111111\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0045\n",
      "\n",
      "Test on train set: loss= 0.0017216104315593839 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.9851510524749756 acc= 0.6415801048278809 auc= 0.9595555555555555\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0050\n",
      "\n",
      "Test on train set: loss= 0.001434341655112803 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.96977961063385 acc= 0.6344729065895081 auc= 0.9608888888888888\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0046\n",
      "\n",
      "Test on train set: loss= 0.0014194378163665533 acc= 0.99983811378479 auc= 1.0\n",
      "Test on valid set: loss= 2.0604970455169678 acc= 0.6290170550346375 auc= 0.96\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0045\n",
      "\n",
      "Test on train set: loss= 0.0013568218564614654 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.1042864322662354 acc= 0.6261398792266846 auc= 0.9613333333333334\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0039\n",
      "\n",
      "Test on train set: loss= 0.0012747987639158964 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.996773362159729 acc= 0.6369127631187439 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0043\n",
      "\n",
      "Test on train set: loss= 0.0011902943952009082 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.039649248123169 acc= 0.6310065984725952 auc= 0.9604444444444444\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0038\n",
      "\n",
      "Test on train set: loss= 0.0012478254502639174 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.991798996925354 acc= 0.6333696246147156 auc= 0.960888888888889\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0037\n",
      "\n",
      "Test on train set: loss= 0.0013188354205340147 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.010467767715454 acc= 0.6309852600097656 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0037\n",
      "\n",
      "Test on train set: loss= 0.0010981719242408872 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.018127202987671 acc= 0.6362482309341431 auc= 0.9613333333333334\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0040\n",
      "\n",
      "Test on train set: loss= 0.0010364213958382607 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.011464834213257 acc= 0.6307029128074646 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0027\n",
      "\n",
      "Test on train set: loss= 0.0009794012876227498 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.013216495513916 acc= 0.6301388740539551 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0035\n",
      "\n",
      "Test on train set: loss= 0.001008693827316165 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0287129878997803 acc= 0.6313303112983704 auc= 0.9617777777777776\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0027\n",
      "\n",
      "Test on train set: loss= 0.0010194224305450916 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0134193897247314 acc= 0.6274011731147766 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0033\n",
      "\n",
      "Test on train set: loss= 0.0009667824488133192 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9801874160766602 acc= 0.6350396871566772 auc= 0.9617777777777776\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0027\n",
      "\n",
      "Test on train set: loss= 0.0009167424286715686 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.973251461982727 acc= 0.6330863237380981 auc= 0.9617777777777776\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0036\n",
      "\n",
      "Test on train set: loss= 0.0009196235332638025 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9820784330368042 acc= 0.6324611306190491 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0033\n",
      "\n",
      "Test on train set: loss= 0.0008926436421461403 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.965964674949646 acc= 0.6380150318145752 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0031\n",
      "\n",
      "Test on train set: loss= 0.0008975730743259192 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9417178630828857 acc= 0.6337794065475464 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0032\n",
      "\n",
      "Test on train set: loss= 0.000762175943236798 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9991074800491333 acc= 0.631588339805603 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0028\n",
      "\n",
      "Test on train set: loss= 0.0007835718570277095 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9320892095565796 acc= 0.6329272985458374 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0031\n",
      "\n",
      "Test on train set: loss= 0.0008109409827739 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.992424726486206 acc= 0.6339623928070068 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0027\n",
      "\n",
      "Test on train set: loss= 0.000792474951595068 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.005221366882324 acc= 0.6377511620521545 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0036\n",
      "\n",
      "Test on train set: loss= 0.0008283737115561962 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.899887204170227 acc= 0.6456232666969299 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0007595154456794262 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.988264799118042 acc= 0.6386600732803345 auc= 0.9608888888888888\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0037\n",
      "\n",
      "Test on train set: loss= 0.0007856807787902653 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0189123153686523 acc= 0.6379777789115906 auc= 0.9613333333333334\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0029\n",
      "\n",
      "Test on train set: loss= 0.0007386358920484781 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9833439588546753 acc= 0.6393157243728638 auc= 0.9595555555555555\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0006911622476764023 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9284635782241821 acc= 0.6432257890701294 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0028\n",
      "\n",
      "Test on train set: loss= 0.0007365855271928012 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9182661771774292 acc= 0.6425175666809082 auc= 0.9608888888888888\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0026\n",
      "\n",
      "Test on train set: loss= 0.0007334349793381989 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9055942296981812 acc= 0.6419785022735596 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0029\n",
      "\n",
      "Test on train set: loss= 0.0006414907984435558 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9686596393585205 acc= 0.6307846307754517 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0006521175964735448 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9423290491104126 acc= 0.6376309394836426 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0026\n",
      "\n",
      "Test on train set: loss= 0.0006968712550587952 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.976235032081604 acc= 0.6423094868659973 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0028\n",
      "\n",
      "Test on train set: loss= 0.0006606880342587829 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0236599445343018 acc= 0.6354297399520874 auc= 0.9639999999999999\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0006488336366601288 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9955878257751465 acc= 0.6385355591773987 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0026\n",
      "\n",
      "Test on train set: loss= 0.0006017763516865671 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.958573341369629 acc= 0.6477655172348022 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005337096517905593 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0037872791290283 acc= 0.6338503956794739 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0006231432198546827 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9525833129882812 acc= 0.6436868906021118 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.000600402825511992 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9363995790481567 acc= 0.6528175473213196 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005610960652120411 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9991652965545654 acc= 0.637815535068512 auc= 0.9639999999999999\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0006032800301909447 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0374600887298584 acc= 0.6337651014328003 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005976507090963423 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9692623615264893 acc= 0.6514564752578735 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0035\n",
      "\n",
      "Test on train set: loss= 0.0005360523937270045 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0418670177459717 acc= 0.6343517899513245 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0005551159265451133 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.988986611366272 acc= 0.6399732232093811 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0005831685848534107 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9156639575958252 acc= 0.6561565399169922 auc= 0.963111111111111\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0006046011112630367 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9576025009155273 acc= 0.6416612863540649 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.0005478397943079472 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.998724102973938 acc= 0.6354609727859497 auc= 0.963111111111111\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005775676690973341 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.976040005683899 acc= 0.6432905793190002 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0005438031512312591 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9300552606582642 acc= 0.6487265229225159 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005789836868643761 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9580426216125488 acc= 0.64266037940979 auc= 0.963111111111111\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0005482666310854256 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0106122493743896 acc= 0.6397441625595093 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.000519509834703058 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9916174411773682 acc= 0.6439251899719238 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005207437206991017 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0092368125915527 acc= 0.6411170363426208 auc= 0.9613333333333334\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005448562442325056 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0185701847076416 acc= 0.6382866501808167 auc= 0.9617777777777776\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0017\n",
      "\n",
      "Test on train set: loss= 0.0005353839951567352 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.013266086578369 acc= 0.6393420100212097 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0005641253083012998 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.899347186088562 acc= 0.6485402584075928 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005483857239596546 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.943833589553833 acc= 0.6415520906448364 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005711360718123615 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9896320104599 acc= 0.6407459378242493 auc= 0.9626666666666666\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005361809744499624 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9774084091186523 acc= 0.6480899453163147 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.000551536213606596 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.00100040435791 acc= 0.6379930973052979 auc= 0.9653333333333334\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0026\n",
      "\n",
      "Test on train set: loss= 0.0005212377873249352 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9318069219589233 acc= 0.6541610956192017 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0005387362907640636 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9433529376983643 acc= 0.6493371725082397 auc= 0.9613333333333334\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0006132303969934583 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9456714391708374 acc= 0.6424193382263184 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0005149297649040818 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9660755395889282 acc= 0.6477442979812622 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005503217107616365 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9831678867340088 acc= 0.641801118850708 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.0005428227595984936 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0013527870178223 acc= 0.6380163431167603 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0006095610442571342 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.973385214805603 acc= 0.6425848603248596 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005378330242820084 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0352227687835693 acc= 0.6357353329658508 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.000492608523927629 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.001253128051758 acc= 0.6394760012626648 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005353605374693871 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9526572227478027 acc= 0.6496384739875793 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0005326506798155606 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9852467775344849 acc= 0.6384596228599548 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005229713278822601 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.012199640274048 acc= 0.6413807272911072 auc= 0.960888888888889\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0006167706451378763 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.998389482498169 acc= 0.6364837288856506 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005366954137571156 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9996764659881592 acc= 0.6461695432662964 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005467906012199819 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0285861492156982 acc= 0.6376549601554871 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.0005012682522647083 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9745497703552246 acc= 0.6468876600265503 auc= 0.9626666666666667\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.000511747959535569 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0115654468536377 acc= 0.6363338232040405 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005238961311988533 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9787784814834595 acc= 0.6384972929954529 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0004971222369931638 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9744340181350708 acc= 0.6364424824714661 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005137227126397192 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0199334621429443 acc= 0.6372015476226807 auc= 0.9644444444444445\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005325431702658534 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9738940000534058 acc= 0.6406427025794983 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0016\n",
      "\n",
      "Test on train set: loss= 0.0005398817593231797 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.972702980041504 acc= 0.6463462710380554 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0017\n",
      "\n",
      "Test on train set: loss= 0.0005217836005613208 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0197505950927734 acc= 0.6389414072036743 auc= 0.963111111111111\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0005115946987643838 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9760890007019043 acc= 0.645632803440094 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.000518139626365155 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9855378866195679 acc= 0.6392355561256409 auc= 0.9644444444444445\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0017\n",
      "\n",
      "Test on train set: loss= 0.0005947492900304496 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0046024322509766 acc= 0.6409175395965576 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0004997652722522616 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9646215438842773 acc= 0.6473003029823303 auc= 0.9617777777777776\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0004903936642222106 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9895581007003784 acc= 0.637283980846405 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0015\n",
      "\n",
      "Test on train set: loss= 0.0004901784704998136 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.0097382068634033 acc= 0.6346141695976257 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0004890562850050628 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9899568557739258 acc= 0.6445178985595703 auc= 0.9644444444444445\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0028\n",
      "\n",
      "Test on train set: loss= 0.00048587212222628295 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9621902704238892 acc= 0.6469032764434814 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0017\n",
      "\n",
      "Test on train set: loss= 0.0005162712186574936 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 2.003410577774048 acc= 0.637100100517273 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0005645775818265975 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9844356775283813 acc= 0.6426810622215271 auc= 0.963111111111111\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.000535177590791136 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.9575271606445312 acc= 0.643990159034729 auc= 0.9631111111111113\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "metric_idx = 0\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_Base(train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Baseline')\n",
    "                        \n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "baselineWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancement Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 6s - loss: 1.6127\n",
      "\n",
      "Test on train set: loss= 1.2967027425765991 acc= 0.5797183513641357 auc= 0.7758357180024144\n",
      "Test on valid set: loss= 2.4654417037963867 acc= 0.16071979701519012 auc= 0.7422222222222222\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 3s - loss: -4.8047e+00\n",
      "\n",
      "Test on train set: loss= 1.6758670806884766 acc= 0.6287633776664734 auc= 0.7810132291950906\n",
      "Test on valid set: loss= 5.556313991546631 acc= 0.23102211952209473 auc= 0.6702222222222222\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 3s - loss: -9.0067e+00\n",
      "\n",
      "Test on train set: loss= 3.2470877170562744 acc= 0.5425704121589661 auc= 0.7552857292343427\n",
      "Test on valid set: loss= 5.984004020690918 acc= 0.23545165359973907 auc= 0.6591111111111111\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 3s - loss: -8.6995e+00\n",
      "\n",
      "Test on train set: loss= 2.251871347427368 acc= 0.6405794620513916 auc= 0.8200046459798367\n",
      "Test on valid set: loss= 5.668622970581055 acc= 0.27612507343292236 auc= 0.7311111111111112\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 3s - loss: -1.4227e+01\n",
      "\n",
      "Test on train set: loss= 4.339864253997803 acc= 0.5914535522460938 auc= 0.7542132639685466\n",
      "Test on valid set: loss= 7.709156513214111 acc= 0.27441468834877014 auc= 0.6986666666666668\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2264e+01\n",
      "\n",
      "Test on train set: loss= 3.032294511795044 acc= 0.6908384561538696 auc= 0.8209860710014271\n",
      "Test on valid set: loss= 8.220244407653809 acc= 0.34016290307044983 auc= 0.7557777777777778\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1773e+01\n",
      "\n",
      "Test on train set: loss= 5.142660140991211 acc= 0.5944480299949646 auc= 0.7934716280659373\n",
      "Test on valid set: loss= 10.102823257446289 acc= 0.2419283390045166 auc= 0.7102222222222222\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2197e+01\n",
      "\n",
      "Test on train set: loss= 4.514429569244385 acc= 0.6598413586616516 auc= 0.7792281282381622\n",
      "Test on valid set: loss= 11.649672508239746 acc= 0.2189089059829712 auc= 0.666\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.4538e+01\n",
      "\n",
      "Test on train set: loss= 5.395528316497803 acc= 0.5969569683074951 auc= 0.727445735086391\n",
      "Test on valid set: loss= 10.482831954956055 acc= 0.25017768144607544 auc= 0.6737777777777778\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0516e+01\n",
      "\n",
      "Test on train set: loss= 4.650464057922363 acc= 0.6253641843795776 auc= 0.814122493130153\n",
      "Test on valid set: loss= 7.383535861968994 acc= 0.39201322197914124 auc= 0.8024444444444443\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.1345e+01\n",
      "\n",
      "Test on train set: loss= 5.342392921447754 acc= 0.592100977897644 auc= 0.8447111976977306\n",
      "Test on valid set: loss= 9.225726127624512 acc= 0.3274264931678772 auc= 0.774\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8065e+01\n",
      "\n",
      "Test on train set: loss= 3.585848093032837 acc= 0.7205406427383423 auc= 0.8337189461177182\n",
      "Test on valid set: loss= 8.59575366973877 acc= 0.32416778802871704 auc= 0.7791111111111111\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.4297e+01\n",
      "\n",
      "Test on train set: loss= 4.946444034576416 acc= 0.6464065909385681 auc= 0.809561388629789\n",
      "Test on valid set: loss= 10.170443534851074 acc= 0.32780003547668457 auc= 0.7491111111111112\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8906e+01\n",
      "\n",
      "Test on train set: loss= 4.8054962158203125 acc= 0.669553279876709 auc= 0.8136753725671891\n",
      "Test on valid set: loss= 10.384847640991211 acc= 0.30460208654403687 auc= 0.768888888888889\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.4433e+01\n",
      "\n",
      "Test on train set: loss= 5.015995502471924 acc= 0.6530430316925049 auc= 0.8200694073230743\n",
      "Test on valid set: loss= 9.925378799438477 acc= 0.33926260471343994 auc= 0.7500000000000001\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.1809e+01\n",
      "\n",
      "Test on train set: loss= 3.917818069458008 acc= 0.7362415194511414 auc= 0.820167546356808\n",
      "Test on valid set: loss= 8.832271575927734 acc= 0.3764950633049011 auc= 0.7722222222222221\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.8901e+01\n",
      "\n",
      "Test on train set: loss= 3.9647583961486816 acc= 0.7385076284408569 auc= 0.7758605990637595\n",
      "Test on valid set: loss= 9.431517601013184 acc= 0.38279908895492554 auc= 0.7791111111111111\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.9835e+01\n",
      "\n",
      "Test on train set: loss= 4.479181289672852 acc= 0.6968274712562561 auc= 0.7690425035431998\n",
      "Test on valid set: loss= 10.804710388183594 acc= 0.3140019476413727 auc= 0.7095555555555556\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3132e+01\n",
      "\n",
      "Test on train set: loss= 3.823620319366455 acc= 0.7453058958053589 auc= 0.8081956378017014\n",
      "Test on valid set: loss= 8.598135948181152 acc= 0.42137226462364197 auc= 0.7722222222222223\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.6388e+01\n",
      "\n",
      "Test on train set: loss= 3.216930866241455 acc= 0.7787309885025024 auc= 0.847136811486233\n",
      "Test on valid set: loss= 9.844000816345215 acc= 0.36008420586586 auc= 0.7853333333333333\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7724e+01\n",
      "\n",
      "Test on train set: loss= 5.391182899475098 acc= 0.6280349493026733 auc= 0.8070597455741731\n",
      "Test on valid set: loss= 9.081779479980469 acc= 0.38951677083969116 auc= 0.7826666666666667\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7842e+01\n",
      "\n",
      "Test on train set: loss= 3.0442283153533936 acc= 0.7965360879898071 auc= 0.8507960053571143\n",
      "Test on valid set: loss= 8.324487686157227 acc= 0.45730823278427124 auc= 0.7977777777777778\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.9235e+01\n",
      "\n",
      "Test on train set: loss= 2.9921391010284424 acc= 0.7992068529129028 auc= 0.8262439659162826\n",
      "Test on valid set: loss= 7.596490383148193 acc= 0.48712459206581116 auc= 0.8073333333333335\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.7593e+01\n",
      "\n",
      "Test on train set: loss= 2.859839677810669 acc= 0.811265766620636 auc= 0.8358705203308527\n",
      "Test on valid set: loss= 7.609835147857666 acc= 0.5165314078330994 auc= 0.8106666666666665\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.6171e+01\n",
      "\n",
      "Test on train set: loss= 2.8831217288970947 acc= 0.807866632938385 auc= 0.8481198888719472\n",
      "Test on valid set: loss= 7.9748215675354 acc= 0.4798446595668793 auc= 0.8435555555555556\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8505e+01\n",
      "\n",
      "Test on train set: loss= 3.0743868350982666 acc= 0.7902233600616455 auc= 0.8860490806953258\n",
      "Test on valid set: loss= 6.403026580810547 acc= 0.565204918384552 auc= 0.8415555555555556\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.1118e+01\n",
      "\n",
      "Test on train set: loss= 2.7585484981536865 acc= 0.8110229969024658 auc= 0.8810995324905223\n",
      "Test on valid set: loss= 6.225507736206055 acc= 0.5800655484199524 auc= 0.8573333333333334\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7483e+01\n",
      "\n",
      "Test on train set: loss= 4.141624450683594 acc= 0.7225639224052429 auc= 0.8654546195664368\n",
      "Test on valid set: loss= 7.093970775604248 acc= 0.5216323733329773 auc= 0.7991111111111111\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8989e+01\n",
      "\n",
      "Test on train set: loss= 2.7785918712615967 acc= 0.8110229969024658 auc= 0.853843163419674\n",
      "Test on valid set: loss= 7.237241744995117 acc= 0.5201317667961121 auc= 0.8395555555555555\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.8679e+01\n",
      "\n",
      "Test on train set: loss= 2.364781379699707 acc= 0.8392683863639832 auc= 0.8883471653920509\n",
      "Test on valid set: loss= 5.90372371673584 acc= 0.6057165861129761 auc= 0.8753333333333334\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.6925e+01\n",
      "\n",
      "Test on train set: loss= 3.555222511291504 acc= 0.7688572406768799 auc= 0.8332426701588593\n",
      "Test on valid set: loss= 7.414711952209473 acc= 0.5396161675453186 auc= 0.7668888888888888\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5320e+01\n",
      "\n",
      "Test on train set: loss= 3.167574167251587 acc= 0.7838296890258789 auc= 0.8790532565450822\n",
      "Test on valid set: loss= 7.060877799987793 acc= 0.5598008036613464 auc= 0.8606666666666667\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4124e+01\n",
      "\n",
      "Test on train set: loss= 3.0079715251922607 acc= 0.7921657562255859 auc= 0.871201889634967\n",
      "Test on valid set: loss= 5.977197170257568 acc= 0.6043647527694702 auc= 0.8480000000000001\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5228e+01\n",
      "\n",
      "Test on train set: loss= 2.884906530380249 acc= 0.807138204574585 auc= 0.8685611754870951\n",
      "Test on valid set: loss= 6.978105545043945 acc= 0.5246719717979431 auc= 0.8233333333333335\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.5741e+01\n",
      "\n",
      "Test on train set: loss= 2.6992969512939453 acc= 0.8194399476051331 auc= 0.8831000212868915\n",
      "Test on valid set: loss= 5.852142810821533 acc= 0.6103668212890625 auc= 0.8835555555555554\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.9365e+01\n",
      "\n",
      "Test on train set: loss= 2.276249647140503 acc= 0.8492230772972107 auc= 0.8898440320244122\n",
      "Test on valid set: loss= 6.055487632751465 acc= 0.5739167332649231 auc= 0.8608888888888888\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1605e+01\n",
      "\n",
      "Test on train set: loss= 2.8303885459899902 acc= 0.8141793608665466 auc= 0.8826047643956085\n",
      "Test on valid set: loss= 6.798895359039307 acc= 0.5638463497161865 auc= 0.8262222222222222\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.0687e+01\n",
      "\n",
      "Test on train set: loss= 2.3613758087158203 acc= 0.8425865769386292 auc= 0.9051271066210138\n",
      "Test on valid set: loss= 6.369647979736328 acc= 0.5928946733474731 auc= 0.9017777777777779\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4384e+01\n",
      "\n",
      "Test on train set: loss= 2.4072091579437256 acc= 0.8371641039848328 auc= 0.8828032438809966\n",
      "Test on valid set: loss= 6.283481597900391 acc= 0.5681453943252563 auc= 0.8568888888888889\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4320e+01\n",
      "\n",
      "Test on train set: loss= 2.370593309402466 acc= 0.8374069333076477 auc= 0.8856702794329058\n",
      "Test on valid set: loss= 6.475452899932861 acc= 0.582476019859314 auc= 0.8168888888888889\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3847e+01\n",
      "\n",
      "Test on train set: loss= 2.539081573486328 acc= 0.8310132622718811 auc= 0.8772339948475114\n",
      "Test on valid set: loss= 6.9557037353515625 acc= 0.5599766969680786 auc= 0.8213333333333332\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3325e+01\n",
      "\n",
      "Test on train set: loss= 2.2940614223480225 acc= 0.8490611910820007 auc= 0.8578476311657631\n",
      "Test on valid set: loss= 7.47944450378418 acc= 0.5201777219772339 auc= 0.8497777777777777\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.0389e+01\n",
      "\n",
      "Test on train set: loss= 2.8752007484436035 acc= 0.8089187145233154 auc= 0.8606264752765341\n",
      "Test on valid set: loss= 6.921983242034912 acc= 0.5582126379013062 auc= 0.8057777777777778\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3976e+01\n",
      "\n",
      "Test on train set: loss= 2.736685276031494 acc= 0.8115085959434509 auc= 0.9045725873543047\n",
      "Test on valid set: loss= 6.552410125732422 acc= 0.5601092576980591 auc= 0.8426666666666666\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.7661e+01\n",
      "\n",
      "Test on train set: loss= 2.465088367462158 acc= 0.8283424973487854 auc= 0.9085864071968093\n",
      "Test on valid set: loss= 5.090292453765869 acc= 0.6614953875541687 auc= 0.892\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.1742e+01\n",
      "\n",
      "Test on train set: loss= 2.581585168838501 acc= 0.8235675096511841 auc= 0.9005789149343928\n",
      "Test on valid set: loss= 4.934697151184082 acc= 0.6800874471664429 auc= 0.9008888888888889\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.6468e+01\n",
      "\n",
      "Test on train set: loss= 3.0441055297851562 acc= 0.8007445931434631 auc= 0.8229586564259226\n",
      "Test on valid set: loss= 7.319727897644043 acc= 0.5174547433853149 auc= 0.7755555555555557\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.8590e+01\n",
      "\n",
      "Test on train set: loss= 2.788771390914917 acc= 0.8141793608665466 auc= 0.8804192860738496\n",
      "Test on valid set: loss= 5.513615131378174 acc= 0.6403965353965759 auc= 0.8644444444444446\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8134e+01\n",
      "\n",
      "Test on train set: loss= 2.4599599838256836 acc= 0.8355454802513123 auc= 0.9072789240143297\n",
      "Test on valid set: loss= 5.9323811531066895 acc= 0.6200026273727417 auc= 0.8655555555555556\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.9580e+01\n",
      "\n",
      "Test on train set: loss= 3.266331195831299 acc= 0.7629491686820984 auc= 0.8886988701159412\n",
      "Test on valid set: loss= 5.568069934844971 acc= 0.6215308904647827 auc= 0.8975555555555556\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.4188e+01\n",
      "\n",
      "Test on train set: loss= 1.9182857275009155 acc= 0.8654094934463501 auc= 0.9281273376901515\n",
      "Test on valid set: loss= 3.951054096221924 acc= 0.7010833024978638 auc= 0.8915555555555554\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.3725e+01\n",
      "\n",
      "Test on train set: loss= 3.071092128753662 acc= 0.780349612236023 auc= 0.9130487933087134\n",
      "Test on valid set: loss= 5.857877254486084 acc= 0.5945591926574707 auc= 0.8575555555555555\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.5542e+01\n",
      "\n",
      "Test on train set: loss= 2.2785556316375732 acc= 0.8413726091384888 auc= 0.9051465532997321\n",
      "Test on valid set: loss= 4.046526908874512 acc= 0.7183237671852112 auc= 0.9046666666666667\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.9513e+01\n",
      "\n",
      "Test on train set: loss= 2.4358785152435303 acc= 0.8392683863639832 auc= 0.9101008492163863\n",
      "Test on valid set: loss= 5.677001476287842 acc= 0.6203527450561523 auc= 0.8477777777777777\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.0464e+01\n",
      "\n",
      "Test on train set: loss= 2.1210005283355713 acc= 0.8577209711074829 auc= 0.913437846085824\n",
      "Test on valid set: loss= 4.6522040367126465 acc= 0.699668288230896 auc= 0.8988888888888888\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3907e+01\n",
      "\n",
      "Test on train set: loss= 2.211109161376953 acc= 0.8480899930000305 auc= 0.8972690299549789\n",
      "Test on valid set: loss= 6.73219108581543 acc= 0.5393548607826233 auc= 0.8391111111111111\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1207e+01\n",
      "\n",
      "Test on train set: loss= 2.4288227558135986 acc= 0.8298802375793457 auc= 0.9097509182733974\n",
      "Test on valid set: loss= 5.849265098571777 acc= 0.6201080083847046 auc= 0.8884444444444444\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.7448e+01\n",
      "\n",
      "Test on train set: loss= 2.7852892875671387 acc= 0.8081094026565552 auc= 0.9027918844170175\n",
      "Test on valid set: loss= 6.774114608764648 acc= 0.5759549140930176 auc= 0.8364444444444444\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.2603e+01\n",
      "\n",
      "Test on train set: loss= 2.2372496128082275 acc= 0.8504370450973511 auc= 0.9077194691816912\n",
      "Test on valid set: loss= 4.547876358032227 acc= 0.703177273273468 auc= 0.8791111111111111\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1738e+02\n",
      "\n",
      "Test on train set: loss= 2.3422131538391113 acc= 0.8461475968360901 auc= 0.8702294247940425\n",
      "Test on valid set: loss= 5.330404281616211 acc= 0.6599933505058289 auc= 0.8708888888888889\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0632e+02\n",
      "\n",
      "Test on train set: loss= 2.4140264987945557 acc= 0.8405632972717285 auc= 0.8830994754524009\n",
      "Test on valid set: loss= 5.267691135406494 acc= 0.6600396633148193 auc= 0.8706666666666667\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.3118e+01\n",
      "\n",
      "Test on train set: loss= 2.1969616413116455 acc= 0.8517319560050964 auc= 0.9089997753252567\n",
      "Test on valid set: loss= 5.825937271118164 acc= 0.6261986494064331 auc= 0.8846666666666667\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.3196e+01\n",
      "\n",
      "Test on train set: loss= 2.286670207977295 acc= 0.8461475968360901 auc= 0.9292580612600359\n",
      "Test on valid set: loss= 5.688081741333008 acc= 0.6151806712150574 auc= 0.8835555555555554\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2435e+02\n",
      "\n",
      "Test on train set: loss= 1.9509435892105103 acc= 0.8718031644821167 auc= 0.8945538018189456\n",
      "Test on valid set: loss= 5.305675029754639 acc= 0.6590899229049683 auc= 0.8704444444444445\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2768e+02\n",
      "\n",
      "Test on train set: loss= 2.411773920059204 acc= 0.8404823541641235 auc= 0.8869719039249242\n",
      "Test on valid set: loss= 7.280242919921875 acc= 0.5400015711784363 auc= 0.847111111111111\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1018e+02\n",
      "\n",
      "Test on train set: loss= 2.288754940032959 acc= 0.8495467901229858 auc= 0.8712315440759808\n",
      "Test on valid set: loss= 6.354456901550293 acc= 0.6000002026557922 auc= 0.8391111111111111\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2410e+02\n",
      "\n",
      "Test on train set: loss= 2.4710018634796143 acc= 0.8384590744972229 auc= 0.870075638941373\n",
      "Test on valid set: loss= 6.427752494812012 acc= 0.5999999642372131 auc= 0.7871111111111111\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0778e+02\n",
      "\n",
      "Test on train set: loss= 2.4672937393188477 acc= 0.8333603143692017 auc= 0.9038316758714426\n",
      "Test on valid set: loss= 4.719778060913086 acc= 0.6963917016983032 auc= 0.8880000000000001\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.2758e+01\n",
      "\n",
      "Test on train set: loss= 2.242326021194458 acc= 0.8522175550460815 auc= 0.882022353736521\n",
      "Test on valid set: loss= 7.091962814331055 acc= 0.5599987506866455 auc= 0.8322222222222223\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.2525e+01\n",
      "\n",
      "Test on train set: loss= 2.1048176288604736 acc= 0.8595014810562134 auc= 0.9024878797348646\n",
      "Test on valid set: loss= 5.814537525177002 acc= 0.630937933921814 auc= 0.8491111111111111\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0869e+02\n",
      "\n",
      "Test on train set: loss= 1.8179020881652832 acc= 0.8818387985229492 auc= 0.8754175078730098\n",
      "Test on valid set: loss= 5.099212169647217 acc= 0.6747478246688843 auc= 0.8831111111111112\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2289e+02\n",
      "\n",
      "Test on train set: loss= 2.4111859798431396 acc= 0.8433959484100342 auc= 0.8986754927400531\n",
      "Test on valid set: loss= 4.837400913238525 acc= 0.6981217861175537 auc= 0.8524444444444444\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.3296e+01\n",
      "\n",
      "Test on train set: loss= 1.871417760848999 acc= 0.8786014914512634 auc= 0.8773962857663256\n",
      "Test on valid set: loss= 5.86520528793335 acc= 0.6207302808761597 auc= 0.8715555555555555\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.4105e+01\n",
      "\n",
      "Test on train set: loss= 1.60511314868927 acc= 0.8944642543792725 auc= 0.8990625525714847\n",
      "Test on valid set: loss= 7.018009185791016 acc= 0.5598035454750061 auc= 0.8364444444444444\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0412e+02\n",
      "\n",
      "Test on train set: loss= 1.935225009918213 acc= 0.8746358156204224 auc= 0.9027987755634467\n",
      "Test on valid set: loss= 3.932818651199341 acc= 0.7208973169326782 auc= 0.8902222222222222\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.6842e+02\n",
      "\n",
      "Test on train set: loss= 1.9503941535949707 acc= 0.8726934194564819 auc= 0.8793164717976006\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.866\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3590e+02\n",
      "\n",
      "Test on train set: loss= 1.8444510698318481 acc= 0.8802201151847839 auc= 0.9039150822094294\n",
      "Test on valid set: loss= 4.479637145996094 acc= 0.6977388858795166 auc= 0.8857777777777779\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1497e+02\n",
      "\n",
      "Test on train set: loss= 1.482993245124817 acc= 0.9016671776771545 auc= 0.9159074081343119\n",
      "Test on valid set: loss= 5.50930643081665 acc= 0.6398094892501831 auc= 0.8495555555555555\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1683e+02\n",
      "\n",
      "Test on train set: loss= 1.8461235761642456 acc= 0.8804629445075989 auc= 0.8901523649610684\n",
      "Test on valid set: loss= 5.490705966949463 acc= 0.6517980694770813 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2925e+02\n",
      "\n",
      "Test on train set: loss= 1.7304683923721313 acc= 0.8841049075126648 auc= 0.9197574912876737\n",
      "Test on valid set: loss= 5.147932052612305 acc= 0.6795608997344971 auc= 0.876222222222222\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2025e+02\n",
      "\n",
      "Test on train set: loss= 2.0938923358917236 acc= 0.8611201047897339 auc= 0.8979047700422351\n",
      "Test on valid set: loss= 5.802535533905029 acc= 0.6399788856506348 auc= 0.8257777777777777\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.9138e+01\n",
      "\n",
      "Test on train set: loss= 1.6169867515563965 acc= 0.8938167691230774 auc= 0.9090354658974279\n",
      "Test on valid set: loss= 4.168431282043457 acc= 0.7398463487625122 auc= 0.8811111111111112\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2049e+02\n",
      "\n",
      "Test on train set: loss= 1.9212331771850586 acc= 0.8739883303642273 auc= 0.9058392687383015\n",
      "Test on valid set: loss= 5.013418197631836 acc= 0.6790083646774292 auc= 0.8902222222222221\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0537e+02\n",
      "\n",
      "Test on train set: loss= 1.76896071434021 acc= 0.8824862241744995 auc= 0.9130718532591182\n",
      "Test on valid set: loss= 4.517889022827148 acc= 0.7157146334648132 auc= 0.8784444444444445\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.6450e+01\n",
      "\n",
      "Test on train set: loss= 2.0723743438720703 acc= 0.8640336394309998 auc= 0.8871701109238499\n",
      "Test on valid set: loss= 4.513364315032959 acc= 0.7197046279907227 auc= 0.8884444444444444\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.4304e+01\n",
      "\n",
      "Test on train set: loss= 14.295889854431152 acc= 0.11233408749103546 auc= 0.5398676481024403\n",
      "Test on valid set: loss= 13.216838836669922 acc= 0.17999999225139618 auc= 0.5542222222222222\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.9449e+01\n",
      "\n",
      "Test on train set: loss= 3.0122220516204834 acc= 0.800178050994873 auc= 0.8209249967762249\n",
      "Test on valid set: loss= 9.10925579071045 acc= 0.42031335830688477 auc= 0.7653333333333333\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0094e+02\n",
      "\n",
      "Test on train set: loss= 2.3116137981414795 acc= 0.854079008102417 auc= 0.8366018846616375\n",
      "Test on valid set: loss= 6.130587577819824 acc= 0.6150263547897339 auc= 0.806\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.6135e+02\n",
      "\n",
      "Test on train set: loss= 1.970558524131775 acc= 0.8743121027946472 auc= 0.8598862105852392\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8266666666666668\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8662e+02\n",
      "\n",
      "Test on train set: loss= 1.945367693901062 acc= 0.8745548725128174 auc= 0.8669191269009549\n",
      "Test on valid set: loss= 5.932165622711182 acc= 0.6199849247932434 auc= 0.8326666666666667\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4692e+02\n",
      "\n",
      "Test on train set: loss= 1.7154550552368164 acc= 0.8898510932922363 auc= 0.8851884475809447\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7399998903274536 auc= 0.8584444444444443\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.6981e+02\n",
      "\n",
      "Test on train set: loss= 1.9741958379745483 acc= 0.8740692734718323 auc= 0.8691277893834654\n",
      "Test on valid set: loss= 5.802515983581543 acc= 0.6399984359741211 auc= 0.8426666666666668\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2393e+02\n",
      "\n",
      "Test on train set: loss= 1.8000335693359375 acc= 0.8846713900566101 auc= 0.8828508226281142\n",
      "Test on valid set: loss= 4.993856906890869 acc= 0.68000727891922 auc= 0.8582222222222222\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7515e+02\n",
      "\n",
      "Test on train set: loss= 1.5886353254318237 acc= 0.8973777890205383 auc= 0.8907668337147004\n",
      "Test on valid set: loss= 4.820626258850098 acc= 0.6981891393661499 auc= 0.8517777777777779\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7183e+02\n",
      "\n",
      "Test on train set: loss= 1.76887047290802 acc= 0.8858854174613953 auc= 0.8872561687983612\n",
      "Test on valid set: loss= 5.802520751953125 acc= 0.6399938464164734 auc= 0.842888888888889\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.6465e+02\n",
      "\n",
      "Test on train set: loss= 1.6144256591796875 acc= 0.8953545093536377 auc= 0.8837993442486736\n",
      "Test on valid set: loss= 5.260128974914551 acc= 0.6407263875007629 auc= 0.8544444444444445\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8206e+02\n",
      "\n",
      "Test on train set: loss= 1.7882722616195679 acc= 0.8819197416305542 auc= 0.9161676618321358\n",
      "Test on valid set: loss= 5.196981906890869 acc= 0.662818431854248 auc= 0.8526666666666667\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7038e+02\n",
      "\n",
      "Test on train set: loss= 1.8638557195663452 acc= 0.8797345161437988 auc= 0.8883551036223911\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8886666666666667\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9894e+02\n",
      "\n",
      "Test on train set: loss= 1.6523574590682983 acc= 0.8948689103126526 auc= 0.8853105922023614\n",
      "Test on valid set: loss= 6.124904632568359 acc= 0.6199713349342346 auc= 0.8433333333333334\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.6509e+02\n",
      "\n",
      "Test on train set: loss= 1.6566872596740723 acc= 0.8927646279335022 auc= 0.8871537392048714\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8728888888888889\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9863e+02\n",
      "\n",
      "Test on train set: loss= 1.7877601385116577 acc= 0.8845905065536499 auc= 0.9039293608027027\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8442222222222224\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0183e+02\n",
      "\n",
      "Test on train set: loss= 1.7835745811462402 acc= 0.8835383653640747 auc= 0.9140550966432116\n",
      "Test on valid set: loss= 4.436686992645264 acc= 0.682846188545227 auc= 0.8791111111111112\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0885e+02\n",
      "\n",
      "Test on train set: loss= 1.8340094089508057 acc= 0.8811103701591492 auc= 0.9118187621982742\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.9500e+02\n",
      "\n",
      "Test on train set: loss= 1.8694508075714111 acc= 0.8791680335998535 auc= 0.9134917124040524\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.6999999284744263 auc= 0.8666666666666666\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2870e+02\n",
      "\n",
      "Test on train set: loss= 1.437737226486206 acc= 0.9081417918205261 auc= 0.8883535444178856\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.8297777777777778\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9477e+02\n",
      "\n",
      "Test on train set: loss= 1.5385888814926147 acc= 0.900291383266449 auc= 0.9202503327196038\n",
      "Test on valid set: loss= 4.887801647186279 acc= 0.6731818318367004 auc= 0.8486666666666667\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8221e+02\n",
      "\n",
      "Test on train set: loss= 1.916617751121521 acc= 0.8744739294052124 auc= 0.9143700592787933\n",
      "Test on valid set: loss= 6.008335590362549 acc= 0.6199917197227478 auc= 0.8368888888888888\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8961e+02\n",
      "\n",
      "Test on train set: loss= 1.3949826955795288 acc= 0.9103269577026367 auc= 0.90195024736852\n",
      "Test on valid set: loss= 5.401667594909668 acc= 0.6600001454353333 auc= 0.842\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.1010e+02\n",
      "\n",
      "Test on train set: loss= 1.7358125448226929 acc= 0.888394296169281 auc= 0.8929597234506479\n",
      "Test on valid set: loss= 5.092057704925537 acc= 0.6759234666824341 auc= 0.8615555555555556\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9526e+02\n",
      "\n",
      "Test on train set: loss= 1.4050334692001343 acc= 0.9095176458358765 auc= 0.9141776812827404\n",
      "Test on valid set: loss= 4.67244291305542 acc= 0.6935186386108398 auc= 0.8539999999999999\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.1243e+02\n",
      "\n",
      "Test on train set: loss= 1.7759371995925903 acc= 0.8866137862205505 auc= 0.9175724133129419\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8315555555555555\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8397e+02\n",
      "\n",
      "Test on train set: loss= 1.6418577432632446 acc= 0.8940595388412476 auc= 0.9024956752723741\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8586666666666668\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.1228e+02\n",
      "\n",
      "Test on train set: loss= 1.8809988498687744 acc= 0.8794108033180237 auc= 0.889364335483944\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.8433333333333332\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0451e+02\n",
      "\n",
      "Test on train set: loss= 1.5315303802490234 acc= 0.9015053510665894 auc= 0.9212562089224917\n",
      "Test on valid set: loss= 5.226973056793213 acc= 0.6400723457336426 auc= 0.858\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7123e+02\n",
      "\n",
      "Test on train set: loss= 2.2729058265686035 acc= 0.8545646071434021 auc= 0.8774808709789303\n",
      "Test on valid set: loss= 5.250702381134033 acc= 0.6601921319961548 auc= 0.820888888888889\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2916e+02\n",
      "\n",
      "Test on train set: loss= 2.0871710777282715 acc= 0.8640336394309998 auc= 0.8929665971473997\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8728888888888889\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.1162e+02\n",
      "\n",
      "Test on train set: loss= 1.7635729312896729 acc= 0.8856425881385803 auc= 0.9087982920649497\n",
      "Test on valid set: loss= 4.489755153656006 acc= 0.7000254392623901 auc= 0.8704444444444445\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9779e+02\n",
      "\n",
      "Test on train set: loss= 1.2636851072311401 acc= 0.9190676808357239 auc= 0.9136456673169839\n",
      "Test on valid set: loss= 5.157845497131348 acc= 0.6799448728561401 auc= 0.8597777777777778\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.3145e+02\n",
      "\n",
      "Test on train set: loss= 1.9452018737792969 acc= 0.8754451274871826 auc= 0.9041194362028694\n",
      "Test on valid set: loss= 5.1578545570373535 acc= 0.6799361705780029 auc= 0.8493333333333334\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.6609e+02\n",
      "\n",
      "Test on train set: loss= 1.8526257276535034 acc= 0.880705714225769 auc= 0.907862923768216\n",
      "Test on valid set: loss= 6.394206523895264 acc= 0.5881780385971069 auc= 0.8284444444444444\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8578e+02\n",
      "\n",
      "Test on train set: loss= 1.747910976409912 acc= 0.8874230980873108 auc= 0.9122974469018004\n",
      "Test on valid set: loss= 4.517499923706055 acc= 0.7159689664840698 auc= 0.8617777777777779\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6039e+02\n",
      "\n",
      "Test on train set: loss= 1.1938079595565796 acc= 0.9224668145179749 auc= 0.9258192899558706\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799997091293335 auc= 0.8264444444444443\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5048e+02\n",
      "\n",
      "Test on train set: loss= 1.2954285144805908 acc= 0.9162350296974182 auc= 0.9246293154689088\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8777777777777779\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5598e+02\n",
      "\n",
      "Test on train set: loss= 1.372481346130371 acc= 0.9114600419998169 auc= 0.9073917225894151\n",
      "Test on valid set: loss= 4.85606575012207 acc= 0.6603398323059082 auc= 0.8619999999999999\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2853e+02\n",
      "\n",
      "Test on train set: loss= 1.6461663246154785 acc= 0.8928455710411072 auc= 0.9149208740810721\n",
      "Test on valid set: loss= 4.364599227905273 acc= 0.7015938758850098 auc= 0.8731111111111112\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9953e+02\n",
      "\n",
      "Test on train set: loss= 1.5348032712936401 acc= 0.8995629549026489 auc= 0.9105855552082447\n",
      "Test on valid set: loss= 5.8056745529174805 acc= 0.6370055675506592 auc= 0.8397777777777777\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.5663e+02\n",
      "\n",
      "Test on train set: loss= 1.8241863250732422 acc= 0.8821625113487244 auc= 0.8853088457866871\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8373333333333333\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.5602e+02\n",
      "\n",
      "Test on train set: loss= 1.4128105640411377 acc= 0.9082227349281311 auc= 0.9058683401991946\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8464444444444444\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0746e+02\n",
      "\n",
      "Test on train set: loss= 1.9721707105636597 acc= 0.8725315928459167 auc= 0.8946357723742523\n",
      "Test on valid set: loss= 7.271595478057861 acc= 0.5400025248527527 auc= 0.8175555555555555\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.3934e+02\n",
      "\n",
      "Test on train set: loss= 1.5100767612457275 acc= 0.9040951728820801 auc= 0.9135346868685099\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.64000004529953 auc= 0.8286666666666667\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0955e+02\n",
      "\n",
      "Test on train set: loss= 1.302190899848938 acc= 0.9168015718460083 auc= 0.899587216070528\n",
      "Test on valid set: loss= 5.596132278442383 acc= 0.6239348649978638 auc= 0.8195555555555556\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1255e+02\n",
      "\n",
      "Test on train set: loss= 1.555104374885559 acc= 0.8997248411178589 auc= 0.8727701561385869\n",
      "Test on valid set: loss= 5.8791937828063965 acc= 0.6204324960708618 auc= 0.815111111111111\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8578e+02\n",
      "\n",
      "Test on train set: loss= 1.197131633758545 acc= 0.9223858714103699 auc= 0.9146793402459578\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8486666666666667\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0558e+02\n",
      "\n",
      "Test on train set: loss= 1.5791414976119995 acc= 0.8992392420768738 auc= 0.8964828025394033\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8417777777777777\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.4381e+02\n",
      "\n",
      "Test on train set: loss= 1.6618542671203613 acc= 0.8913887739181519 auc= 0.9099901258380504\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8524444444444444\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.3448e+02\n",
      "\n",
      "Test on train set: loss= 1.2904773950576782 acc= 0.9165587425231934 auc= 0.9152545279693408\n",
      "Test on valid set: loss= 5.445039749145508 acc= 0.6600000262260437 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.3810e+02\n",
      "\n",
      "Test on train set: loss= 2.1347639560699463 acc= 0.8641955256462097 auc= 0.9095617935160577\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8206666666666667\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.4244e+02\n",
      "\n",
      "Test on train set: loss= 1.264919400215149 acc= 0.9174489974975586 auc= 0.9282277992633254\n",
      "Test on valid set: loss= 4.6773681640625 acc= 0.7000054121017456 auc= 0.8515555555555556\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8049e+02\n",
      "\n",
      "Test on train set: loss= 1.9137629270553589 acc= 0.8788442611694336 auc= 0.8786025822140651\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8315555555555557\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0166e+02\n",
      "\n",
      "Test on train set: loss= 1.5455373525619507 acc= 0.9012625217437744 auc= 0.8987819231038353\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8511111111111113\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3403e+02\n",
      "\n",
      "Test on train set: loss= 1.95063316822052 acc= 0.8726934194564819 auc= 0.875338900214538\n",
      "Test on valid set: loss= 7.736685752868652 acc= 0.5199999809265137 auc= 0.7975555555555556\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.7553e+02\n",
      "\n",
      "Test on train set: loss= 1.774597406387329 acc= 0.8859663605690002 auc= 0.9153312871795528\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8384444444444444\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1043e+02\n",
      "\n",
      "Test on train set: loss= 1.6585787534713745 acc= 0.8947070240974426 auc= 0.8663632055474452\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5799999833106995 auc= 0.786\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9840e+02\n",
      "\n",
      "Test on train set: loss= 1.8092421293258667 acc= 0.884914219379425 auc= 0.8684168315221346\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.839111111111111\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.2480e+02\n",
      "\n",
      "Test on train set: loss= 1.7281742095947266 acc= 0.8909841179847717 auc= 0.8916561154279291\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7873333333333334\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.7070e+02\n",
      "\n",
      "Test on train set: loss= 1.2935261726379395 acc= 0.9168825149536133 auc= 0.9446329170385992\n",
      "Test on valid set: loss= 5.061678886413574 acc= 0.6800002455711365 auc= 0.8882222222222224\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1851e+02\n",
      "\n",
      "Test on train set: loss= 1.5935026407241821 acc= 0.8978633880615234 auc= 0.9229732086959093\n",
      "Test on valid set: loss= 5.106889724731445 acc= 0.6800000071525574 auc= 0.8824444444444446\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0699e+02\n",
      "\n",
      "Test on train set: loss= 1.4340755939483643 acc= 0.9071705937385559 auc= 0.9178485008613926\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.796\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1397e+02\n",
      "\n",
      "Test on train set: loss= 1.099799633026123 acc= 0.9298316836357117 auc= 0.9213372408181628\n",
      "Test on valid set: loss= 4.65529203414917 acc= 0.7000162601470947 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4976e+02\n",
      "\n",
      "Test on train set: loss= 1.3426347970962524 acc= 0.9142926335334778 auc= 0.9147480574429998\n",
      "Test on valid set: loss= 5.517189979553223 acc= 0.642342209815979 auc= 0.8206666666666667\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0179e+02\n",
      "\n",
      "Test on train set: loss= 1.198112964630127 acc= 0.9232761263847351 auc= 0.9316811619426068\n",
      "Test on valid set: loss= 4.829129219055176 acc= 0.699999988079071 auc= 0.8931111111111111\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3149e+02\n",
      "\n",
      "Test on train set: loss= 1.4513157606124878 acc= 0.9069278240203857 auc= 0.8991190238720247\n",
      "Test on valid set: loss= 5.5495452880859375 acc= 0.640622615814209 auc= 0.8468888888888888\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8764e+02\n",
      "\n",
      "Test on train set: loss= 1.2804369926452637 acc= 0.9175299406051636 auc= 0.9434621678521248\n",
      "Test on valid set: loss= 4.2905802726745605 acc= 0.7201355695724487 auc= 0.8851111111111111\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3103e+02\n",
      "\n",
      "Test on train set: loss= 1.9835090637207031 acc= 0.8730980753898621 auc= 0.9259816139378907\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8413333333333334\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6977e+02\n",
      "\n",
      "Test on train set: loss= 1.2230409383773804 acc= 0.9207672476768494 auc= 0.9212094657225943\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.818\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1235e+02\n",
      "\n",
      "Test on train set: loss= 1.4960811138153076 acc= 0.9056329131126404 auc= 0.9012329788719879\n",
      "Test on valid set: loss= 5.954744815826416 acc= 0.6200090646743774 auc= 0.8155555555555555\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8546e+02\n",
      "\n",
      "Test on train set: loss= 1.8852907419204712 acc= 0.8786824345588684 auc= 0.9081524992011486\n",
      "Test on valid set: loss= 5.1577911376953125 acc= 0.6799995303153992 auc= 0.8497777777777777\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.2666e+02\n",
      "\n",
      "Test on train set: loss= 1.316644310951233 acc= 0.915263831615448 auc= 0.8865009505926615\n",
      "Test on valid set: loss= 5.157791614532471 acc= 0.679998517036438 auc= 0.8322222222222223\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.2086e+02\n",
      "\n",
      "Test on train set: loss= 1.4556270837783813 acc= 0.9071705937385559 auc= 0.9140474049772394\n",
      "Test on valid set: loss= 5.279348373413086 acc= 0.660045862197876 auc= 0.8477777777777777\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3209e+02\n",
      "\n",
      "Test on train set: loss= 1.4290963411331177 acc= 0.9070087671279907 auc= 0.9267975381221397\n",
      "Test on valid set: loss= 5.674737453460693 acc= 0.6398657560348511 auc= 0.8362222222222222\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5167e+02\n",
      "\n",
      "Test on train set: loss= 1.3102055788040161 acc= 0.9144545197486877 auc= 0.9046583816239859\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8417777777777777\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6519e+02\n",
      "\n",
      "Test on train set: loss= 1.306549072265625 acc= 0.9162350296974182 auc= 0.9269148250692767\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8526666666666667\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4075e+02\n",
      "\n",
      "Test on train set: loss= 1.4550926685333252 acc= 0.9014244079589844 auc= 0.9413523589103286\n",
      "Test on valid set: loss= 6.641468524932861 acc= 0.5800012350082397 auc= 0.8146666666666667\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8494e+02\n",
      "\n",
      "Test on train set: loss= 1.2005945444107056 acc= 0.9231142997741699 auc= 0.9179380556937732\n",
      "Test on valid set: loss= 4.835439682006836 acc= 0.6999889612197876 auc= 0.8326666666666668\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.6645e+02\n",
      "\n",
      "Test on train set: loss= 1.800947666168213 acc= 0.8860472440719604 auc= 0.8647926177492071\n",
      "Test on valid set: loss= 5.488763332366943 acc= 0.6530030965805054 auc= 0.8091111111111111\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5405e+02\n",
      "\n",
      "Test on train set: loss= 1.3679332733154297 acc= 0.9135642647743225 auc= 0.913108572365787\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8182222222222221\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3768e+02\n",
      "\n",
      "Test on train set: loss= 1.3421781063079834 acc= 0.9144545197486877 auc= 0.891588368761032\n",
      "Test on valid set: loss= 4.836198329925537 acc= 0.6992450952529907 auc= 0.8322222222222223\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.2005e+02\n",
      "\n",
      "Test on train set: loss= 1.2164478302001953 acc= 0.92279052734375 auc= 0.9282664375580911\n",
      "Test on valid set: loss= 3.5459837913513184 acc= 0.7799972891807556 auc= 0.884222222222222\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3073e+02\n",
      "\n",
      "Test on train set: loss= 1.3790823221206665 acc= 0.9110553860664368 auc= 0.9158713465739519\n",
      "Test on valid set: loss= 5.157863616943359 acc= 0.6799265146255493 auc= 0.8497777777777777\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.7485e+02\n",
      "\n",
      "Test on train set: loss= 1.0399657487869263 acc= 0.9325833320617676 auc= 0.9379224319834611\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8624444444444446\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8346e+02\n",
      "\n",
      "Test on train set: loss= 1.4437222480773926 acc= 0.907737135887146 auc= 0.8638800088687064\n",
      "Test on valid set: loss= 5.260878086090088 acc= 0.6601154804229736 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.0349e+02\n",
      "\n",
      "Test on train set: loss= 1.2880229949951172 acc= 0.9186630249023438 auc= 0.8754958164942183\n",
      "Test on valid set: loss= 5.158685207366943 acc= 0.6791247129440308 auc= 0.8422222222222222\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5535e+02\n",
      "\n",
      "Test on train set: loss= 1.0340862274169922 acc= 0.9340401291847229 auc= 0.9275732348115572\n",
      "Test on valid set: loss= 5.62249755859375 acc= 0.6400161981582642 auc= 0.8377777777777778\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.2601e+02\n",
      "\n",
      "Test on train set: loss= 1.4230589866638184 acc= 0.9090320467948914 auc= 0.917292413464095\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.842\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4926e+02\n",
      "\n",
      "Test on train set: loss= 1.1578007936477661 acc= 0.9248138666152954 auc= 0.9467809579017388\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8195555555555554\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9263e+02\n",
      "\n",
      "Test on train set: loss= 1.1128772497177124 acc= 0.9282130002975464 auc= 0.9378656246923727\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8591111111111112\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7240e+02\n",
      "\n",
      "Test on train set: loss= 1.1328240633010864 acc= 0.9279702305793762 auc= 0.9126858404922032\n",
      "Test on valid set: loss= 5.997602462768555 acc= 0.6200011372566223 auc= 0.8046666666666666\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7343e+02\n",
      "\n",
      "Test on train set: loss= 1.2298753261566162 acc= 0.9204435348510742 auc= 0.9384711229387237\n",
      "Test on valid set: loss= 5.5410614013671875 acc= 0.6409515142440796 auc= 0.8482222222222223\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3383e+02\n",
      "\n",
      "Test on train set: loss= 1.3826029300689697 acc= 0.9123502969741821 auc= 0.9028048166933932\n",
      "Test on valid set: loss= 5.7824788093566895 acc= 0.6215962171554565 auc= 0.8393333333333333\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.4245e+02\n",
      "\n",
      "Test on train set: loss= 1.3324495553970337 acc= 0.9149401187896729 auc= 0.9113798366456892\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8393333333333333\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.9400e+02\n",
      "\n",
      "Test on train set: loss= 1.3939332962036133 acc= 0.9114600419998169 auc= 0.8617892173701585\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.820888888888889\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.4458e+02\n",
      "\n",
      "Test on train set: loss= 1.179686188697815 acc= 0.9249756932258606 auc= 0.9201440875964494\n",
      "Test on valid set: loss= 4.190708160400391 acc= 0.7399965524673462 auc= 0.8637777777777778\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.1918e+02\n",
      "\n",
      "Test on train set: loss= 1.2439348697662354 acc= 0.9210100173950195 auc= 0.8988725351588844\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.841777777777778\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6597e+02\n",
      "\n",
      "Test on train set: loss= 1.2157933712005615 acc= 0.9214146733283997 auc= 0.9278702217849759\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799997091293335 auc= 0.8491111111111111\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7661e+02\n",
      "\n",
      "Test on train set: loss= 1.1601542234420776 acc= 0.9252994656562805 auc= 0.9411896478234931\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7600000500679016 auc= 0.8755555555555556\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9903e+02\n",
      "\n",
      "Test on train set: loss= 1.3654366731643677 acc= 0.9125121235847473 auc= 0.8846110741973925\n",
      "Test on valid set: loss= 5.489118576049805 acc= 0.6527742147445679 auc= 0.8402222222222221\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4068e+02\n",
      "\n",
      "Test on train set: loss= 1.1491390466690063 acc= 0.9246519804000854 auc= 0.9102671379098165\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8295555555555556\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5469e+02\n",
      "\n",
      "Test on train set: loss= 1.3759398460388184 acc= 0.9094367027282715 auc= 0.9066706393012989\n",
      "Test on valid set: loss= 6.447238922119141 acc= 0.6000000238418579 auc= 0.7851111111111112\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.1590e+02\n",
      "\n",
      "Test on train set: loss= 1.2023476362228394 acc= 0.9233570694923401 auc= 0.9297802258719449\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8462222222222223\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.2669e+02\n",
      "\n",
      "Test on train set: loss= 1.1191002130508423 acc= 0.9283748865127563 auc= 0.9397698221813388\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8533333333333333\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.8871e+02\n",
      "\n",
      "Test on train set: loss= 1.1308701038360596 acc= 0.9277274012565613 auc= 0.9347380892715599\n",
      "Test on valid set: loss= 5.033458709716797 acc= 0.6800010204315186 auc= 0.8404444444444443\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3826e+02\n",
      "\n",
      "Test on train set: loss= 0.970938503742218 acc= 0.9376820921897888 auc= 0.9325251862355127\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8397777777777777\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1462e+02\n",
      "\n",
      "Test on train set: loss= 0.9979023933410645 acc= 0.9353350400924683 auc= 0.9450991948165889\n",
      "Test on valid set: loss= 5.74690055847168 acc= 0.6399999856948853 auc= 0.8320000000000001\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5596e+02\n",
      "\n",
      "Test on train set: loss= 1.3239080905914307 acc= 0.9137261509895325 auc= 0.9073467154313815\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.0894e+02\n",
      "\n",
      "Test on train set: loss= 1.107359766960144 acc= 0.9287795424461365 auc= 0.927826081986496\n",
      "Test on valid set: loss= 5.671866416931152 acc= 0.6400012373924255 auc= 0.8288888888888888\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5084e+02\n",
      "\n",
      "Test on train set: loss= 0.8814162611961365 acc= 0.94221431016922 auc= 0.9357277145067615\n",
      "Test on valid set: loss= 5.1961822509765625 acc= 0.662933349609375 auc= 0.8517777777777777\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.9473e+02\n",
      "\n",
      "Test on train set: loss= 1.0159868001937866 acc= 0.934768557548523 auc= 0.9260265493104278\n",
      "Test on valid set: loss= 5.740275859832764 acc= 0.6200603246688843 auc= 0.8195555555555556\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.9592e+02\n",
      "\n",
      "Test on train set: loss= 1.0895076990127563 acc= 0.9299935102462769 auc= 0.9297343045619012\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8075555555555555\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1447e+02\n",
      "\n",
      "Test on train set: loss= 1.3787119388580322 acc= 0.9125121235847473 auc= 0.8951730288795552\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8593333333333334\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7807e+02\n",
      "\n",
      "Test on train set: loss= 1.2904537916183472 acc= 0.9176108837127686 auc= 0.9361037287463679\n",
      "Test on valid set: loss= 4.208352088928223 acc= 0.7282759547233582 auc= 0.873777777777778\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -3.9703e+02\n",
      "\n",
      "Test on train set: loss= 0.9049146175384521 acc= 0.9418096542358398 auc= 0.9467822930879111\n",
      "Test on valid set: loss= 5.157808780670166 acc= 0.6799814701080322 auc= 0.8591111111111112\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.1478e+02\n",
      "\n",
      "Test on train set: loss= 0.7416861057281494 acc= 0.9517643451690674 auc= 0.9585952151308073\n",
      "Test on valid set: loss= 4.611993312835693 acc= 0.7000449895858765 auc= 0.8837777777777779\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.8782e+02\n",
      "\n",
      "Test on train set: loss= 0.6498545408248901 acc= 0.9581579566001892 auc= 0.9601182585072037\n",
      "Test on valid set: loss= 4.513067245483398 acc= 0.7199999690055847 auc= 0.8848888888888888\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.3068e+02\n",
      "\n",
      "Test on train set: loss= 0.6431010365486145 acc= 0.9578342437744141 auc= 0.9663571323772266\n",
      "Test on valid set: loss= 4.793662071228027 acc= 0.7000000476837158 auc= 0.8751111111111112\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.4557e+02\n",
      "\n",
      "Test on train set: loss= 0.6383377909660339 acc= 0.9583198428153992 auc= 0.9672010711801983\n",
      "Test on valid set: loss= 4.513067245483398 acc= 0.7199996709823608 auc= 0.8737777777777778\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.1847e+02\n",
      "\n",
      "Test on train set: loss= 0.6157059669494629 acc= 0.9591291546821594 auc= 0.971665950546474\n",
      "Test on valid set: loss= 3.944636344909668 acc= 0.7404409646987915 auc= 0.886888888888889\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.0458e+02\n",
      "\n",
      "Test on train set: loss= 0.649552047252655 acc= 0.9568630456924438 auc= 0.9738169035981498\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599998712539673 auc= 0.8864444444444445\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -3.9629e+02\n",
      "\n",
      "Test on train set: loss= 0.6132077574729919 acc= 0.9586435556411743 auc= 0.9737611236296317\n",
      "Test on valid set: loss= 4.190893173217773 acc= 0.7398125529289246 auc= 0.8737777777777778\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -3.8446e+02\n",
      "\n",
      "Test on train set: loss= 0.6040377616882324 acc= 0.9593719840049744 auc= 0.9725484806213336\n",
      "Test on valid set: loss= 3.7601141929626465 acc= 0.7600004076957703 auc= 0.8873333333333333\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -3.8541e+02\n",
      "\n",
      "Test on train set: loss= 0.6012799739837646 acc= 0.9590482115745544 auc= 0.9728436757849324\n",
      "Test on valid set: loss= 3.9636502265930176 acc= 0.7401703596115112 auc= 0.8862222222222222\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -3.7830e+02\n",
      "\n",
      "Test on train set: loss= 0.5827377438545227 acc= 0.9606668949127197 auc= 0.9672578682416013\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8657777777777778\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.4808e+02\n",
      "\n",
      "Test on train set: loss= 0.5775760412216187 acc= 0.9607478380203247 auc= 0.9712701102941048\n",
      "Test on valid set: loss= 4.190864086151123 acc= 0.7398412823677063 auc= 0.8764444444444445\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.8154e+02\n",
      "\n",
      "Test on train set: loss= 0.6272948980331421 acc= 0.9570249319076538 auc= 0.9705860728951319\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.7887e+02\n",
      "\n",
      "Test on train set: loss= 0.597935140132904 acc= 0.9595338106155396 auc= 0.9673197155054742\n",
      "Test on valid set: loss= 4.256316184997559 acc= 0.7206296324729919 auc= 0.8573333333333334\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.0009e+02\n",
      "\n",
      "Test on train set: loss= 0.5195534229278564 acc= 0.9652799963951111 auc= 0.9670225499076428\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8531111111111113\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.1190e+02\n",
      "\n",
      "Test on train set: loss= 0.5243535041809082 acc= 0.965522825717926 auc= 0.965743587990224\n",
      "Test on valid set: loss= 4.513068675994873 acc= 0.7199980020523071 auc= 0.8539999999999999\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.1511e+02\n",
      "\n",
      "Test on train set: loss= 0.5175769925117493 acc= 0.9650372266769409 auc= 0.9733245763875406\n",
      "Test on valid set: loss= 3.936497449874878 acc= 0.7406623959541321 auc= 0.8675555555555556\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.2232e+02\n",
      "\n",
      "Test on train set: loss= 0.5499241352081299 acc= 0.9627711176872253 auc= 0.9745144517726558\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000691413879 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.4402e+02\n",
      "\n",
      "Test on train set: loss= 0.5357165932655334 acc= 0.9635804295539856 auc= 0.9770827139202536\n",
      "Test on valid set: loss= 3.894644260406494 acc= 0.7453691363334656 auc= 0.8782222222222223\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.2606e+02\n",
      "\n",
      "Test on train set: loss= 0.569957971572876 acc= 0.9611524939537048 auc= 0.9733901950385168\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.8780000000000001\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.6612e+02\n",
      "\n",
      "Test on train set: loss= 0.4831800162792206 acc= 0.9662511944770813 auc= 0.9781006739515504\n",
      "Test on valid set: loss= 3.8683440685272217 acc= 0.7599987387657166 auc= 0.8682222222222222\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.3057e+02\n",
      "\n",
      "Test on train set: loss= 0.530750036239624 acc= 0.9632567167282104 auc= 0.9774133482232642\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.866\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -4.7613e+02\n",
      "\n",
      "Test on train set: loss= 0.47326141595840454 acc= 0.9678698778152466 auc= 0.9759668270806803\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.0614e+02\n",
      "\n",
      "Test on train set: loss= 0.47030240297317505 acc= 0.9683554768562317 auc= 0.975402013174454\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8635555555555555\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.0872e+02\n",
      "\n",
      "Test on train set: loss= 0.4604089558124542 acc= 0.9681935906410217 auc= 0.9797658339609138\n",
      "Test on valid set: loss= 4.276537895202637 acc= 0.7202736139297485 auc= 0.8642222222222223\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.2445e+02\n",
      "\n",
      "Test on train set: loss= 0.4953388571739197 acc= 0.965603768825531 auc= 0.979131840275276\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.3223e+02\n",
      "\n",
      "Test on train set: loss= 0.5819395184516907 acc= 0.9607478380203247 auc= 0.9735189181756745\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8753333333333334\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.3618e+02\n",
      "\n",
      "Test on train set: loss= 0.5056597590446472 acc= 0.9639042019844055 auc= 0.9796995879145431\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8780000000000001\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.0297e+02\n",
      "\n",
      "Test on train set: loss= 0.6903236508369446 acc= 0.9501456618309021 auc= 0.9766257298143358\n",
      "Test on valid set: loss= 3.9632210731506348 acc= 0.7399120330810547 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.2119e+02\n",
      "\n",
      "Test on train set: loss= 0.4603956341743469 acc= 0.9683554768562317 auc= 0.9778523190343128\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8673333333333334\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.3966e+02\n",
      "\n",
      "Test on train set: loss= 0.4721353352069855 acc= 0.9683554768562317 auc= 0.9782139605221787\n",
      "Test on valid set: loss= 4.470071792602539 acc= 0.7200000286102295 auc= 0.8620000000000001\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.4082e+02\n",
      "\n",
      "Test on train set: loss= 0.4333377480506897 acc= 0.9711880683898926 auc= 0.9787043985840119\n",
      "Test on valid set: loss= 4.1907057762146 acc= 0.7399992942810059 auc= 0.866\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.6762e+02\n",
      "\n",
      "Test on train set: loss= 0.4145109951496124 acc= 0.973211407661438 auc= 0.9762250654441622\n",
      "Test on valid set: loss= 4.605665683746338 acc= 0.7001947164535522 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.8941e+02\n",
      "\n",
      "Test on train set: loss= 0.4184677004814148 acc= 0.9726448655128479 auc= 0.9777532813959106\n",
      "Test on valid set: loss= 4.363616466522217 acc= 0.7198367714881897 auc= 0.8757777777777779\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -6.0700e+02\n",
      "\n",
      "Test on train set: loss= 0.3960440754890442 acc= 0.9735351204872131 auc= 0.981146128376216\n",
      "Test on valid set: loss= 4.4322829246521 acc= 0.720000147819519 auc= 0.8544444444444445\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.7630e+02\n",
      "\n",
      "Test on train set: loss= 0.4074949622154236 acc= 0.9722402095794678 auc= 0.9829435820666805\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999308586121 auc= 0.8786666666666667\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -6.0169e+02\n",
      "\n",
      "Test on train set: loss= 0.39709171652793884 acc= 0.9725639224052429 auc= 0.9819025590558148\n",
      "Test on valid set: loss= 3.8692147731781006 acc= 0.7591468095779419 auc= 0.868\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.6827e+02\n",
      "\n",
      "Test on train set: loss= 0.3844444155693054 acc= 0.9737779498100281 auc= 0.9825170291167856\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.3849e+02\n",
      "\n",
      "Test on train set: loss= 0.4268933832645416 acc= 0.9710262417793274 auc= 0.9763889451981154\n",
      "Test on valid set: loss= 4.513116359710693 acc= 0.719950258731842 auc= 0.8646666666666667\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.4681e+02\n",
      "\n",
      "Test on train set: loss= 0.45985889434814453 acc= 0.9683554768562317 auc= 0.9770778749616585\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.5945e+02\n",
      "\n",
      "Test on train set: loss= 0.40217718482017517 acc= 0.9726448655128479 auc= 0.9818525283716568\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.8161e+02\n",
      "\n",
      "Test on train set: loss= 0.42292845249176025 acc= 0.9721592664718628 auc= 0.9800431042668969\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555557\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.8229e+02\n",
      "\n",
      "Test on train set: loss= 0.4573493003845215 acc= 0.9701359868049622 auc= 0.9785131893411009\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8748888888888888\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.6212e+02\n",
      "\n",
      "Test on train set: loss= 0.41070765256881714 acc= 0.973130464553833 auc= 0.977874449025121\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8753333333333334\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.5954e+02\n",
      "\n",
      "Test on train set: loss= 0.4295497536659241 acc= 0.9715927243232727 auc= 0.9821844927824686\n",
      "Test on valid set: loss= 4.1907057762146 acc= 0.7399993538856506 auc= 0.8842222222222222\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.7389e+02\n",
      "\n",
      "Test on train set: loss= 0.3876093327999115 acc= 0.9745872616767883 auc= 0.9818728172203466\n",
      "Test on valid set: loss= 4.289693832397461 acc= 0.7201407551765442 auc= 0.8748888888888888\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.6584e+02\n",
      "\n",
      "Test on train set: loss= 0.3728635013103485 acc= 0.9752346873283386 auc= 0.983226654820767\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.7000000476837158 auc= 0.8726666666666667\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -5.9166e+02\n",
      "\n",
      "Test on train set: loss= 0.38189181685447693 acc= 0.9749109745025635 auc= 0.9821640696208623\n",
      "Test on valid set: loss= 5.162222385406494 acc= 0.6601583361625671 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -6.1178e+02\n",
      "\n",
      "Test on train set: loss= 0.3712805211544037 acc= 0.9755584597587585 auc= 0.9813223423878291\n",
      "Test on valid set: loss= 5.0298943519592285 acc= 0.6800012588500977 auc= 0.8828888888888888\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -6.3841e+02\n",
      "\n",
      "Test on train set: loss= 0.3775903880596161 acc= 0.9756393432617188 auc= 0.953674939560407\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8415555555555556\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.0885e+02\n",
      "\n",
      "Test on train set: loss= 0.39054861664772034 acc= 0.9742634892463684 auc= 0.9537963058759782\n",
      "Test on valid set: loss= 4.835892677307129 acc= 0.699541449546814 auc= 0.8537777777777779\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.9636e+02\n",
      "\n",
      "Test on train set: loss= 0.40957197546958923 acc= 0.973292350769043 auc= 0.9538499716281426\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8320000000000001\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.7020e+02\n",
      "\n",
      "Test on train set: loss= 0.47583916783332825 acc= 0.9686791896820068 auc= 0.9519867322789727\n",
      "Test on valid set: loss= 5.185770034790039 acc= 0.664936900138855 auc= 0.8315555555555557\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.8635e+02\n",
      "\n",
      "Test on train set: loss= 0.46491503715515137 acc= 0.9693266153335571 auc= 0.954711580439435\n",
      "Test on valid set: loss= 5.157796859741211 acc= 0.6799938082695007 auc= 0.8315555555555557\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.0212e+02\n",
      "\n",
      "Test on train set: loss= 0.4133230149745941 acc= 0.9728067517280579 auc= 0.9582933615217094\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8333333333333334\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1104e+02\n",
      "\n",
      "Test on train set: loss= 0.3779386878013611 acc= 0.9755584597587585 auc= 0.9601156271524498\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8333333333333334\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1948e+02\n",
      "\n",
      "Test on train set: loss= 0.3582651913166046 acc= 0.9762058854103088 auc= 0.9655075974198611\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8533333333333333\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2278e+02\n",
      "\n",
      "Test on train set: loss= 0.3609039783477783 acc= 0.9764487147331238 auc= 0.970816333163491\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.8626666666666667\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3465e+02\n",
      "\n",
      "Test on train set: loss= 0.35159653425216675 acc= 0.976529598236084 auc= 0.9736366111289485\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.7000000476837158 auc= 0.8635555555555555\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1507e+02\n",
      "\n",
      "Test on train set: loss= 0.35424602031707764 acc= 0.976610541343689 auc= 0.9733975469822196\n",
      "Test on valid set: loss= 4.462159633636475 acc= 0.713902473449707 auc= 0.8671111111111112\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2900e+02\n",
      "\n",
      "Test on train set: loss= 0.3530709743499756 acc= 0.9760439991950989 auc= 0.9767140051586999\n",
      "Test on valid set: loss= 4.236142635345459 acc= 0.7200215458869934 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2972e+02\n",
      "\n",
      "Test on train set: loss= 0.3643380403518677 acc= 0.9753156304359436 auc= 0.9771940261680478\n",
      "Test on valid set: loss= 4.119319915771484 acc= 0.7400000691413879 auc= 0.8771111111111111\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3748e+02\n",
      "\n",
      "Test on train set: loss= 0.35839003324508667 acc= 0.9761249423027039 auc= 0.9767998777203021\n",
      "Test on valid set: loss= 4.357646465301514 acc= 0.7200047373771667 auc= 0.8666666666666666\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.8842e+02\n",
      "\n",
      "Test on train set: loss= 0.3783642649650574 acc= 0.9745063185691833 auc= 0.9781188353120868\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.866\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.9599e+02\n",
      "\n",
      "Test on train set: loss= 0.42266079783439636 acc= 0.9707834124565125 auc= 0.9750162359761259\n",
      "Test on valid set: loss= 4.725438117980957 acc= 0.6932283043861389 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.8148e+02\n",
      "\n",
      "Test on train set: loss= 0.3935190737247467 acc= 0.973049521446228 auc= 0.9768303988055337\n",
      "Test on valid set: loss= 4.362796306610107 acc= 0.7190058827400208 auc= 0.8568888888888889\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.0263e+02\n",
      "\n",
      "Test on train set: loss= 0.3383549451828003 acc= 0.9781482815742493 auc= 0.9756615768633697\n",
      "Test on valid set: loss= 4.513913631439209 acc= 0.7191708087921143 auc= 0.8742222222222222\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1148e+02\n",
      "\n",
      "Test on train set: loss= 0.3368344008922577 acc= 0.9786338806152344 auc= 0.971801989391899\n",
      "Test on valid set: loss= 5.119022369384766 acc= 0.6800000667572021 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1501e+02\n",
      "\n",
      "Test on train set: loss= 0.3448821008205414 acc= 0.9777436256408691 auc= 0.9736356330022605\n",
      "Test on valid set: loss= 4.513239860534668 acc= 0.7198275327682495 auc= 0.853111111111111\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2290e+02\n",
      "\n",
      "Test on train set: loss= 0.34899699687957764 acc= 0.9775007963180542 auc= 0.975011569995772\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.7000000476837158 auc= 0.8622222222222222\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2842e+02\n",
      "\n",
      "Test on train set: loss= 0.3457604646682739 acc= 0.9776626825332642 auc= 0.9763201968829787\n",
      "Test on valid set: loss= 4.513067722320557 acc= 0.7199991345405579 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3607e+02\n",
      "\n",
      "Test on train set: loss= 0.31787559390068054 acc= 0.9795241355895996 auc= 0.9762950626308948\n",
      "Test on valid set: loss= 4.5140700340271 acc= 0.7190214395523071 auc= 0.8526666666666667\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3881e+02\n",
      "\n",
      "Test on train set: loss= 0.38852807879447937 acc= 0.9740207195281982 auc= 0.9776325030011902\n",
      "Test on valid set: loss= 3.5459814071655273 acc= 0.7799994945526123 auc= 0.897111111111111\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3918e+02\n",
      "\n",
      "Test on train set: loss= 0.3339349329471588 acc= 0.9781482815742493 auc= 0.9768077208802696\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8635555555555555\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -4.8951e+02\n",
      "\n",
      "Test on train set: loss= 0.3284493088722229 acc= 0.9783910512924194 auc= 0.9783455466242502\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8746666666666666\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.6589e+02\n",
      "\n",
      "Test on train set: loss= 0.34580034017562866 acc= 0.9774198532104492 auc= 0.9805985715993766\n",
      "Test on valid set: loss= 4.313502311706543 acc= 0.7200431227684021 auc= 0.8855555555555554\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.0659e+02\n",
      "\n",
      "Test on train set: loss= 0.3431268334388733 acc= 0.9777436256408691 auc= 0.9800395108759741\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8753333333333334\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.0372e+02\n",
      "\n",
      "Test on train set: loss= 0.36631539463996887 acc= 0.9754775166511536 auc= 0.9797673651737666\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8864444444444445\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -5.9905e+02\n",
      "\n",
      "Test on train set: loss= 0.5113846659660339 acc= 0.9645516276359558 auc= 0.9764616772619427\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8428888888888888\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.0742e+02\n",
      "\n",
      "Test on train set: loss= 0.48475101590156555 acc= 0.9667367935180664 auc= 0.9762670217535806\n",
      "Test on valid set: loss= 4.8355607986450195 acc= 0.6998685598373413 auc= 0.8426666666666668\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1107e+02\n",
      "\n",
      "Test on train set: loss= 0.3257553279399872 acc= 0.9786338806152344 auc= 0.9767495211298037\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2859e+02\n",
      "\n",
      "Test on train set: loss= 0.33438462018966675 acc= 0.9783101081848145 auc= 0.977417441091301\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.863111111111111\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3095e+02\n",
      "\n",
      "Test on train set: loss= 0.36018306016921997 acc= 0.976529598236084 auc= 0.9753101675869568\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8631111111111112\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3938e+02\n",
      "\n",
      "Test on train set: loss= 0.34252092242240906 acc= 0.9775817394256592 auc= 0.978687919818704\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8631111111111112\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3881e+02\n",
      "\n",
      "Test on train set: loss= 0.35599851608276367 acc= 0.9762868285179138 auc= 0.978294707672393\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8626666666666667\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.4175e+02\n",
      "\n",
      "Test on train set: loss= 0.32989275455474854 acc= 0.9782292246818542 auc= 0.98004851538893\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8642222222222223\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.4466e+02\n",
      "\n",
      "Test on train set: loss= 0.32924145460128784 acc= 0.9787148237228394 auc= 0.9789038668795472\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8626666666666667\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3840e+02\n",
      "\n",
      "Test on train set: loss= 0.3551623225212097 acc= 0.9764487147331238 auc= 0.9818427181373034\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8822222222222222\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.4173e+02\n",
      "\n",
      "Test on train set: loss= 0.326602965593338 acc= 0.9786338806152344 auc= 0.9785356027735309\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.4247e+02\n",
      "\n",
      "Test on train set: loss= 0.3286246359348297 acc= 0.9783910512924194 auc= 0.9811649203069305\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8657777777777778\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2092e+02\n",
      "\n",
      "Test on train set: loss= 0.3550068736076355 acc= 0.9761249423027039 auc= 0.979128652192627\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8528888888888888\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1483e+02\n",
      "\n",
      "Test on train set: loss= 0.35619932413101196 acc= 0.9758012294769287 auc= 0.9783115049025369\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.1494e+02\n",
      "\n",
      "Test on train set: loss= 0.4071276783943176 acc= 0.9727258086204529 auc= 0.9781295566229412\n",
      "Test on valid set: loss= 4.558789253234863 acc= 0.7020332217216492 auc= 0.8437777777777778\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2934e+02\n",
      "\n",
      "Test on train set: loss= 0.34111714363098145 acc= 0.9775817394256592 auc= 0.9756308301954209\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8655555555555556\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3993e+02\n",
      "\n",
      "Test on train set: loss= 0.3153366446495056 acc= 0.9792003631591797 auc= 0.9772370446019483\n",
      "Test on valid set: loss= 4.513067245483398 acc= 0.7199997901916504 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.4051e+02\n",
      "\n",
      "Test on train set: loss= 0.3206041157245636 acc= 0.9792813062667847 auc= 0.9779475897549625\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8635555555555555\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.4423e+02\n",
      "\n",
      "Test on train set: loss= 0.3572525382041931 acc= 0.976610541343689 auc= 0.980435589324337\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8642222222222223\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.4662e+02\n",
      "\n",
      "Test on train set: loss= 0.3373498320579529 acc= 0.9778245091438293 auc= 0.9798315395016551\n",
      "Test on valid set: loss= 4.469275951385498 acc= 0.7200000286102295 auc= 0.8751111111111112\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.2106e+02\n",
      "\n",
      "Test on train set: loss= 0.32416582107543945 acc= 0.9786338806152344 auc= 0.9809360093816168\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -6.3407e+02\n",
      "\n",
      "Test on train set: loss= 0.33934399485588074 acc= 0.9776626825332642 auc= 0.9800872174311943\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8635555555555555\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "metric_idx = 1\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_EEC(beginEncodingEpoch, endEncodingEpoch,  \n",
    "                                               codingEnhancementRate, mu,\n",
    "                                               train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Enhancement')\n",
    "                        \n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "enhancementWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 5s - loss: 1.6258\n",
      "\n",
      "Test on train set: loss= 1.2684370279312134 acc= 0.5868403911590576 auc= 0.7479441563008782\n",
      "Test on valid set: loss= 2.3826491832733154 acc= 0.18080157041549683 auc= 0.723111111111111\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 3s - loss: 1.8957\n",
      "\n",
      "Test on train set: loss= 1.914907693862915 acc= 0.6144383549690247 auc= 0.7638707966229192\n",
      "Test on valid set: loss= 5.060939311981201 acc= 0.2635937035083771 auc= 0.6724444444444444\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 3s - loss: 2.6265\n",
      "\n",
      "Test on train set: loss= 2.799982786178589 acc= 0.5958238840103149 auc= 0.7649961826260412\n",
      "Test on valid set: loss= 6.960259914398193 acc= 0.23299920558929443 auc= 0.6551111111111112\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 3s - loss: 3.1378\n",
      "\n",
      "Test on train set: loss= 2.504493236541748 acc= 0.6688248515129089 auc= 0.7978030122061266\n",
      "Test on valid set: loss= 7.327996730804443 acc= 0.2515842020511627 auc= 0.662\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 3s - loss: 4.0250\n",
      "\n",
      "Test on train set: loss= 3.718918800354004 acc= 0.634914219379425 auc= 0.7842886268392691\n",
      "Test on valid set: loss= 9.141695976257324 acc= 0.2833099961280823 auc= 0.67\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 4.6808\n",
      "\n",
      "Test on train set: loss= 4.18964958190918 acc= 0.619051456451416 auc= 0.8034350747931212\n",
      "Test on valid set: loss= 8.888388633728027 acc= 0.3173034191131592 auc= 0.7022222222222222\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 5.3063\n",
      "\n",
      "Test on train set: loss= 3.350097179412842 acc= 0.7016024589538574 auc= 0.8683954858613087\n",
      "Test on valid set: loss= 10.236538887023926 acc= 0.3026542067527771 auc= 0.7631111111111111\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 5.7108\n",
      "\n",
      "Test on train set: loss= 2.402569055557251 acc= 0.7514567971229553 auc= 0.9145612936036768\n",
      "Test on valid set: loss= 7.2012786865234375 acc= 0.41138386726379395 auc= 0.8708888888888889\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 5.5660\n",
      "\n",
      "Test on train set: loss= 3.4053714275360107 acc= 0.6719812154769897 auc= 0.8705937912697884\n",
      "Test on valid set: loss= 7.980261325836182 acc= 0.3680857717990875 auc= 0.8148888888888889\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 5.4539\n",
      "\n",
      "Test on train set: loss= 2.9442741870880127 acc= 0.6757850646972656 auc= 0.8570550169976873\n",
      "Test on valid set: loss= 7.158137798309326 acc= 0.39835992455482483 auc= 0.8031111111111111\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 6.0792\n",
      "\n",
      "Test on train set: loss= 4.776371955871582 acc= 0.5719488263130188 auc= 0.7897815871631868\n",
      "Test on valid set: loss= 8.000222206115723 acc= 0.40206941962242126 auc= 0.78\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 7.2427\n",
      "\n",
      "Test on train set: loss= 5.597087383270264 acc= 0.4801715910434723 auc= 0.8283742495733255\n",
      "Test on valid set: loss= 7.337419509887695 acc= 0.3966468274593353 auc= 0.804888888888889\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 10.3408\n",
      "\n",
      "Test on train set: loss= 3.976902723312378 acc= 0.7041922807693481 auc= 0.8149530537840111\n",
      "Test on valid set: loss= 8.950267791748047 acc= 0.354930579662323 auc= 0.8117777777777778\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 12.3945\n",
      "\n",
      "Test on train set: loss= 4.796387672424316 acc= 0.6581417918205261 auc= 0.7946826269652096\n",
      "Test on valid set: loss= 9.52294635772705 acc= 0.3792489171028137 auc= 0.7482222222222222\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 13.1097\n",
      "\n",
      "Test on train set: loss= 4.775004863739014 acc= 0.6353188753128052 auc= 0.7652616034716999\n",
      "Test on valid set: loss= 10.269591331481934 acc= 0.3564613461494446 auc= 0.7184444444444444\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 14.9775\n",
      "\n",
      "Test on train set: loss= 4.40323543548584 acc= 0.696422815322876 auc= 0.8147508732490024\n",
      "Test on valid set: loss= 9.330050468444824 acc= 0.37591731548309326 auc= 0.778\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.1306\n",
      "\n",
      "Test on train set: loss= 5.666232585906982 acc= 0.619132399559021 auc= 0.7512029942591505\n",
      "Test on valid set: loss= 11.804247856140137 acc= 0.25998455286026 auc= 0.7231111111111111\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.1541\n",
      "\n",
      "Test on train set: loss= 3.2944912910461426 acc= 0.7546941041946411 auc= 0.8752961015164493\n",
      "Test on valid set: loss= 7.836992263793945 acc= 0.47054523229599 auc= 0.8188888888888888\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.3858\n",
      "\n",
      "Test on train set: loss= 3.0686655044555664 acc= 0.776950478553772 auc= 0.9245352530295428\n",
      "Test on valid set: loss= 8.247097969055176 acc= 0.4547765254974365 auc= 0.850888888888889\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.9852\n",
      "\n",
      "Test on train set: loss= 3.8607757091522217 acc= 0.730657160282135 auc= 0.8023626409202762\n",
      "Test on valid set: loss= 10.371798515319824 acc= 0.30831584334373474 auc= 0.7342222222222222\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 18.3986\n",
      "\n",
      "Test on train set: loss= 4.1475605964660645 acc= 0.7227258086204529 auc= 0.7847377739581481\n",
      "Test on valid set: loss= 9.392682075500488 acc= 0.3999209702014923 auc= 0.7953333333333333\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.5927\n",
      "\n",
      "Test on train set: loss= 3.571132183074951 acc= 0.7489478588104248 auc= 0.8532346835394872\n",
      "Test on valid set: loss= 9.185491561889648 acc= 0.3837029039859772 auc= 0.7977777777777778\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.7671\n",
      "\n",
      "Test on train set: loss= 2.6509480476379395 acc= 0.8118323087692261 auc= 0.9022190509354713\n",
      "Test on valid set: loss= 6.300052642822266 acc= 0.5997945070266724 auc= 0.8613333333333333\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 18.6150\n",
      "\n",
      "Test on train set: loss= 3.1590561866760254 acc= 0.7829394340515137 auc= 0.8681580411725905\n",
      "Test on valid set: loss= 7.055129528045654 acc= 0.5394654273986816 auc= 0.8568888888888889\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.8171\n",
      "\n",
      "Test on train set: loss= 3.085806369781494 acc= 0.7846390604972839 auc= 0.8944911440883029\n",
      "Test on valid set: loss= 5.493140697479248 acc= 0.5855737328529358 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.5675\n",
      "\n",
      "Test on train set: loss= 3.322313070297241 acc= 0.7647296786308289 auc= 0.8737846571923461\n",
      "Test on valid set: loss= 7.316041469573975 acc= 0.5201151371002197 auc= 0.8413333333333334\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.1418\n",
      "\n",
      "Test on train set: loss= 3.02885103225708 acc= 0.7852864861488342 auc= 0.8409394145864326\n",
      "Test on valid set: loss= 8.066657066345215 acc= 0.4636922776699066 auc= 0.8377777777777778\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.8096\n",
      "\n",
      "Test on train set: loss= 5.2098517417907715 acc= 0.649967610836029 auc= 0.7997560538586121\n",
      "Test on valid set: loss= 8.360546112060547 acc= 0.46013951301574707 auc= 0.7815555555555556\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 14.6288\n",
      "\n",
      "Test on train set: loss= 2.2657179832458496 acc= 0.8315798044204712 auc= 0.9086738768324123\n",
      "Test on valid set: loss= 7.08212423324585 acc= 0.5335244536399841 auc= 0.858\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 18.5163\n",
      "\n",
      "Test on train set: loss= 3.1838035583496094 acc= 0.7822110652923584 auc= 0.8820181817467334\n",
      "Test on valid set: loss= 6.447595596313477 acc= 0.542746365070343 auc= 0.8633333333333333\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.2078\n",
      "\n",
      "Test on train set: loss= 3.3815884590148926 acc= 0.773308515548706 auc= 0.8519030926538524\n",
      "Test on valid set: loss= 6.203789234161377 acc= 0.6000009775161743 auc= 0.8595555555555554\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.9105\n",
      "\n",
      "Test on train set: loss= 2.6470117568969727 acc= 0.8143411874771118 auc= 0.9046773260221951\n",
      "Test on valid set: loss= 7.947949409484863 acc= 0.4804624617099762 auc= 0.8748888888888888\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.7394\n",
      "\n",
      "Test on train set: loss= 2.3903465270996094 acc= 0.8354645371437073 auc= 0.9134290429315021\n",
      "Test on valid set: loss= 7.053459644317627 acc= 0.5597354173660278 auc= 0.8559999999999999\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.2076\n",
      "\n",
      "Test on train set: loss= 3.538590908050537 acc= 0.7638394236564636 auc= 0.8616353576673381\n",
      "Test on valid set: loss= 6.447762489318848 acc= 0.599482536315918 auc= 0.8213333333333332\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.8257\n",
      "\n",
      "Test on train set: loss= 3.103109121322632 acc= 0.7879572510719299 auc= 0.8970046993517362\n",
      "Test on valid set: loss= 4.999388694763184 acc= 0.6653023958206177 auc= 0.8828888888888888\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.4008\n",
      "\n",
      "Test on train set: loss= 2.601085662841797 acc= 0.8240531086921692 auc= 0.8848606526056757\n",
      "Test on valid set: loss= 7.414323806762695 acc= 0.5399999618530273 auc= 0.8177777777777779\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.6188\n",
      "\n",
      "Test on train set: loss= 2.4235079288482666 acc= 0.8275331854820251 auc= 0.9234418143282056\n",
      "Test on valid set: loss= 5.01411247253418 acc= 0.6645883917808533 auc= 0.8851111111111111\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.2039\n",
      "\n",
      "Test on train set: loss= 2.255363941192627 acc= 0.8492230772972107 auc= 0.8547416229614242\n",
      "Test on valid set: loss= 7.1702561378479 acc= 0.5366854071617126 auc= 0.8037777777777778\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.9855\n",
      "\n",
      "Test on train set: loss= 2.3396193981170654 acc= 0.8361120223999023 auc= 0.906357727132628\n",
      "Test on valid set: loss= 6.7696027755737305 acc= 0.579997181892395 auc= 0.8677777777777778\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 18.4748\n",
      "\n",
      "Test on train set: loss= 2.555222988128662 acc= 0.8251861333847046 auc= 0.896967084580246\n",
      "Test on valid set: loss= 6.519244194030762 acc= 0.5638166666030884 auc= 0.8888888888888887\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.0257\n",
      "\n",
      "Test on train set: loss= 2.37668776512146 acc= 0.8333603143692017 auc= 0.9135195928432592\n",
      "Test on valid set: loss= 6.3892822265625 acc= 0.5805869698524475 auc= 0.8615555555555556\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.9282\n",
      "\n",
      "Test on train set: loss= 2.511857032775879 acc= 0.8300420641899109 auc= 0.9069423029920385\n",
      "Test on valid set: loss= 4.829805850982666 acc= 0.6800188422203064 auc= 0.8808888888888889\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.7682\n",
      "\n",
      "Test on train set: loss= 2.0556323528289795 acc= 0.8584493398666382 auc= 0.9001266592877644\n",
      "Test on valid set: loss= 6.765025615692139 acc= 0.562524139881134 auc= 0.826\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.3911\n",
      "\n",
      "Test on train set: loss= 1.8666174411773682 acc= 0.8699417114257812 auc= 0.92749633956042\n",
      "Test on valid set: loss= 7.226541519165039 acc= 0.520005464553833 auc= 0.8871111111111112\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.5441\n",
      "\n",
      "Test on train set: loss= 1.926430106163025 acc= 0.8582065105438232 auc= 0.905745065553021\n",
      "Test on valid set: loss= 6.678311824798584 acc= 0.522854745388031 auc= 0.8615555555555556\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.9693\n",
      "\n",
      "Test on train set: loss= 3.848747491836548 acc= 0.7316283583641052 auc= 0.8941017658368751\n",
      "Test on valid set: loss= 4.132814884185791 acc= 0.7318592667579651 auc= 0.9046666666666667\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.9607\n",
      "\n",
      "Test on train set: loss= 3.6210265159606934 acc= 0.7515377402305603 auc= 0.8802173186951856\n",
      "Test on valid set: loss= 6.1289873123168945 acc= 0.5856478214263916 auc= 0.844\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.3331\n",
      "\n",
      "Test on train set: loss= 1.8292255401611328 acc= 0.8748785853385925 auc= 0.9287553134879204\n",
      "Test on valid set: loss= 5.555245876312256 acc= 0.6229896545410156 auc= 0.886888888888889\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.1144\n",
      "\n",
      "Test on train set: loss= 2.357123851776123 acc= 0.827128529548645 auc= 0.9041524972969869\n",
      "Test on valid set: loss= 6.094660758972168 acc= 0.5903382301330566 auc= 0.8462222222222222\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.2705\n",
      "\n",
      "Test on train set: loss= 2.108607292175293 acc= 0.8535124659538269 auc= 0.8905611056287193\n",
      "Test on valid set: loss= 6.795142650604248 acc= 0.5609146356582642 auc= 0.8220000000000001\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.3781\n",
      "\n",
      "Test on train set: loss= 2.1771469116210938 acc= 0.8501942157745361 auc= 0.9094466398527142\n",
      "Test on valid set: loss= 7.527563571929932 acc= 0.5200005769729614 auc= 0.8335555555555556\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.8572\n",
      "\n",
      "Test on train set: loss= 2.0939340591430664 acc= 0.8542408347129822 auc= 0.9205707158761041\n",
      "Test on valid set: loss= 5.274343490600586 acc= 0.6526722311973572 auc= 0.8511111111111112\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 24.3038\n",
      "\n",
      "Test on train set: loss= 2.2046103477478027 acc= 0.8513273000717163 auc= 0.907332490505954\n",
      "Test on valid set: loss= 6.047204971313477 acc= 0.6195811629295349 auc= 0.8715555555555555\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.6733\n",
      "\n",
      "Test on train set: loss= 2.6132192611694336 acc= 0.8226772546768188 auc= 0.8556926519849121\n",
      "Test on valid set: loss= 7.396670341491699 acc= 0.48318901658058167 auc= 0.7917777777777777\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.7594\n",
      "\n",
      "Test on train set: loss= 2.369831085205078 acc= 0.8395111560821533 auc= 0.8964754171463735\n",
      "Test on valid set: loss= 7.200550556182861 acc= 0.5400012135505676 auc= 0.8122222222222222\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.3252\n",
      "\n",
      "Test on train set: loss= 7.851236343383789 acc= 0.5071220397949219 auc= 0.6353879654809005\n",
      "Test on valid set: loss= 10.637943267822266 acc= 0.3400000035762787 auc= 0.632\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 35.0450\n",
      "\n",
      "Test on train set: loss= 2.2439823150634766 acc= 0.8511654138565063 auc= 0.878374739448639\n",
      "Test on valid set: loss= 6.4472455978393555 acc= 0.5999928712844849 auc= 0.8308888888888888\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 35.2762\n",
      "\n",
      "Test on train set: loss= 2.2831108570098877 acc= 0.8520556688308716 auc= 0.8713878935310732\n",
      "Test on valid set: loss= 5.364675521850586 acc= 0.6560841798782349 auc= 0.8622222222222222\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.0813\n",
      "\n",
      "Test on train set: loss= 2.3947324752807617 acc= 0.8429103493690491 auc= 0.8779878454783103\n",
      "Test on valid set: loss= 7.154971122741699 acc= 0.5408565998077393 auc= 0.7882222222222223\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.2596\n",
      "\n",
      "Test on train set: loss= 2.491565227508545 acc= 0.8357073664665222 auc= 0.9201756896232007\n",
      "Test on valid set: loss= 6.363050937652588 acc= 0.5824412107467651 auc= 0.849777777777778\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.9781\n",
      "\n",
      "Test on train set: loss= 1.841615080833435 acc= 0.8788442611694336 auc= 0.9065654033862222\n",
      "Test on valid set: loss= 5.190216064453125 acc= 0.6636412143707275 auc= 0.8697777777777779\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.1576\n",
      "\n",
      "Test on train set: loss= 2.7906453609466553 acc= 0.8174975514411926 auc= 0.8996328413352194\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.64000004529953 auc= 0.8846666666666666\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.5354\n",
      "\n",
      "Test on train set: loss= 1.7874773740768433 acc= 0.8804629445075989 auc= 0.9243032518116413\n",
      "Test on valid set: loss= 5.651046276092529 acc= 0.6194397807121277 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.0788\n",
      "\n",
      "Test on train set: loss= 1.9318715333938599 acc= 0.8728553056716919 auc= 0.8948661749683187\n",
      "Test on valid set: loss= 5.660086154937744 acc= 0.6400023102760315 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.7329\n",
      "\n",
      "Test on train set: loss= 2.2100298404693604 acc= 0.8530268669128418 auc= 0.9070630737526091\n",
      "Test on valid set: loss= 5.6589460372924805 acc= 0.6348069906234741 auc= 0.8795555555555555\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.8748\n",
      "\n",
      "Test on train set: loss= 1.7646677494049072 acc= 0.880705714225769 auc= 0.9247393936395418\n",
      "Test on valid set: loss= 5.776305675506592 acc= 0.6292654275894165 auc= 0.8375555555555556\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.6860\n",
      "\n",
      "Test on train set: loss= 3.1508326530456543 acc= 0.7901424169540405 auc= 0.8857177687924842\n",
      "Test on valid set: loss= 7.091961860656738 acc= 0.5600000023841858 auc= 0.8071111111111111\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.9952\n",
      "\n",
      "Test on train set: loss= 2.236743211746216 acc= 0.8433150053024292 auc= 0.9137058901554143\n",
      "Test on valid set: loss= 5.58071231842041 acc= 0.6175206899642944 auc= 0.8668888888888888\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.2957\n",
      "\n",
      "Test on train set: loss= 2.3768723011016846 acc= 0.8427484631538391 auc= 0.8934047248633739\n",
      "Test on valid set: loss= 6.770655632019043 acc= 0.578971803188324 auc= 0.8577777777777778\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.8221\n",
      "\n",
      "Test on train set: loss= 2.0761871337890625 acc= 0.861605703830719 auc= 0.8931882136091472\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.805111111111111\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.6698\n",
      "\n",
      "Test on train set: loss= 2.308530807495117 acc= 0.8429103493690491 auc= 0.9079553444117858\n",
      "Test on valid set: loss= 5.046381950378418 acc= 0.6729599237442017 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.5361\n",
      "\n",
      "Test on train set: loss= 2.003849983215332 acc= 0.8666235208511353 auc= 0.9182067179757544\n",
      "Test on valid set: loss= 5.870306491851807 acc= 0.6205602884292603 auc= 0.8859999999999999\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.4573\n",
      "\n",
      "Test on train set: loss= 1.6254256963729858 acc= 0.8937358260154724 auc= 0.9120285721436131\n",
      "Test on valid set: loss= 6.840742111206055 acc= 0.5605605840682983 auc= 0.8308888888888889\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.9333\n",
      "\n",
      "Test on train set: loss= 2.0751538276672363 acc= 0.8636289834976196 auc= 0.899592633590164\n",
      "Test on valid set: loss= 6.198934555053711 acc= 0.6000252962112427 auc= 0.874\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.0430\n",
      "\n",
      "Test on train set: loss= 2.235992431640625 acc= 0.8548073768615723 auc= 0.873834492318033\n",
      "Test on valid set: loss= 6.3051252365112305 acc= 0.6000024080276489 auc= 0.8246666666666667\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.6701\n",
      "\n",
      "Test on train set: loss= 1.9440900087356567 acc= 0.8722078204154968 auc= 0.9088764135322178\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8386666666666667\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.3246\n",
      "\n",
      "Test on train set: loss= 2.090000629425049 acc= 0.8590967655181885 auc= 0.9006304708563384\n",
      "Test on valid set: loss= 4.90389347076416 acc= 0.6784473657608032 auc= 0.831111111111111\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.5001\n",
      "\n",
      "Test on train set: loss= 1.7581340074539185 acc= 0.8857235312461853 auc= 0.8971498314396318\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8495555555555555\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.2098\n",
      "\n",
      "Test on train set: loss= 1.783496618270874 acc= 0.8826481103897095 auc= 0.9396126752563658\n",
      "Test on valid set: loss= 5.865785121917725 acc= 0.6208454966545105 auc= 0.836\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.3276\n",
      "\n",
      "Test on train set: loss= 1.6973166465759277 acc= 0.8875849843025208 auc= 0.9395633439093425\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7399999499320984 auc= 0.8946666666666665\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.1571\n",
      "\n",
      "Test on train set: loss= 1.6890841722488403 acc= 0.8901748061180115 auc= 0.9066139562514494\n",
      "Test on valid set: loss= 4.860062122344971 acc= 0.6840410828590393 auc= 0.8295555555555556\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.4240\n",
      "\n",
      "Test on train set: loss= 1.7446759939193726 acc= 0.8861281871795654 auc= 0.9184703698823513\n",
      "Test on valid set: loss= 5.502667427062988 acc= 0.6401175856590271 auc= 0.8575555555555555\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.9164\n",
      "\n",
      "Test on train set: loss= 1.6330463886260986 acc= 0.8923599720001221 auc= 0.9157371084823955\n",
      "Test on valid set: loss= 5.657398700714111 acc= 0.6400028467178345 auc= 0.82\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.1684\n",
      "\n",
      "Test on train set: loss= 1.4434770345687866 acc= 0.9053900837898254 auc= 0.9258705964043369\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8613333333333333\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.9229\n",
      "\n",
      "Test on train set: loss= 1.6961296796798706 acc= 0.8904985189437866 auc= 0.8993023125938574\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8333333333333334\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.4863\n",
      "\n",
      "Test on train set: loss= 1.9273812770843506 acc= 0.873259961605072 auc= 0.8953134440806266\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.5999999642372131 auc= 0.8420000000000002\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.9385\n",
      "\n",
      "Test on train set: loss= 1.517126202583313 acc= 0.9021527767181396 auc= 0.9015689808167892\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.828888888888889\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.3998\n",
      "\n",
      "Test on train set: loss= 1.5051580667495728 acc= 0.9024765491485596 auc= 0.9129980068392876\n",
      "Test on valid set: loss= 5.756717681884766 acc= 0.6391773819923401 auc= 0.8195555555555554\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.8619\n",
      "\n",
      "Test on train set: loss= 1.5299103260040283 acc= 0.8990773558616638 auc= 0.9138436100035555\n",
      "Test on valid set: loss= 5.744264125823975 acc= 0.64000004529953 auc= 0.8193333333333334\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.4994\n",
      "\n",
      "Test on train set: loss= 2.078395366668701 acc= 0.8632243275642395 auc= 0.8697936763179506\n",
      "Test on valid set: loss= 6.956348419189453 acc= 0.5592726469039917 auc= 0.8046666666666666\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.5160\n",
      "\n",
      "Test on train set: loss= 1.3658299446105957 acc= 0.9100841879844666 auc= 0.9054411202346735\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.8417777777777777\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.3896\n",
      "\n",
      "Test on train set: loss= 1.6393686532974243 acc= 0.8911460041999817 auc= 0.9163817071420393\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8364444444444444\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.2277\n",
      "\n",
      "Test on train set: loss= 1.4759533405303955 acc= 0.9015862941741943 auc= 0.8998553253785675\n",
      "Test on valid set: loss= 5.833250522613525 acc= 0.6205751299858093 auc= 0.8284444444444444\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.3743\n",
      "\n",
      "Test on train set: loss= 2.2231640815734863 acc= 0.8563451170921326 auc= 0.8839554855288749\n",
      "Test on valid set: loss= 5.178591251373291 acc= 0.6670690178871155 auc= 0.8320000000000001\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.8734\n",
      "\n",
      "Test on train set: loss= 1.7691439390182495 acc= 0.8859663605690002 auc= 0.9000158558782617\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8057777777777778\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.3362\n",
      "\n",
      "Test on train set: loss= 1.5889012813568115 acc= 0.8966494202613831 auc= 0.9259991803933584\n",
      "Test on valid set: loss= 5.189795017242432 acc= 0.6636289954185486 auc= 0.8393333333333333\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.6245\n",
      "\n",
      "Test on train set: loss= 1.4788753986358643 acc= 0.9027193188667297 auc= 0.9309068086620444\n",
      "Test on valid set: loss= 5.773094654083252 acc= 0.6206259727478027 auc= 0.8351111111111111\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.4922\n",
      "\n",
      "Test on train set: loss= 1.6423134803771973 acc= 0.8909841179847717 auc= 0.8919697534770317\n",
      "Test on valid set: loss= 5.941852569580078 acc= 0.6200186014175415 auc= 0.8068888888888889\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.5994\n",
      "\n",
      "Test on train set: loss= 1.6254814863204956 acc= 0.892683744430542 auc= 0.9261360768374773\n",
      "Test on valid set: loss= 5.763253211975098 acc= 0.6399999856948853 auc= 0.8728888888888889\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.3072\n",
      "\n",
      "Test on train set: loss= 1.4207029342651367 acc= 0.9080608487129211 auc= 0.9283704522497157\n",
      "Test on valid set: loss= 3.224257230758667 acc= 0.7993720769882202 auc= 0.897777777777778\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.2757\n",
      "\n",
      "Test on train set: loss= 1.2059211730957031 acc= 0.9207672476768494 auc= 0.9301427521773389\n",
      "Test on valid set: loss= 6.124876022338867 acc= 0.6200000047683716 auc= 0.8268888888888888\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.8112\n",
      "\n",
      "Test on train set: loss= 1.1144033670425415 acc= 0.9260278344154358 auc= 0.9327108646552407\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8304444444444444\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.1061\n",
      "\n",
      "Test on train set: loss= 1.4516878128051758 acc= 0.9057947397232056 auc= 0.933319569992824\n",
      "Test on valid set: loss= 4.124889373779297 acc= 0.7400000691413879 auc= 0.8751111111111112\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.6912\n",
      "\n",
      "Test on train set: loss= 1.3031693696975708 acc= 0.9157494306564331 auc= 0.9207973756840756\n",
      "Test on valid set: loss= 6.7696003913879395 acc= 0.5800000429153442 auc= 0.812\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.4361\n",
      "\n",
      "Test on train set: loss= 1.4804773330688477 acc= 0.903609573841095 auc= 0.9170390894118544\n",
      "Test on valid set: loss= 5.1492838859558105 acc= 0.679999589920044 auc= 0.8700000000000001\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.3181\n",
      "\n",
      "Test on train set: loss= 2.003390073776245 acc= 0.8701035976409912 auc= 0.9065896621556842\n",
      "Test on valid set: loss= 5.173133373260498 acc= 0.6692476868629456 auc= 0.8306666666666667\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.9087\n",
      "\n",
      "Test on train set: loss= 1.3732879161834717 acc= 0.9105697870254517 auc= 0.9342122859719003\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8366666666666667\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.4929\n",
      "\n",
      "Test on train set: loss= 1.6241333484649658 acc= 0.8945451378822327 auc= 0.9376106580458335\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8297777777777778\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.1945\n",
      "\n",
      "Test on train set: loss= 1.4629440307617188 acc= 0.9054710268974304 auc= 0.9304224686106878\n",
      "Test on valid set: loss= 6.7696003913879395 acc= 0.5799999833106995 auc= 0.8002222222222223\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.1317\n",
      "\n",
      "Test on train set: loss= 1.9376249313354492 acc= 0.8769828677177429 auc= 0.8981846905161543\n",
      "Test on valid set: loss= 6.782291889190674 acc= 0.5600159764289856 auc= 0.782888888888889\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.7936\n",
      "\n",
      "Test on train set: loss= 1.4581221342086792 acc= 0.9058756828308105 auc= 0.9350420412855552\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.6999999284744263 auc= 0.8615555555555556\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 35.6477\n",
      "\n",
      "Test on train set: loss= 1.3793514966964722 acc= 0.9115409255027771 auc= 0.9448946707910231\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8937777777777779\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.8242\n",
      "\n",
      "Test on train set: loss= 1.4023175239562988 acc= 0.9096795320510864 auc= 0.8958279155228583\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8506666666666666\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.6064\n",
      "\n",
      "Test on train set: loss= 1.5666306018829346 acc= 0.8998866677284241 auc= 0.9428289068028448\n",
      "Test on valid set: loss= 5.80251407623291 acc= 0.6399999856948853 auc= 0.828888888888889\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 29.8026\n",
      "\n",
      "Test on train set: loss= 2.0068724155426025 acc= 0.8715603947639465 auc= 0.8936284132830921\n",
      "Test on valid set: loss= 6.488056659698486 acc= 0.5796622037887573 auc= 0.7946666666666666\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.7719\n",
      "\n",
      "Test on train set: loss= 1.4477976560592651 acc= 0.907494306564331 auc= 0.9233348427499765\n",
      "Test on valid set: loss= 4.837218761444092 acc= 0.6982871890068054 auc= 0.8502222222222222\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 37.5706\n",
      "\n",
      "Test on train set: loss= 1.2834866046905518 acc= 0.9180964827537537 auc= 0.9210581433315772\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8213333333333332\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.0039\n",
      "\n",
      "Test on train set: loss= 1.3274530172348022 acc= 0.9149401187896729 auc= 0.9228017182287596\n",
      "Test on valid set: loss= 5.858119010925293 acc= 0.621240496635437 auc= 0.8262222222222222\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.7835\n",
      "\n",
      "Test on train set: loss= 1.5828102827072144 acc= 0.8983489871025085 auc= 0.894525384917913\n",
      "Test on valid set: loss= 6.447238922119141 acc= 0.5999999642372131 auc= 0.7939999999999999\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.1239\n",
      "\n",
      "Test on train set: loss= 1.4652880430221558 acc= 0.9059566259384155 auc= 0.9240668072721878\n",
      "Test on valid set: loss= 6.153364181518555 acc= 0.6048129796981812 auc= 0.8066666666666666\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.1841\n",
      "\n",
      "Test on train set: loss= 1.2590118646621704 acc= 0.9189867377281189 auc= 0.9390723323221613\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8304444444444444\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.3635\n",
      "\n",
      "Test on train set: loss= 1.32652747631073 acc= 0.9158303737640381 auc= 0.9300588675699248\n",
      "Test on valid set: loss= 5.230717658996582 acc= 0.6605216860771179 auc= 0.8331111111111111\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.1653\n",
      "\n",
      "Test on train set: loss= 1.0915331840515137 acc= 0.9299125671386719 auc= 0.9292274227813666\n",
      "Test on valid set: loss= 5.824211597442627 acc= 0.6195412278175354 auc= 0.8375555555555554\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.7774\n",
      "\n",
      "Test on train set: loss= 1.3035762310028076 acc= 0.9167206287384033 auc= 0.907274885538874\n",
      "Test on valid set: loss= 7.0334296226501465 acc= 0.5599997043609619 auc= 0.792888888888889\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.1511\n",
      "\n",
      "Test on train set: loss= 1.2308225631713867 acc= 0.9209290742874146 auc= 0.9249728360116954\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8593333333333334\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.2925\n",
      "\n",
      "Test on train set: loss= 1.247748851776123 acc= 0.9199579358100891 auc= 0.9058084763334238\n",
      "Test on valid set: loss= 6.224193096160889 acc= 0.6000677347183228 auc= 0.788\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.2075\n",
      "\n",
      "Test on train set: loss= 1.578128457069397 acc= 0.8991582989692688 auc= 0.9387292672244711\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8177777777777777\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.0357\n",
      "\n",
      "Test on train set: loss= 1.2085285186767578 acc= 0.9227096438407898 auc= 0.9253180394305789\n",
      "Test on valid set: loss= 4.520461559295654 acc= 0.7138184309005737 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.2220\n",
      "\n",
      "Test on train set: loss= 1.279247522354126 acc= 0.9182583093643188 auc= 0.9351747345630999\n",
      "Test on valid set: loss= 4.587557792663574 acc= 0.7004824876785278 auc= 0.8526666666666667\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.6314\n",
      "\n",
      "Test on train set: loss= 1.3839212656021118 acc= 0.9116218686103821 auc= 0.929441494511097\n",
      "Test on valid set: loss= 6.124908924102783 acc= 0.6199669241905212 auc= 0.8068888888888889\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.3197\n",
      "\n",
      "Test on train set: loss= 1.3882169723510742 acc= 0.9102460145950317 auc= 0.9280763265415566\n",
      "Test on valid set: loss= 5.8025221824646 acc= 0.6399925947189331 auc= 0.8075555555555555\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.3661\n",
      "\n",
      "Test on train set: loss= 1.362741231918335 acc= 0.911783754825592 auc= 0.9350970207935522\n",
      "Test on valid set: loss= 6.125299453735352 acc= 0.6195810437202454 auc= 0.8068888888888889\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.0897\n",
      "\n",
      "Test on train set: loss= 1.3437485694885254 acc= 0.9146164059638977 auc= 0.9373007710414539\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8322222222222223\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.1185\n",
      "\n",
      "Test on train set: loss= 1.3076714277267456 acc= 0.9169633984565735 auc= 0.921998059578673\n",
      "Test on valid set: loss= 5.808194637298584 acc= 0.6350554823875427 auc= 0.808\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.4434\n",
      "\n",
      "Test on train set: loss= 0.9416033625602722 acc= 0.9393007159233093 auc= 0.9500297868344605\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.7979999999999999\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.8646\n",
      "\n",
      "Test on train set: loss= 1.0914949178695679 acc= 0.9295888543128967 auc= 0.9432578744486145\n",
      "Test on valid set: loss= 5.480168342590332 acc= 0.6599841117858887 auc= 0.8195555555555556\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.4768\n",
      "\n",
      "Test on train set: loss= 1.1022591590881348 acc= 0.9291032552719116 auc= 0.9499225567183146\n",
      "Test on valid set: loss= 5.852442741394043 acc= 0.6216475367546082 auc= 0.808\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 31.0757\n",
      "\n",
      "Test on train set: loss= 0.9083985686302185 acc= 0.9418096542358398 auc= 0.9289659765340857\n",
      "Test on valid set: loss= 6.124944686889648 acc= 0.6199321150779724 auc= 0.8166666666666667\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.6648\n",
      "\n",
      "Test on train set: loss= 1.0154969692230225 acc= 0.9353350400924683 auc= 0.9206015403284523\n",
      "Test on valid set: loss= 6.7696003913879395 acc= 0.5799999833106995 auc= 0.7857777777777779\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 36.5469\n",
      "\n",
      "Test on train set: loss= 1.0981022119522095 acc= 0.9291841983795166 auc= 0.9400765326185535\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.839111111111111\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.2637\n",
      "\n",
      "Test on train set: loss= 0.9726327657699585 acc= 0.9376011490821838 auc= 0.9539971608018536\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000667572021 auc= 0.8324444444444445\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.3562\n",
      "\n",
      "Test on train set: loss= 1.7576050758361816 acc= 0.8872612714767456 auc= 0.9253077143006969\n",
      "Test on valid set: loss= 5.544363021850586 acc= 0.6408048868179321 auc= 0.8302222222222223\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.1606\n",
      "\n",
      "Test on train set: loss= 1.1140576601028442 acc= 0.9285367727279663 auc= 0.9313498739783983\n",
      "Test on valid set: loss= 5.80251407623291 acc= 0.6399999856948853 auc= 0.8184444444444445\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.6714\n",
      "\n",
      "Test on train set: loss= 1.01750910282135 acc= 0.9348494410514832 auc= 0.9349573181714463\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.7975555555555556\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.1047\n",
      "\n",
      "Test on train set: loss= 1.0610675811767578 acc= 0.9316930770874023 auc= 0.9373250062879102\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8535555555555556\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.1288\n",
      "\n",
      "Test on train set: loss= 1.0642225742340088 acc= 0.9308837652206421 auc= 0.9408284996513615\n",
      "Test on valid set: loss= 6.447238922119141 acc= 0.6000000238418579 auc= 0.8066666666666666\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.2914\n",
      "\n",
      "Test on train set: loss= 0.8093072175979614 acc= 0.9467465281486511 auc= 0.9617039229891933\n",
      "Test on valid set: loss= 4.932345390319824 acc= 0.6801572442054749 auc= 0.8524444444444444\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.2620\n",
      "\n",
      "Test on train set: loss= 1.0529578924179077 acc= 0.9323405623435974 auc= 0.9428741827402976\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.821111111111111\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.5723\n",
      "\n",
      "Test on train set: loss= 0.7894031405448914 acc= 0.9492554068565369 auc= 0.9636338371719017\n",
      "Test on valid set: loss= 4.382591247558594 acc= 0.7198587656021118 auc= 0.8746666666666668\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.7244\n",
      "\n",
      "Test on train set: loss= 1.0874007940292358 acc= 0.930398166179657 auc= 0.9229867314988752\n",
      "Test on valid set: loss= 7.091961860656738 acc= 0.559999942779541 auc= 0.7942222222222223\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.4461\n",
      "\n",
      "Test on train set: loss= 1.1145339012145996 acc= 0.9282939434051514 auc= 0.9515292246513789\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5800000429153442 auc= 0.7746666666666666\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 37.6054\n",
      "\n",
      "Test on train set: loss= 1.3540204763412476 acc= 0.9144545197486877 auc= 0.9516732997491444\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.2257\n",
      "\n",
      "Test on train set: loss= 0.9423502683639526 acc= 0.9395435452461243 auc= 0.9512049986496194\n",
      "Test on valid set: loss= 5.480216026306152 acc= 0.6599365472793579 auc= 0.8111111111111111\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 36.2532\n",
      "\n",
      "Test on train set: loss= 1.2490652799606323 acc= 0.9201197624206543 auc= 0.9427334923727109\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.844\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.2311\n",
      "\n",
      "Test on train set: loss= 0.9368500113487244 acc= 0.9398672580718994 auc= 0.9600604318116799\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999260902405 auc= 0.8197777777777778\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.1695\n",
      "\n",
      "Test on train set: loss= 1.1143419742584229 acc= 0.9286176562309265 auc= 0.9466871923833888\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000667572021 auc= 0.8324444444444445\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 36.2291\n",
      "\n",
      "Test on train set: loss= 1.313908576965332 acc= 0.9159113168716431 auc= 0.9308472496384533\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.64000004529953 auc= 0.8286666666666667\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.5308\n",
      "\n",
      "Test on train set: loss= 0.875551164150238 acc= 0.9437520503997803 auc= 0.9343534175648711\n",
      "Test on valid set: loss= 5.491288661956787 acc= 0.6514607071876526 auc= 0.8104444444444445\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.4742\n",
      "\n",
      "Test on train set: loss= 1.5565868616104126 acc= 0.8998057842254639 auc= 0.9404963364692204\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.6999999284744263 auc= 0.8326666666666667\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.4637\n",
      "\n",
      "Test on train set: loss= 0.7705340385437012 acc= 0.9502266049385071 auc= 0.9531111811981685\n",
      "Test on valid set: loss= 6.124881744384766 acc= 0.6199944019317627 auc= 0.8073333333333332\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.9750\n",
      "\n",
      "Test on train set: loss= 1.1017314195632935 acc= 0.9272418022155762 auc= 0.9567646195243139\n",
      "Test on valid set: loss= 4.835432052612305 acc= 0.6999965906143188 auc= 0.8428888888888888\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.8116\n",
      "\n",
      "Test on train set: loss= 1.0505571365356445 acc= 0.9315312504768372 auc= 0.9414033186753027\n",
      "Test on valid set: loss= 7.091961860656738 acc= 0.5600000023841858 auc= 0.7842222222222223\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.9032\n",
      "\n",
      "Test on train set: loss= 0.8222364783287048 acc= 0.9472321271896362 auc= 0.9543403616446282\n",
      "Test on valid set: loss= 3.8784661293029785 acc= 0.7520540952682495 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.7594\n",
      "\n",
      "Test on train set: loss= 1.0229617357254028 acc= 0.9352541565895081 auc= 0.9404817346228519\n",
      "Test on valid set: loss= 4.269589900970459 acc= 0.72038733959198 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.4015\n",
      "\n",
      "Test on train set: loss= 0.896237313747406 acc= 0.9423761963844299 auc= 0.9375878468622023\n",
      "Test on valid set: loss= 5.810809135437012 acc= 0.6332100629806519 auc= 0.8084444444444445\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.1108\n",
      "\n",
      "Test on train set: loss= 1.1817981004714966 acc= 0.9248138666152954 auc= 0.9161991169975604\n",
      "Test on valid set: loss= 6.136455535888672 acc= 0.6112095713615417 auc= 0.7980000000000002\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.1381\n",
      "\n",
      "Test on train set: loss= 1.1279147863388062 acc= 0.9276464581489563 auc= 0.9324595512061743\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8297777777777778\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.3931\n",
      "\n",
      "Test on train set: loss= 1.607894778251648 acc= 0.8980252742767334 auc= 0.945161461979616\n",
      "Test on valid set: loss= 3.5505876541137695 acc= 0.7758853435516357 auc= 0.8764444444444445\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 35.5623\n",
      "\n",
      "Test on train set: loss= 1.0885393619537354 acc= 0.9306409955024719 auc= 0.9371905329968439\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7884444444444444\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.4452\n",
      "\n",
      "Test on train set: loss= 0.9533458948135376 acc= 0.9394626021385193 auc= 0.9386718350344012\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8344444444444445\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.4072\n",
      "\n",
      "Test on train set: loss= 1.0306568145751953 acc= 0.9341210722923279 auc= 0.9323132344601108\n",
      "Test on valid set: loss= 6.1269145011901855 acc= 0.6180617809295654 auc= 0.7977777777777778\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.7064\n",
      "\n",
      "Test on train set: loss= 1.1222988367080688 acc= 0.9285367727279663 auc= 0.9399039696080292\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8215555555555556\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.7516\n",
      "\n",
      "Test on train set: loss= 1.0223888158798218 acc= 0.934525728225708 auc= 0.9627476752521404\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8646666666666668\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.5534\n",
      "\n",
      "Test on train set: loss= 0.6772574782371521 acc= 0.9565393328666687 auc= 0.9580110550918068\n",
      "Test on valid set: loss= 4.653643608093262 acc= 0.7000177502632141 auc= 0.8442222222222224\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.5139\n",
      "\n",
      "Test on train set: loss= 0.7826959490776062 acc= 0.949741005897522 auc= 0.9546861006018421\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.8795555555555555\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.5365\n",
      "\n",
      "Test on train set: loss= 0.8081159591674805 acc= 0.9479604959487915 auc= 0.9506411971728562\n",
      "Test on valid set: loss= 4.418912887573242 acc= 0.7200002670288086 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.7903\n",
      "\n",
      "Test on train set: loss= 0.8794990181922913 acc= 0.9439138770103455 auc= 0.9487733100504784\n",
      "Test on valid set: loss= 7.102067947387695 acc= 0.5520663261413574 auc= 0.7757777777777778\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.0637\n",
      "\n",
      "Test on train set: loss= 0.8273574113845825 acc= 0.9475558400154114 auc= 0.9625871796583649\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8113333333333334\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.7503\n",
      "\n",
      "Test on train set: loss= 0.9376041889190674 acc= 0.9405147433280945 auc= 0.946916913359812\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8304444444444444\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.9704\n",
      "\n",
      "Test on train set: loss= 1.1911436319351196 acc= 0.9248138666152954 auc= 0.938342152283813\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.855111111111111\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 36.5756\n",
      "\n",
      "Test on train set: loss= 0.8097571730613708 acc= 0.9481223821640015 auc= 0.9359369220644072\n",
      "Test on valid set: loss= 6.447238922119141 acc= 0.6000000238418579 auc= 0.7871111111111111\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.8269\n",
      "\n",
      "Test on train set: loss= 0.7507060766220093 acc= 0.9517643451690674 auc= 0.9485997748303019\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8333333333333334\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.6816\n",
      "\n",
      "Test on train set: loss= 1.102905511856079 acc= 0.9299125671386719 auc= 0.9551491022226075\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7799999713897705 auc= 0.8875555555555555\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.8535\n",
      "\n",
      "Test on train set: loss= 0.9121357798576355 acc= 0.9418096542358398 auc= 0.9685928410883109\n",
      "Test on valid set: loss= 4.571082592010498 acc= 0.7010995745658875 auc= 0.8626666666666667\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.5569\n",
      "\n",
      "Test on train set: loss= 0.7719215750694275 acc= 0.950388491153717 auc= 0.9628204697109399\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8197777777777778\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.0003\n",
      "\n",
      "Test on train set: loss= 0.7990369200706482 acc= 0.9488507509231567 auc= 0.9622972292145622\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7982222222222222\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.0498\n",
      "\n",
      "Test on train set: loss= 0.9601780772209167 acc= 0.9382486343383789 auc= 0.9439461467561753\n",
      "Test on valid set: loss= 6.124876022338867 acc= 0.6200000047683716 auc= 0.8006666666666666\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 36.1842\n",
      "\n",
      "Test on train set: loss= 0.9266273975372314 acc= 0.9410812854766846 auc= 0.9608838229981675\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.7988888888888888\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.4730\n",
      "\n",
      "Test on train set: loss= 0.7749454975128174 acc= 0.9502266049385071 auc= 0.9657103090939246\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8317777777777777\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 35.3608\n",
      "\n",
      "Test on train set: loss= 0.9544520974159241 acc= 0.9392198324203491 auc= 0.947345912666691\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.828\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.0471\n",
      "\n",
      "Test on train set: loss= 0.8315548300743103 acc= 0.9468274712562561 auc= 0.9446711901627672\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8222222222222222\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.0772\n",
      "\n",
      "Test on train set: loss= 1.0251672267913818 acc= 0.9352541565895081 auc= 0.9546753819928109\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8204444444444444\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.8905\n",
      "\n",
      "Test on train set: loss= 0.7294834852218628 acc= 0.9526546001434326 auc= 0.960480355521876\n",
      "Test on valid set: loss= 4.853408336639404 acc= 0.6881396174430847 auc= 0.842\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.3706\n",
      "\n",
      "Test on train set: loss= 1.1702852249145508 acc= 0.9257041215896606 auc= 0.9500719937123548\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.9618\n",
      "\n",
      "Test on train set: loss= 0.8057377338409424 acc= 0.9484460949897766 auc= 0.9658318595550766\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8742222222222222\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.3910\n",
      "\n",
      "Test on train set: loss= 0.8002861738204956 acc= 0.9484460949897766 auc= 0.9740429661951838\n",
      "Test on valid set: loss= 4.171619415283203 acc= 0.7400000095367432 auc= 0.8642222222222223\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.5214\n",
      "\n",
      "Test on train set: loss= 1.0708115100860596 acc= 0.9314503073692322 auc= 0.938287433371982\n",
      "Test on valid set: loss= 5.480158805847168 acc= 0.6599938869476318 auc= 0.8102222222222222\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.3263\n",
      "\n",
      "Test on train set: loss= 0.8194239735603333 acc= 0.9477177262306213 auc= 0.9720310617108533\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.9519\n",
      "\n",
      "Test on train set: loss= 0.7504735589027405 acc= 0.9521690011024475 auc= 0.9499442171156774\n",
      "Test on valid set: loss= 4.513086318969727 acc= 0.7199801206588745 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.6282\n",
      "\n",
      "Test on train set: loss= 0.8365382552146912 acc= 0.9469084143638611 auc= 0.9530532837237253\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8215555555555556\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 25.4094\n",
      "\n",
      "Test on train set: loss= 0.43100565671920776 acc= 0.9721592664718628 auc= 0.9806539772957583\n",
      "Test on valid set: loss= 4.513067245483398 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 13.5015\n",
      "\n",
      "Test on train set: loss= 0.35354432463645935 acc= 0.9770961403846741 auc= 0.988345911609426\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 11.5443\n",
      "\n",
      "Test on train set: loss= 0.2963515520095825 acc= 0.98057621717453 auc= 0.9865897864674853\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8655555555555556\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 8.9095\n",
      "\n",
      "Test on train set: loss= 0.3112330436706543 acc= 0.9797669053077698 auc= 0.9877438193955715\n",
      "Test on valid set: loss= 4.387641906738281 acc= 0.7200010418891907 auc= 0.8540000000000001\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 12.5167\n",
      "\n",
      "Test on train set: loss= 0.2805205285549164 acc= 0.9816283583641052 auc= 0.9869562493113382\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 9.3902\n",
      "\n",
      "Test on train set: loss= 0.2762737274169922 acc= 0.9822758436203003 auc= 0.9919536889881604\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 9.9134\n",
      "\n",
      "Test on train set: loss= 0.2690034508705139 acc= 0.9825186133384705 auc= 0.990806195922209\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8540000000000001\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 9.1416\n",
      "\n",
      "Test on train set: loss= 0.25476187467575073 acc= 0.9834898114204407 auc= 0.9919277294707916\n",
      "Test on valid set: loss= 4.509799003601074 acc= 0.7199999690055847 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.3821\n",
      "\n",
      "Test on train set: loss= 0.2279197722673416 acc= 0.9851893782615662 auc= 0.9913400439264244\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.5291\n",
      "\n",
      "Test on train set: loss= 0.24765649437904358 acc= 0.9838944673538208 auc= 0.9919192970562088\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.0636\n",
      "\n",
      "Test on train set: loss= 0.2227342128753662 acc= 0.9857559204101562 auc= 0.9909887405262662\n",
      "Test on valid set: loss= 3.8980066776275635 acc= 0.7445381879806519 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.6144\n",
      "\n",
      "Test on train set: loss= 0.22779206931591034 acc= 0.9855940341949463 auc= 0.9924298394092259\n",
      "Test on valid set: loss= 4.4453511238098145 acc= 0.7195662260055542 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 9.8357\n",
      "\n",
      "Test on train set: loss= 0.21894721686840057 acc= 0.9856749773025513 auc= 0.9922802993480669\n",
      "Test on valid set: loss= 3.9891157150268555 acc= 0.7400476932525635 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.7853\n",
      "\n",
      "Test on train set: loss= 0.2221929281949997 acc= 0.9860796332359314 auc= 0.9921009174949473\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8542222222222223\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.1664\n",
      "\n",
      "Test on train set: loss= 0.2334829866886139 acc= 0.984703779220581 auc= 0.992127362039529\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 8.5611\n",
      "\n",
      "Test on train set: loss= 0.20153555274009705 acc= 0.9869698882102966 auc= 0.9938478536481956\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.6496\n",
      "\n",
      "Test on train set: loss= 0.19601784646511078 acc= 0.9873745441436768 auc= 0.9907571361626832\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.853111111111111\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.3661\n",
      "\n",
      "Test on train set: loss= 0.20314082503318787 acc= 0.9868889451026917 auc= 0.9930798355803928\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8433333333333334\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.9664\n",
      "\n",
      "Test on train set: loss= 0.1938607096672058 acc= 0.9876173734664917 auc= 0.9930712420525991\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.3839\n",
      "\n",
      "Test on train set: loss= 0.17683382332324982 acc= 0.9885885119438171 auc= 0.9942105330779857\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.844\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.0867\n",
      "\n",
      "Test on train set: loss= 0.16854508221149445 acc= 0.9889122843742371 auc= 0.9948147971093695\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.4411\n",
      "\n",
      "Test on train set: loss= 0.20918095111846924 acc= 0.9864842891693115 auc= 0.9932243265336439\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8437777777777778\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.0144\n",
      "\n",
      "Test on train set: loss= 0.2011764645576477 acc= 0.9870508313179016 auc= 0.9947887664206456\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8437777777777778\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.3593\n",
      "\n",
      "Test on train set: loss= 0.167902871966362 acc= 0.9891550540924072 auc= 0.993752641544584\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.844\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.4210\n",
      "\n",
      "Test on train set: loss= 0.17484524846076965 acc= 0.988426685333252 auc= 0.9944501743408616\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8540000000000001\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.4016\n",
      "\n",
      "Test on train set: loss= 0.17755001783370972 acc= 0.988264799118042 auc= 0.9933372020354329\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.6523\n",
      "\n",
      "Test on train set: loss= 0.17967580258846283 acc= 0.988426685333252 auc= 0.9944491324183039\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.834\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.2458\n",
      "\n",
      "Test on train set: loss= 0.1652536392211914 acc= 0.9893169403076172 auc= 0.9937721207499433\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.6999999284744263 auc= 0.8424444444444446\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.9515\n",
      "\n",
      "Test on train set: loss= 0.16931575536727905 acc= 0.9892359972000122 auc= 0.9948020740683583\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8560000000000001\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.4262\n",
      "\n",
      "Test on train set: loss= 0.15011900663375854 acc= 0.9901262521743774 auc= 0.9951201461217991\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8544444444444445\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.5273\n",
      "\n",
      "Test on train set: loss= 0.16920071840286255 acc= 0.9892359972000122 auc= 0.9949979739392548\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8437777777777778\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.9039\n",
      "\n",
      "Test on train set: loss= 0.16399666666984558 acc= 0.9890741109848022 auc= 0.9953996655803421\n",
      "Test on valid set: loss= 4.624232769012451 acc= 0.7000771760940552 auc= 0.8437777777777778\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.8633\n",
      "\n",
      "Test on train set: loss= 0.1342739462852478 acc= 0.9912593364715576 auc= 0.9959372804877343\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.842\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.4266\n",
      "\n",
      "Test on train set: loss= 0.1670389324426651 acc= 0.9893169403076172 auc= 0.9956987209822514\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.844888888888889\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.5286\n",
      "\n",
      "Test on train set: loss= 0.13896790146827698 acc= 0.9907737374305725 auc= 0.9958653794046558\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8457777777777779\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.0055\n",
      "\n",
      "Test on train set: loss= 0.13777954876422882 acc= 0.9906927943229675 auc= 0.994156800516048\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8542222222222222\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.7409\n",
      "\n",
      "Test on train set: loss= 0.13592834770679474 acc= 0.9911783933639526 auc= 0.9961154205312596\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8426666666666668\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 7.4698\n",
      "\n",
      "Test on train set: loss= 0.15304218232631683 acc= 0.9901262521743774 auc= 0.9957706336778106\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.8850\n",
      "\n",
      "Test on train set: loss= 0.15251323580741882 acc= 0.9903690218925476 auc= 0.9959504950422413\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8540000000000001\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.0027\n",
      "\n",
      "Test on train set: loss= 0.1326564997434616 acc= 0.9913402199745178 auc= 0.9957691850639232\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8568888888888889\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.2095\n",
      "\n",
      "Test on train set: loss= 0.1356755942106247 acc= 0.9911783933639526 auc= 0.9936524356358278\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.852888888888889\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.8727\n",
      "\n",
      "Test on train set: loss= 0.1313670426607132 acc= 0.9915021061897278 auc= 0.9961818276609572\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8675555555555556\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.2537\n",
      "\n",
      "Test on train set: loss= 0.12636655569076538 acc= 0.9918258190155029 auc= 0.9962672154347615\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.2979\n",
      "\n",
      "Test on train set: loss= 0.12601076066493988 acc= 0.9916639924049377 auc= 0.9965092044904663\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.844\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.5722\n",
      "\n",
      "Test on train set: loss= 0.12421266734600067 acc= 0.9918258190155029 auc= 0.9963660123006081\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.9906\n",
      "\n",
      "Test on train set: loss= 0.11462002992630005 acc= 0.992392361164093 auc= 0.9953819743034259\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8537777777777779\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.7032\n",
      "\n",
      "Test on train set: loss= 0.11742794513702393 acc= 0.992473304271698 auc= 0.9952376855587206\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.1498\n",
      "\n",
      "Test on train set: loss= 0.1270483285188675 acc= 0.9916639924049377 auc= 0.9952195678790728\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.9071\n",
      "\n",
      "Test on train set: loss= 0.117832250893116 acc= 0.992311418056488 auc= 0.9962916994793872\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.4608\n",
      "\n",
      "Test on train set: loss= 0.10552003234624863 acc= 0.9930398464202881 auc= 0.9966834981520885\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8455555555555556\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 7.4042\n",
      "\n",
      "Test on train set: loss= 0.11783306300640106 acc= 0.9922304749488831 auc= 0.9961615051990776\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8535555555555556\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.3173\n",
      "\n",
      "Test on train set: loss= 0.10983522981405258 acc= 0.992392361164093 auc= 0.9962615136031054\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.9241\n",
      "\n",
      "Test on train set: loss= 0.10739275068044662 acc= 0.9927970170974731 auc= 0.9964179796168781\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7399999499320984 auc= 0.866\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.6990\n",
      "\n",
      "Test on train set: loss= 0.1081894114613533 acc= 0.9930398464202881 auc= 0.9968646502295331\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8648888888888889\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.2257\n",
      "\n",
      "Test on train set: loss= 0.11392515152692795 acc= 0.992311418056488 auc= 0.9963446637550387\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.0625\n",
      "\n",
      "Test on train set: loss= 0.1111426055431366 acc= 0.9926351308822632 auc= 0.9961766162354827\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.1755\n",
      "\n",
      "Test on train set: loss= 0.11130242794752121 acc= 0.9928779602050781 auc= 0.9963437442577329\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8648888888888889\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.3349\n",
      "\n",
      "Test on train set: loss= 0.11340253800153732 acc= 0.992392361164093 auc= 0.9966565354148977\n",
      "Test on valid set: loss= 4.190733432769775 acc= 0.7399711608886719 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9125\n",
      "\n",
      "Test on train set: loss= 0.10755950212478638 acc= 0.9929589033126831 auc= 0.9941147273326931\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8560000000000001\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.8594\n",
      "\n",
      "Test on train set: loss= 0.10844440013170242 acc= 0.9930398464202881 auc= 0.9964975739324224\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8648888888888889\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.2677\n",
      "\n",
      "Test on train set: loss= 0.10589414089918137 acc= 0.9928779602050781 auc= 0.9969895959454937\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.4999\n",
      "\n",
      "Test on train set: loss= 0.10841258615255356 acc= 0.9929589033126831 auc= 0.9965044077265514\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8666666666666668\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.8974\n",
      "\n",
      "Test on train set: loss= 0.11059761792421341 acc= 0.9926351308822632 auc= 0.9963918437514547\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8644444444444443\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.9826\n",
      "\n",
      "Test on train set: loss= 0.11409418284893036 acc= 0.9926351308822632 auc= 0.9968344640579387\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8571111111111112\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9170\n",
      "\n",
      "Test on train set: loss= 0.10509923845529556 acc= 0.9931207299232483 auc= 0.9965108115054877\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8666666666666668\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9434\n",
      "\n",
      "Test on train set: loss= 0.11168888211250305 acc= 0.9927970170974731 auc= 0.9964567607897091\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.2196\n",
      "\n",
      "Test on train set: loss= 0.118306465446949 acc= 0.9922304749488831 auc= 0.9964815273556997\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9251\n",
      "\n",
      "Test on train set: loss= 0.10998544842004776 acc= 0.9926351308822632 auc= 0.9965025681240365\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.1281\n",
      "\n",
      "Test on train set: loss= 0.10715840756893158 acc= 0.9931207299232483 auc= 0.99641790294045\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8644444444444443\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9243\n",
      "\n",
      "Test on train set: loss= 0.0996045246720314 acc= 0.9932826161384583 auc= 0.9965708757225045\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8573333333333334\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.8780\n",
      "\n",
      "Test on train set: loss= 0.10209206491708755 acc= 0.9934445023536682 auc= 0.9969389716241462\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8686666666666667\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.5062\n",
      "\n",
      "Test on train set: loss= 0.10909933596849442 acc= 0.9928779602050781 auc= 0.996654533598899\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8453333333333333\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.4541\n",
      "\n",
      "Test on train set: loss= 0.12423418462276459 acc= 0.9919877052307129 auc= 0.9971260785029058\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.9690\n",
      "\n",
      "Test on train set: loss= 0.10480260103940964 acc= 0.9932016730308533 auc= 0.9965042446826311\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.3038\n",
      "\n",
      "Test on train set: loss= 0.103099025785923 acc= 0.9930398464202881 auc= 0.9965358833863955\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.9960\n",
      "\n",
      "Test on train set: loss= 0.11001039296388626 acc= 0.9930398464202881 auc= 0.9966489935768369\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.0572\n",
      "\n",
      "Test on train set: loss= 0.10131187736988068 acc= 0.9933635592460632 auc= 0.996700783503484\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.9576\n",
      "\n",
      "Test on train set: loss= 0.10174454003572464 acc= 0.9936063289642334 auc= 0.9967609130838617\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8537777777777779\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.2420\n",
      "\n",
      "Test on train set: loss= 0.10022508352994919 acc= 0.9935253858566284 auc= 0.9968728869437282\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8648888888888889\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.6636\n",
      "\n",
      "Test on train set: loss= 0.10385625064373016 acc= 0.9931207299232483 auc= 0.9970553647792839\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8571111111111112\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.6112\n",
      "\n",
      "Test on train set: loss= 0.10466872155666351 acc= 0.9932826161384583 auc= 0.9969708113007479\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8540000000000001\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 2.7748\n",
      "\n",
      "Test on train set: loss= 0.1158255785703659 acc= 0.9926351308822632 auc= 0.9965805637716125\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8537777777777779\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.9629\n",
      "\n",
      "Test on train set: loss= 0.10159047693014145 acc= 0.9934445023536682 auc= 0.996662249889886\n",
      "Test on valid set: loss= 4.190800189971924 acc= 0.739905059337616 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.4640\n",
      "\n",
      "Test on train set: loss= 0.10033664107322693 acc= 0.9934445023536682 auc= 0.9966408917971963\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8568888888888889\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9387\n",
      "\n",
      "Test on train set: loss= 0.0994580090045929 acc= 0.9936063289642334 auc= 0.9944710982970013\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8533333333333333\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.3027\n",
      "\n",
      "Test on train set: loss= 0.10543401539325714 acc= 0.9932826161384583 auc= 0.9964733237857766\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8673333333333334\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.1093\n",
      "\n",
      "Test on train set: loss= 0.10409955680370331 acc= 0.9933635592460632 auc= 0.9969888026465886\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8673333333333334\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.2112\n",
      "\n",
      "Test on train set: loss= 0.09920044243335724 acc= 0.9936063289642334 auc= 0.9971579185993811\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8637777777777778\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.6338\n",
      "\n",
      "Test on train set: loss= 0.10006528347730637 acc= 0.9935253858566284 auc= 0.9968712326349245\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8655555555555556\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.0243\n",
      "\n",
      "Test on train set: loss= 0.10284390300512314 acc= 0.9932016730308533 auc= 0.9969941505393226\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.866\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.3247\n",
      "\n",
      "Test on train set: loss= 0.1025225967168808 acc= 0.9932826161384583 auc= 0.9967329943595322\n",
      "Test on valid set: loss= 3.8683435916900635 acc= 0.7599993944168091 auc= 0.8668888888888888\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.1976\n",
      "\n",
      "Test on train set: loss= 0.10881958901882172 acc= 0.9928779602050781 auc= 0.9970432808188681\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8535555555555556\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.8366\n",
      "\n",
      "Test on train set: loss= 0.1080697551369667 acc= 0.9928779602050781 auc= 0.9971048167633321\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8453333333333333\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.7335\n",
      "\n",
      "Test on train set: loss= 0.09916495531797409 acc= 0.9934445023536682 auc= 0.9966989011774532\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 2.6662\n",
      "\n",
      "Test on train set: loss= 0.11039945483207703 acc= 0.9927970170974731 auc= 0.9957330592756902\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8533333333333333\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.7767\n",
      "\n",
      "Test on train set: loss= 0.11066868156194687 acc= 0.9927160739898682 auc= 0.9968779636381043\n",
      "Test on valid set: loss= 4.771694660186768 acc= 0.699999988079071 auc= 0.844\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.4333\n",
      "\n",
      "Test on train set: loss= 0.10967142879962921 acc= 0.9927970170974731 auc= 0.9967848058735699\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8462222222222223\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.2344\n",
      "\n",
      "Test on train set: loss= 0.10727046430110931 acc= 0.9930398464202881 auc= 0.996429978779911\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.2387\n",
      "\n",
      "Test on train set: loss= 0.11060724407434464 acc= 0.9927160739898682 auc= 0.9968190648041071\n",
      "Test on valid set: loss= 4.191171646118164 acc= 0.739538311958313 auc= 0.8637777777777778\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.3841\n",
      "\n",
      "Test on train set: loss= 0.10656684637069702 acc= 0.9931207299232483 auc= 0.9964740606349434\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8573333333333334\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "metric_idx = 2\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_Reweight(beginEncodingEpoch, endEncodingEpoch,  \n",
    "                                               codingEnhancementRate, mu,\n",
    "                                               train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Reweight')\n",
    "                        \n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "reweightWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoSen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 5s - loss: 1.5983\n",
      "\n",
      "Test on train set: loss= 1.4286205768585205 acc= 0.5352055430412292 auc= 0.7596063250862601\n",
      "Test on valid set: loss= 2.8367857933044434 acc= 0.15294723212718964 auc= 0.6568888888888889\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 3s - loss: -4.3608e+00\n",
      "\n",
      "Test on train set: loss= 1.17711341381073 acc= 0.6213985085487366 auc= 0.8189981433508677\n",
      "Test on valid set: loss= 3.649296522140503 acc= 0.2329140156507492 auc= 0.7431111111111112\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 3s - loss: -6.9246e+00\n",
      "\n",
      "Test on train set: loss= 1.4625314474105835 acc= 0.6110391616821289 auc= 0.7567558056248131\n",
      "Test on valid set: loss= 3.9307825565338135 acc= 0.2207115888595581 auc= 0.6875555555555556\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 3s - loss: -5.6393e+00\n",
      "\n",
      "Test on train set: loss= 1.6139284372329712 acc= 0.5944480299949646 auc= 0.7678586297490461\n",
      "Test on valid set: loss= 3.7681045532226562 acc= 0.2110917568206787 auc= 0.6866666666666668\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 3s - loss: -5.3219e+00\n",
      "\n",
      "Test on train set: loss= 1.259010672569275 acc= 0.63499516248703 auc= 0.7774154383580054\n",
      "Test on valid set: loss= 3.5851240158081055 acc= 0.23802044987678528 auc= 0.6777777777777778\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5523e+00\n",
      "\n",
      "Test on train set: loss= 1.5728508234024048 acc= 0.6685011386871338 auc= 0.8086942908938628\n",
      "Test on valid set: loss= 3.254096746444702 acc= 0.2960859537124634 auc= 0.7471111111111111\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.2092e+00\n",
      "\n",
      "Test on train set: loss= 1.1477165222167969 acc= 0.7138232588768005 auc= 0.8231719495050973\n",
      "Test on valid set: loss= 3.6097412109375 acc= 0.28848063945770264 auc= 0.7008888888888889\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0921e+00\n",
      "\n",
      "Test on train set: loss= 0.9912947416305542 acc= 0.7400453090667725 auc= 0.8540318750122047\n",
      "Test on valid set: loss= 3.630436897277832 acc= 0.3043304681777954 auc= 0.7302222222222222\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.7464e+00\n",
      "\n",
      "Test on train set: loss= 0.8744269013404846 acc= 0.7494334578514099 auc= 0.8380256930315764\n",
      "Test on valid set: loss= 3.6067187786102295 acc= 0.30356091260910034 auc= 0.732\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7343e+00\n",
      "\n",
      "Test on train set: loss= 1.0757404565811157 acc= 0.7339754104614258 auc= 0.8453485260572075\n",
      "Test on valid set: loss= 3.416311740875244 acc= 0.2935144901275635 auc= 0.7373333333333334\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0653e+00\n",
      "\n",
      "Test on train set: loss= 1.3493537902832031 acc= 0.7067821025848389 auc= 0.8414289569867881\n",
      "Test on valid set: loss= 3.3273496627807617 acc= 0.32575559616088867 auc= 0.7644444444444444\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5679e+00\n",
      "\n",
      "Test on train set: loss= 0.9757915139198303 acc= 0.7439301013946533 auc= 0.8439562930195125\n",
      "Test on valid set: loss= 3.596268892288208 acc= 0.29474568367004395 auc= 0.704\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.4877e+00\n",
      "\n",
      "Test on train set: loss= 1.0037834644317627 acc= 0.7520232796669006 auc= 0.8592931119614062\n",
      "Test on valid set: loss= 3.2447092533111572 acc= 0.32096412777900696 auc= 0.7626666666666667\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7725e+00\n",
      "\n",
      "Test on train set: loss= 0.9193698763847351 acc= 0.7512139678001404 auc= 0.8834704853461887\n",
      "Test on valid set: loss= 2.374696731567383 acc= 0.35266393423080444 auc= 0.7902222222222222\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7721e+00\n",
      "\n",
      "Test on train set: loss= 1.7262026071548462 acc= 0.6984460949897766 auc= 0.7677206568479849\n",
      "Test on valid set: loss= 5.156229019165039 acc= 0.24806301295757294 auc= 0.6884444444444444\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5108e+00\n",
      "\n",
      "Test on train set: loss= 1.331796407699585 acc= 0.7334088683128357 auc= 0.8722618801570455\n",
      "Test on valid set: loss= 3.0914623737335205 acc= 0.3768143653869629 auc= 0.7782222222222223\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.9222e+00\n",
      "\n",
      "Test on train set: loss= 1.3311251401901245 acc= 0.7253965735435486 auc= 0.8292798303844467\n",
      "Test on valid set: loss= 2.8823492527008057 acc= 0.3570546507835388 auc= 0.7817777777777778\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5276e+00\n",
      "\n",
      "Test on train set: loss= 1.1358387470245361 acc= 0.7467626929283142 auc= 0.8314531484013278\n",
      "Test on valid set: loss= 4.429224014282227 acc= 0.2692173421382904 auc= 0.7204444444444444\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.2433e+00\n",
      "\n",
      "Test on train set: loss= 1.5470558404922485 acc= 0.7022499442100525 auc= 0.8208347528889777\n",
      "Test on valid set: loss= 4.334660530090332 acc= 0.2850646376609802 auc= 0.6777777777777777\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.8713e+00\n",
      "\n",
      "Test on train set: loss= 1.53254234790802 acc= 0.7210262417793274 auc= 0.8638546548479239\n",
      "Test on valid set: loss= 3.9149293899536133 acc= 0.3150433599948883 auc= 0.7346666666666667\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.3248e+00\n",
      "\n",
      "Test on train set: loss= 2.064427137374878 acc= 0.6915668249130249 auc= 0.8306716446806204\n",
      "Test on valid set: loss= 5.522502422332764 acc= 0.2775840163230896 auc= 0.7053333333333333\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3204e+01\n",
      "\n",
      "Test on train set: loss= 2.159883737564087 acc= 0.7420686483383179 auc= 0.8423648869388145\n",
      "Test on valid set: loss= 6.053433418273926 acc= 0.34865206480026245 auc= 0.7364444444444445\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1858e+01\n",
      "\n",
      "Test on train set: loss= 2.327680826187134 acc= 0.7221592664718628 auc= 0.8172681142165631\n",
      "Test on valid set: loss= 5.68790340423584 acc= 0.2962915301322937 auc= 0.7497777777777778\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0088e+01\n",
      "\n",
      "Test on train set: loss= 2.3630971908569336 acc= 0.7117999196052551 auc= 0.8272025219597804\n",
      "Test on valid set: loss= 5.375283718109131 acc= 0.289379745721817 auc= 0.716\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.5308e+01\n",
      "\n",
      "Test on train set: loss= 2.664017677307129 acc= 0.6724668145179749 auc= 0.8031594193461895\n",
      "Test on valid set: loss= 5.738526821136475 acc= 0.24063114821910858 auc= 0.742\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0440e+01\n",
      "\n",
      "Test on train set: loss= 2.3995652198791504 acc= 0.684363842010498 auc= 0.7641118129065332\n",
      "Test on valid set: loss= 4.714257717132568 acc= 0.23537075519561768 auc= 0.6551111111111111\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4205e+01\n",
      "\n",
      "Test on train set: loss= 1.9037562608718872 acc= 0.7011978030204773 auc= 0.8085044536905264\n",
      "Test on valid set: loss= 3.2610411643981934 acc= 0.29770058393478394 auc= 0.7328888888888889\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.5888e+01\n",
      "\n",
      "Test on train set: loss= 1.6171607971191406 acc= 0.7408546209335327 auc= 0.8684404770636925\n",
      "Test on valid set: loss= 2.929394006729126 acc= 0.3436998426914215 auc= 0.7733333333333333\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2563e+01\n",
      "\n",
      "Test on train set: loss= 2.1089026927948 acc= 0.7060537338256836 auc= 0.8477843047367084\n",
      "Test on valid set: loss= 4.234938144683838 acc= 0.34373530745506287 auc= 0.7395555555555556\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2497e+01\n",
      "\n",
      "Test on train set: loss= 1.2309972047805786 acc= 0.7805114984512329 auc= 0.874952367110317\n",
      "Test on valid set: loss= 3.2222747802734375 acc= 0.41830629110336304 auc= 0.7942222222222222\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0284e+01\n",
      "\n",
      "Test on train set: loss= 1.819693684577942 acc= 0.7625445127487183 auc= 0.8372964129956879\n",
      "Test on valid set: loss= 4.266787528991699 acc= 0.41874027252197266 auc= 0.7782222222222223\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1608e+01\n",
      "\n",
      "Test on train set: loss= 3.070557117462158 acc= 0.6162188649177551 auc= 0.8392135591602041\n",
      "Test on valid set: loss= 4.406922817230225 acc= 0.3480915129184723 auc= 0.7671111111111111\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1351e+01\n",
      "\n",
      "Test on train set: loss= 1.8284401893615723 acc= 0.7426351308822632 auc= 0.830108820183975\n",
      "Test on valid set: loss= 3.903618097305298 acc= 0.39958062767982483 auc= 0.7764444444444444\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.1541e+01\n",
      "\n",
      "Test on train set: loss= 1.3597620725631714 acc= 0.8092424869537354 auc= 0.8691560742977653\n",
      "Test on valid set: loss= 5.40673828125 acc= 0.36167842149734497 auc= 0.735111111111111\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.5851e+01\n",
      "\n",
      "Test on train set: loss= 1.2861716747283936 acc= 0.7790547013282776 auc= 0.8940724272412061\n",
      "Test on valid set: loss= 3.2655670642852783 acc= 0.415257066488266 auc= 0.8133333333333332\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4909e+01\n",
      "\n",
      "Test on train set: loss= 1.885959267616272 acc= 0.7066202759742737 auc= 0.8767415170686128\n",
      "Test on valid set: loss= 4.038161754608154 acc= 0.39621809124946594 auc= 0.7533333333333333\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1996e+01\n",
      "\n",
      "Test on train set: loss= 1.64188814163208 acc= 0.7918420433998108 auc= 0.8668601675599419\n",
      "Test on valid set: loss= 4.41633415222168 acc= 0.32857170701026917 auc= 0.7653333333333333\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.2028e+00\n",
      "\n",
      "Test on train set: loss= 1.9447712898254395 acc= 0.7462770938873291 auc= 0.8497755141809844\n",
      "Test on valid set: loss= 5.833702564239502 acc= 0.2855752408504486 auc= 0.7246666666666667\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1518e+01\n",
      "\n",
      "Test on train set: loss= 1.3704873323440552 acc= 0.7958077192306519 auc= 0.8616105284005501\n",
      "Test on valid set: loss= 3.5990149974823 acc= 0.47026634216308594 auc= 0.8088888888888889\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2758e+01\n",
      "\n",
      "Test on train set: loss= 1.506147027015686 acc= 0.8056814670562744 auc= 0.8623419304934645\n",
      "Test on valid set: loss= 5.123639106750488 acc= 0.3449913263320923 auc= 0.7486666666666667\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4521e+01\n",
      "\n",
      "Test on train set: loss= 1.3257317543029785 acc= 0.8090806007385254 auc= 0.8969389924636492\n",
      "Test on valid set: loss= 3.9481287002563477 acc= 0.37235480546951294 auc= 0.7795555555555556\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.5060e+01\n",
      "\n",
      "Test on train set: loss= 1.2222328186035156 acc= 0.8280997276306152 auc= 0.8660664547539675\n",
      "Test on valid set: loss= 4.643232345581055 acc= 0.4398892819881439 auc= 0.7884444444444445\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8751e+01\n",
      "\n",
      "Test on train set: loss= 1.5641783475875854 acc= 0.7567173838615417 auc= 0.8624612198116408\n",
      "Test on valid set: loss= 5.605351448059082 acc= 0.33864814043045044 auc= 0.7257777777777779\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7086e+01\n",
      "\n",
      "Test on train set: loss= 1.4456804990768433 acc= 0.7922466993331909 auc= 0.8936931438298104\n",
      "Test on valid set: loss= 4.182233810424805 acc= 0.4193941652774811 auc= 0.8053333333333335\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9620e+01\n",
      "\n",
      "Test on train set: loss= 1.8163644075393677 acc= 0.7283101081848145 auc= 0.8762816495161333\n",
      "Test on valid set: loss= 5.658714771270752 acc= 0.35033494234085083 auc= 0.7155555555555556\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7304e+01\n",
      "\n",
      "Test on train set: loss= 1.5826083421707153 acc= 0.7893331050872803 auc= 0.8857798800752825\n",
      "Test on valid set: loss= 4.969789028167725 acc= 0.3647204339504242 auc= 0.778888888888889\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4982e+01\n",
      "\n",
      "Test on train set: loss= 2.5157625675201416 acc= 0.7304143905639648 auc= 0.8270657716905191\n",
      "Test on valid set: loss= 5.877910137176514 acc= 0.37421196699142456 auc= 0.7195555555555556\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4243e+01\n",
      "\n",
      "Test on train set: loss= 0.9853474497795105 acc= 0.8276950716972351 auc= 0.9314328631106527\n",
      "Test on valid set: loss= 3.4824044704437256 acc= 0.44386234879493713 auc= 0.8346666666666668\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8420e+01\n",
      "\n",
      "Test on train set: loss= 1.4807181358337402 acc= 0.7813208103179932 auc= 0.8683661302110928\n",
      "Test on valid set: loss= 5.022587776184082 acc= 0.3570311963558197 auc= 0.7635555555555557\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.5199e+01\n",
      "\n",
      "Test on train set: loss= 1.486202597618103 acc= 0.791922926902771 auc= 0.8349751624699542\n",
      "Test on valid set: loss= 4.753999710083008 acc= 0.3883880376815796 auc= 0.7333333333333334\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.4656e+01\n",
      "\n",
      "Test on train set: loss= 1.1622984409332275 acc= 0.7883619070053101 auc= 0.8958934471234421\n",
      "Test on valid set: loss= 3.674323081970215 acc= 0.37490206956863403 auc= 0.7851111111111111\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5548e+01\n",
      "\n",
      "Test on train set: loss= 1.5009629726409912 acc= 0.7853674292564392 auc= 0.9022538738896028\n",
      "Test on valid set: loss= 4.160411834716797 acc= 0.4277876317501068 auc= 0.8204444444444444\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2062e+01\n",
      "\n",
      "Test on train set: loss= 2.0248749256134033 acc= 0.7199741005897522 auc= 0.8897400277324439\n",
      "Test on valid set: loss= 5.142288684844971 acc= 0.3874509632587433 auc= 0.76\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3400e+01\n",
      "\n",
      "Test on train set: loss= 2.1663272380828857 acc= 0.7548559308052063 auc= 0.8542189261962196\n",
      "Test on valid set: loss= 6.040118217468262 acc= 0.42440876364707947 auc= 0.762888888888889\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4795e+01\n",
      "\n",
      "Test on train set: loss= 1.3356499671936035 acc= 0.7958077192306519 auc= 0.9016524474235961\n",
      "Test on valid set: loss= 4.173990726470947 acc= 0.4590834677219391 auc= 0.7871111111111111\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.9562e+01\n",
      "\n",
      "Test on train set: loss= 1.277130126953125 acc= 0.8211395144462585 auc= 0.8871929043072981\n",
      "Test on valid set: loss= 4.8762006759643555 acc= 0.46681326627731323 auc= 0.8011111111111111\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0288e+01\n",
      "\n",
      "Test on train set: loss= 1.204590916633606 acc= 0.8217869997024536 auc= 0.9174691169864329\n",
      "Test on valid set: loss= 4.528401851654053 acc= 0.3897903263568878 auc= 0.8066666666666666\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.8160e+01\n",
      "\n",
      "Test on train set: loss= 1.3615200519561768 acc= 0.7898996472358704 auc= 0.8859861152177793\n",
      "Test on valid set: loss= 5.238262176513672 acc= 0.3809516131877899 auc= 0.7544444444444445\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9309e+01\n",
      "\n",
      "Test on train set: loss= 1.34071946144104 acc= 0.8145840167999268 auc= 0.8955599990549397\n",
      "Test on valid set: loss= 5.47784423828125 acc= 0.38813528418540955 auc= 0.7575555555555555\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.1156e+01\n",
      "\n",
      "Test on train set: loss= 1.1966472864151 acc= 0.8212204575538635 auc= 0.9206743737460068\n",
      "Test on valid set: loss= 4.387710094451904 acc= 0.3946352005004883 auc= 0.7928888888888889\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5727e+01\n",
      "\n",
      "Test on train set: loss= 1.7261531352996826 acc= 0.7985594272613525 auc= 0.8727607714654726\n",
      "Test on valid set: loss= 5.62578010559082 acc= 0.37145161628723145 auc= 0.7682222222222223\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.3578e+01\n",
      "\n",
      "Test on train set: loss= 1.5046210289001465 acc= 0.8149077296257019 auc= 0.8859088368549857\n",
      "Test on valid set: loss= 5.482860565185547 acc= 0.3905542492866516 auc= 0.7744444444444444\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5196e+01\n",
      "\n",
      "Test on train set: loss= 2.21458101272583 acc= 0.7527517080307007 auc= 0.8789694708751732\n",
      "Test on valid set: loss= 4.950243949890137 acc= 0.3962317407131195 auc= 0.8031111111111111\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.2679e+01\n",
      "\n",
      "Test on train set: loss= 1.7373287677764893 acc= 0.818792462348938 auc= 0.9047025807906168\n",
      "Test on valid set: loss= 5.637629985809326 acc= 0.4243876039981842 auc= 0.8166666666666668\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8393e+01\n",
      "\n",
      "Test on train set: loss= 1.275165319442749 acc= 0.8242958784103394 auc= 0.912372914477853\n",
      "Test on valid set: loss= 5.626124382019043 acc= 0.40901270508766174 auc= 0.8022222222222222\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9965e+01\n",
      "\n",
      "Test on train set: loss= 1.4732425212860107 acc= 0.7818064093589783 auc= 0.9272356994944468\n",
      "Test on valid set: loss= 4.009585380554199 acc= 0.5150617361068726 auc= 0.8346666666666668\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8142e+01\n",
      "\n",
      "Test on train set: loss= 2.5879738330841064 acc= 0.7219974398612976 auc= 0.8854584845976028\n",
      "Test on valid set: loss= 5.973643779754639 acc= 0.37171995639801025 auc= 0.7748888888888888\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2541e+01\n",
      "\n",
      "Test on train set: loss= 1.5309687852859497 acc= 0.7814017534255981 auc= 0.9233156850264574\n",
      "Test on valid set: loss= 5.009466648101807 acc= 0.367211252450943 auc= 0.8011111111111111\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4273e+01\n",
      "\n",
      "Test on train set: loss= 1.629539132118225 acc= 0.7943509221076965 auc= 0.8675506211125894\n",
      "Test on valid set: loss= 5.0509538650512695 acc= 0.47334522008895874 auc= 0.8035555555555554\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.2461e+01\n",
      "\n",
      "Test on train set: loss= 3.1853129863739014 acc= 0.6737617254257202 auc= 0.8226190576029602\n",
      "Test on valid set: loss= 8.601105690002441 acc= 0.2524823248386383 auc= 0.6964444444444445\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5129e+01\n",
      "\n",
      "Test on train set: loss= 1.8963817358016968 acc= 0.7797021865844727 auc= 0.8824728367067062\n",
      "Test on valid set: loss= 5.139282703399658 acc= 0.40123963356018066 auc= 0.792888888888889\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.9643e+01\n",
      "\n",
      "Test on train set: loss= 1.346372127532959 acc= 0.8182259798049927 auc= 0.9035567949862615\n",
      "Test on valid set: loss= 5.176524639129639 acc= 0.3980737030506134 auc= 0.8220000000000001\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6511e+01\n",
      "\n",
      "Test on train set: loss= 2.0874133110046387 acc= 0.8017966747283936 auc= 0.8717170745797608\n",
      "Test on valid set: loss= 6.842559814453125 acc= 0.3625732660293579 auc= 0.7582222222222222\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9916e+01\n",
      "\n",
      "Test on train set: loss= 1.2679048776626587 acc= 0.8484137058258057 auc= 0.9219627003369132\n",
      "Test on valid set: loss= 5.689754486083984 acc= 0.46165919303894043 auc= 0.8171111111111111\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3144e+01\n",
      "\n",
      "Test on train set: loss= 1.2865601778030396 acc= 0.8443670868873596 auc= 0.9413037010894426\n",
      "Test on valid set: loss= 4.1189284324646 acc= 0.5086516737937927 auc= 0.8306666666666667\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5488e+01\n",
      "\n",
      "Test on train set: loss= 1.3103636503219604 acc= 0.8302039504051208 auc= 0.9472307473471362\n",
      "Test on valid set: loss= 5.566421031951904 acc= 0.4569883942604065 auc= 0.8204444444444444\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1404e+01\n",
      "\n",
      "Test on train set: loss= 1.4314348697662354 acc= 0.8247005343437195 auc= 0.9295029535113171\n",
      "Test on valid set: loss= 5.69724178314209 acc= 0.45048943161964417 auc= 0.8004444444444445\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.6026e+01\n",
      "\n",
      "Test on train set: loss= 1.725103497505188 acc= 0.815393328666687 auc= 0.900329563365586\n",
      "Test on valid set: loss= 6.230323314666748 acc= 0.4064706861972809 auc= 0.8122222222222222\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6040e+01\n",
      "\n",
      "Test on train set: loss= 1.4863168001174927 acc= 0.8494658470153809 auc= 0.9206753404906518\n",
      "Test on valid set: loss= 6.204285144805908 acc= 0.47824087738990784 auc= 0.853111111111111\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.7382e+01\n",
      "\n",
      "Test on train set: loss= 1.5632853507995605 acc= 0.8321463465690613 auc= 0.9096250085404872\n",
      "Test on valid set: loss= 4.408045291900635 acc= 0.4941977858543396 auc= 0.8608888888888888\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5610e+01\n",
      "\n",
      "Test on train set: loss= 1.278622031211853 acc= 0.8476043939590454 auc= 0.9325736833628773\n",
      "Test on valid set: loss= 4.89561653137207 acc= 0.5054315328598022 auc= 0.8380000000000001\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2315e+01\n",
      "\n",
      "Test on train set: loss= 1.6057335138320923 acc= 0.8100517988204956 auc= 0.9060956543219001\n",
      "Test on valid set: loss= 5.160132884979248 acc= 0.46531161665916443 auc= 0.8266666666666665\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8488e+01\n",
      "\n",
      "Test on train set: loss= 2.730264186859131 acc= 0.6837973594665527 auc= 0.9174772755837077\n",
      "Test on valid set: loss= 6.185917854309082 acc= 0.3731718957424164 auc= 0.8351111111111112\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9562e+01\n",
      "\n",
      "Test on train set: loss= 1.372813105583191 acc= 0.8257526755332947 auc= 0.9168782828187197\n",
      "Test on valid set: loss= 4.822987079620361 acc= 0.43838152289390564 auc= 0.8331111111111111\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8270e+01\n",
      "\n",
      "Test on train set: loss= 2.285045862197876 acc= 0.7933797240257263 auc= 0.9009611016814425\n",
      "Test on valid set: loss= 4.782032012939453 acc= 0.541586697101593 auc= 0.8726666666666667\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.2400e+01\n",
      "\n",
      "Test on train set: loss= 1.9403151273727417 acc= 0.804386556148529 auc= 0.8874334439598555\n",
      "Test on valid set: loss= 4.899129867553711 acc= 0.47251906991004944 auc= 0.8453333333333333\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6762e+01\n",
      "\n",
      "Test on train set: loss= 1.3859658241271973 acc= 0.8386209011077881 auc= 0.9239552208432352\n",
      "Test on valid set: loss= 5.658026218414307 acc= 0.47426024079322815 auc= 0.8133333333333332\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5608e+01\n",
      "\n",
      "Test on train set: loss= 1.6361515522003174 acc= 0.8160408139228821 auc= 0.8818146143311258\n",
      "Test on valid set: loss= 5.542084217071533 acc= 0.4778288006782532 auc= 0.7933333333333332\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4901e+01\n",
      "\n",
      "Test on train set: loss= 1.3968068361282349 acc= 0.8411298394203186 auc= 0.916494777340151\n",
      "Test on valid set: loss= 5.154422760009766 acc= 0.5367881059646606 auc= 0.818\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1845e+01\n",
      "\n",
      "Test on train set: loss= 1.4792910814285278 acc= 0.8476853370666504 auc= 0.9324824791139296\n",
      "Test on valid set: loss= 3.832090377807617 acc= 0.5286674499511719 auc= 0.8402222222222221\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9985e+01\n",
      "\n",
      "Test on train set: loss= 1.4269880056381226 acc= 0.8290709257125854 auc= 0.945686396651731\n",
      "Test on valid set: loss= 5.204740047454834 acc= 0.4630414843559265 auc= 0.8246666666666667\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1612e+01\n",
      "\n",
      "Test on train set: loss= 1.6645420789718628 acc= 0.8183069229125977 auc= 0.8555432611561352\n",
      "Test on valid set: loss= 5.8330464363098145 acc= 0.4702295660972595 auc= 0.7631111111111111\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3623e+01\n",
      "\n",
      "Test on train set: loss= 1.1761362552642822 acc= 0.8454192280769348 auc= 0.9441706794653897\n",
      "Test on valid set: loss= 4.334250450134277 acc= 0.48708075284957886 auc= 0.8417777777777777\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5053e+01\n",
      "\n",
      "Test on train set: loss= 1.491952657699585 acc= 0.8484946489334106 auc= 0.9262934529993846\n",
      "Test on valid set: loss= 5.62710428237915 acc= 0.49733781814575195 auc= 0.8204444444444443\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5873e+01\n",
      "\n",
      "Test on train set: loss= 2.348562717437744 acc= 0.7811589241027832 auc= 0.9130480861180089\n",
      "Test on valid set: loss= 6.935833930969238 acc= 0.3664166033267975 auc= 0.8302222222222222\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.4996e+01\n",
      "\n",
      "Test on train set: loss= 1.8300611972808838 acc= 0.8074619770050049 auc= 0.9216127169240549\n",
      "Test on valid set: loss= 6.717624664306641 acc= 0.44645634293556213 auc= 0.7697777777777778\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5703e+01\n",
      "\n",
      "Test on train set: loss= 1.2744741439819336 acc= 0.8315798044204712 auc= 0.9532889093458721\n",
      "Test on valid set: loss= 3.8349242210388184 acc= 0.5989542007446289 auc= 0.8946666666666667\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4957e+01\n",
      "\n",
      "Test on train set: loss= 1.6450475454330444 acc= 0.8269666433334351 auc= 0.8979266733334775\n",
      "Test on valid set: loss= 6.356762886047363 acc= 0.4276890754699707 auc= 0.7906666666666667\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3066e+01\n",
      "\n",
      "Test on train set: loss= 1.635135531425476 acc= 0.8140984177589417 auc= 0.9407762306451282\n",
      "Test on valid set: loss= 5.2414231300354 acc= 0.46490639448165894 auc= 0.8648888888888889\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.0129e+01\n",
      "\n",
      "Test on train set: loss= 1.9793123006820679 acc= 0.8224344253540039 auc= 0.9409088652870589\n",
      "Test on valid set: loss= 4.415403366088867 acc= 0.6091474294662476 auc= 0.913111111111111\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.2810e+01\n",
      "\n",
      "Test on train set: loss= 1.898199439048767 acc= 0.8333603143692017 auc= 0.9314392593890257\n",
      "Test on valid set: loss= 5.599505424499512 acc= 0.5345508456230164 auc= 0.8364444444444444\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.5847e+01\n",
      "\n",
      "Test on train set: loss= 1.9043922424316406 acc= 0.834817111492157 auc= 0.9191138938562577\n",
      "Test on valid set: loss= 6.170018196105957 acc= 0.5396520495414734 auc= 0.861111111111111\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4800e+01\n",
      "\n",
      "Test on train set: loss= 2.34442138671875 acc= 0.7780025601387024 auc= 0.8976071344654137\n",
      "Test on valid set: loss= 5.834751605987549 acc= 0.5050625205039978 auc= 0.8273333333333334\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.6268e+01\n",
      "\n",
      "Test on train set: loss= 1.5904102325439453 acc= 0.8477662801742554 auc= 0.9074988689475034\n",
      "Test on valid set: loss= 4.640944957733154 acc= 0.5745137929916382 auc= 0.8977777777777776\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7937e+01\n",
      "\n",
      "Test on train set: loss= 1.9138422012329102 acc= 0.8058432936668396 auc= 0.9245698726702074\n",
      "Test on valid set: loss= 3.364753484725952 acc= 0.6335445642471313 auc= 0.9019999999999998\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.3371e+01\n",
      "\n",
      "Test on train set: loss= 1.845317006111145 acc= 0.8245387077331543 auc= 0.9356755279122604\n",
      "Test on valid set: loss= 5.422422885894775 acc= 0.536532998085022 auc= 0.8786666666666667\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.9270e+01\n",
      "\n",
      "Test on train set: loss= 1.4812840223312378 acc= 0.861524760723114 auc= 0.914072344111289\n",
      "Test on valid set: loss= 6.615325450897217 acc= 0.5188423991203308 auc= 0.8195555555555556\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.8981e+01\n",
      "\n",
      "Test on train set: loss= 1.8513364791870117 acc= 0.8410488963127136 auc= 0.8890177040024347\n",
      "Test on valid set: loss= 6.097173690795898 acc= 0.5425087213516235 auc= 0.8464444444444446\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.9167e+01\n",
      "\n",
      "Test on train set: loss= 1.8778437376022339 acc= 0.8326319456100464 auc= 0.8971285198826198\n",
      "Test on valid set: loss= 6.747298717498779 acc= 0.3888380825519562 auc= 0.7908888888888889\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8126e+01\n",
      "\n",
      "Test on train set: loss= 1.5038350820541382 acc= 0.8391874432563782 auc= 0.9252452012765984\n",
      "Test on valid set: loss= 4.704823970794678 acc= 0.5637558102607727 auc= 0.8691111111111111\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8407e+01\n",
      "\n",
      "Test on train set: loss= 1.8787225484848022 acc= 0.8249433636665344 auc= 0.9177509730167186\n",
      "Test on valid set: loss= 6.037937164306641 acc= 0.46370619535446167 auc= 0.8228888888888889\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4106e+01\n",
      "\n",
      "Test on train set: loss= 1.8399931192398071 acc= 0.8250243067741394 auc= 0.9132966770405705\n",
      "Test on valid set: loss= 6.204481601715088 acc= 0.43045902252197266 auc= 0.834\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9973e+01\n",
      "\n",
      "Test on train set: loss= 1.5921316146850586 acc= 0.8517319560050964 auc= 0.9055588578220286\n",
      "Test on valid set: loss= 4.580545425415039 acc= 0.5929518938064575 auc= 0.9008888888888889\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.0850e+01\n",
      "\n",
      "Test on train set: loss= 1.5097918510437012 acc= 0.8660569787025452 auc= 0.9182711556767675\n",
      "Test on valid set: loss= 5.742546558380127 acc= 0.5156561136245728 auc= 0.8597777777777778\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0716e+01\n",
      "\n",
      "Test on train set: loss= 1.696482539176941 acc= 0.8332793712615967 auc= 0.9443771880774389\n",
      "Test on valid set: loss= 5.150804042816162 acc= 0.5388996601104736 auc= 0.868\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.0456e+01\n",
      "\n",
      "Test on train set: loss= 1.6286171674728394 acc= 0.861767590045929 auc= 0.8914766461917785\n",
      "Test on valid set: loss= 5.60476541519165 acc= 0.5741012692451477 auc= 0.8531111111111113\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7167e+01\n",
      "\n",
      "Test on train set: loss= 1.5438469648361206 acc= 0.8608773350715637 auc= 0.9393497185636818\n",
      "Test on valid set: loss= 5.27491569519043 acc= 0.5779420733451843 auc= 0.8511111111111112\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.0527e+01\n",
      "\n",
      "Test on train set: loss= 1.929323673248291 acc= 0.8287471532821655 auc= 0.8973377861375713\n",
      "Test on valid set: loss= 6.622276782989502 acc= 0.4357697069644928 auc= 0.8255555555555556\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.0422e+01\n",
      "\n",
      "Test on train set: loss= 1.5821185111999512 acc= 0.8518128991127014 auc= 0.9411698785771272\n",
      "Test on valid set: loss= 6.117160797119141 acc= 0.5016050934791565 auc= 0.8628888888888889\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.7539e+01\n",
      "\n",
      "Test on train set: loss= 1.7291254997253418 acc= 0.8518938422203064 auc= 0.9160017280952552\n",
      "Test on valid set: loss= 5.36978816986084 acc= 0.576525866985321 auc= 0.8395555555555555\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.8121e+01\n",
      "\n",
      "Test on train set: loss= 1.5829989910125732 acc= 0.8467141389846802 auc= 0.9523729087244799\n",
      "Test on valid set: loss= 4.763958930969238 acc= 0.580919623374939 auc= 0.89\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.1486e+01\n",
      "\n",
      "Test on train set: loss= 1.6051257848739624 acc= 0.8356264233589172 auc= 0.9418453659350531\n",
      "Test on valid set: loss= 5.680313587188721 acc= 0.5172182321548462 auc= 0.8597777777777778\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6942e+01\n",
      "\n",
      "Test on train set: loss= 1.2747019529342651 acc= 0.8522984981536865 auc= 0.9512676171221566\n",
      "Test on valid set: loss= 4.892037391662598 acc= 0.5414189100265503 auc= 0.8773333333333333\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.8555e+01\n",
      "\n",
      "Test on train set: loss= 2.2339470386505127 acc= 0.8180640935897827 auc= 0.8769683184826654\n",
      "Test on valid set: loss= 7.986833572387695 acc= 0.4472993016242981 auc= 0.7904444444444444\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4234e+01\n",
      "\n",
      "Test on train set: loss= 1.9482197761535645 acc= 0.8255098462104797 auc= 0.9178731806407256\n",
      "Test on valid set: loss= 6.317727088928223 acc= 0.4775691628456116 auc= 0.8482222222222221\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8942e+01\n",
      "\n",
      "Test on train set: loss= 1.869824767112732 acc= 0.8399967551231384 auc= 0.9199256210298081\n",
      "Test on valid set: loss= 4.706040859222412 acc= 0.6169023513793945 auc= 0.8804444444444446\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1097e+01\n",
      "\n",
      "Test on train set: loss= 2.5908477306365967 acc= 0.7821301221847534 auc= 0.8718138916987449\n",
      "Test on valid set: loss= 7.908812046051025 acc= 0.4177512526512146 auc= 0.7673333333333334\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.4968e+01\n",
      "\n",
      "Test on train set: loss= 1.4579473733901978 acc= 0.8693752288818359 auc= 0.9317853412991633\n",
      "Test on valid set: loss= 5.263055324554443 acc= 0.5156886577606201 auc= 0.8995555555555557\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.2500e+01\n",
      "\n",
      "Test on train set: loss= 2.2564637660980225 acc= 0.8191971778869629 auc= 0.8945416247083134\n",
      "Test on valid set: loss= 6.668779373168945 acc= 0.5227210521697998 auc= 0.8137777777777778\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.1504e+01\n",
      "\n",
      "Test on train set: loss= 1.9126235246658325 acc= 0.8412107229232788 auc= 0.9127630115011971\n",
      "Test on valid set: loss= 6.55693244934082 acc= 0.48245248198509216 auc= 0.8402222222222221\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.1868e+01\n",
      "\n",
      "Test on train set: loss= 1.3382105827331543 acc= 0.8824052810668945 auc= 0.9338252284001125\n",
      "Test on valid set: loss= 5.846647262573242 acc= 0.5452457070350647 auc= 0.8684444444444445\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.7779e+01\n",
      "\n",
      "Test on train set: loss= 1.7058988809585571 acc= 0.8522175550460815 auc= 0.9386507923023306\n",
      "Test on valid set: loss= 7.3431477546691895 acc= 0.41893696784973145 auc= 0.8293333333333333\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.1054e+01\n",
      "\n",
      "Test on train set: loss= 1.8062363862991333 acc= 0.8405632972717285 auc= 0.9112255403489723\n",
      "Test on valid set: loss= 6.619083881378174 acc= 0.4844852387905121 auc= 0.8402222222222223\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.3709e+01\n",
      "\n",
      "Test on train set: loss= 1.3458787202835083 acc= 0.8736646175384521 auc= 0.9518980727355079\n",
      "Test on valid set: loss= 6.0765204429626465 acc= 0.5317767858505249 auc= 0.85\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0413e+02\n",
      "\n",
      "Test on train set: loss= 1.5370759963989258 acc= 0.8603107929229736 auc= 0.9352437528046804\n",
      "Test on valid set: loss= 5.892559051513672 acc= 0.56293785572052 auc= 0.8715555555555555\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2069e+02\n",
      "\n",
      "Test on train set: loss= 1.4203338623046875 acc= 0.8709938526153564 auc= 0.9460060478437597\n",
      "Test on valid set: loss= 5.728425979614258 acc= 0.5710428357124329 auc= 0.9033333333333333\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1879e+02\n",
      "\n",
      "Test on train set: loss= 1.58785080909729 acc= 0.8527031540870667 auc= 0.9554750439910643\n",
      "Test on valid set: loss= 5.006826877593994 acc= 0.599088728427887 auc= 0.8877777777777777\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.9875e+01\n",
      "\n",
      "Test on train set: loss= 2.0660622119903564 acc= 0.8326319456100464 auc= 0.9418997758193909\n",
      "Test on valid set: loss= 7.974571704864502 acc= 0.46063488721847534 auc= 0.813111111111111\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4910e+01\n",
      "\n",
      "Test on train set: loss= 2.5786495208740234 acc= 0.7955648899078369 auc= 0.9420417064352776\n",
      "Test on valid set: loss= 5.856728553771973 acc= 0.5536739230155945 auc= 0.8811111111111114\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.1134e+01\n",
      "\n",
      "Test on train set: loss= 1.9347933530807495 acc= 0.8465522527694702 auc= 0.9253140973572279\n",
      "Test on valid set: loss= 7.200543403625488 acc= 0.4765892028808594 auc= 0.8255555555555556\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.0724e+01\n",
      "\n",
      "Test on train set: loss= 1.7643054723739624 acc= 0.8478472232818604 auc= 0.9237669111264288\n",
      "Test on valid set: loss= 6.052852630615234 acc= 0.5201154947280884 auc= 0.818\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3855e+01\n",
      "\n",
      "Test on train set: loss= 1.990281105041504 acc= 0.8397539854049683 auc= 0.9231906739580598\n",
      "Test on valid set: loss= 5.9205641746521 acc= 0.5416741967201233 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.5779e+01\n",
      "\n",
      "Test on train set: loss= 1.6720085144042969 acc= 0.861443817615509 auc= 0.9040420463987926\n",
      "Test on valid set: loss= 6.426153182983398 acc= 0.5224684476852417 auc= 0.8597777777777778\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4563e+01\n",
      "\n",
      "Test on train set: loss= 1.6762652397155762 acc= 0.8531887531280518 auc= 0.9368494149867747\n",
      "Test on valid set: loss= 4.769628524780273 acc= 0.6458483934402466 auc= 0.894888888888889\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4493e+01\n",
      "\n",
      "Test on train set: loss= 1.3708575963974 acc= 0.8790061473846436 auc= 0.9364250489225885\n",
      "Test on valid set: loss= 3.9960174560546875 acc= 0.6349890232086182 auc= 0.9022222222222224\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.3771e+01\n",
      "\n",
      "Test on train set: loss= 1.5911442041397095 acc= 0.8491421341896057 auc= 0.9388527048247681\n",
      "Test on valid set: loss= 5.0505805015563965 acc= 0.5892871618270874 auc= 0.8880000000000001\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.6626e+01\n",
      "\n",
      "Test on train set: loss= 1.5206868648529053 acc= 0.8651667237281799 auc= 0.9447879786229787\n",
      "Test on valid set: loss= 5.499577045440674 acc= 0.5405879616737366 auc= 0.8762222222222222\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7793e+01\n",
      "\n",
      "Test on train set: loss= 1.6197729110717773 acc= 0.8405632972717285 auc= 0.9480476266522102\n",
      "Test on valid set: loss= 5.157394886016846 acc= 0.6198586225509644 auc= 0.8615555555555556\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.5260e+01\n",
      "\n",
      "Test on train set: loss= 1.5772323608398438 acc= 0.8778731226921082 auc= 0.9146875651066664\n",
      "Test on valid set: loss= 4.652015209197998 acc= 0.6482893824577332 auc= 0.8991111111111112\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.1996e+01\n",
      "\n",
      "Test on train set: loss= 1.7932188510894775 acc= 0.8372450470924377 auc= 0.9342754230001706\n",
      "Test on valid set: loss= 5.106030464172363 acc= 0.5509119033813477 auc= 0.8435555555555556\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.2061e+01\n",
      "\n",
      "Test on train set: loss= 1.6865646839141846 acc= 0.8357073664665222 auc= 0.9327056766822144\n",
      "Test on valid set: loss= 5.572230815887451 acc= 0.5279268026351929 auc= 0.8208888888888888\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.1031e+01\n",
      "\n",
      "Test on train set: loss= 1.7550320625305176 acc= 0.853836178779602 auc= 0.8568314466151034\n",
      "Test on valid set: loss= 6.690042495727539 acc= 0.5190442204475403 auc= 0.8064444444444444\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.8500e+01\n",
      "\n",
      "Test on train set: loss= 1.569931983947754 acc= 0.8518128991127014 auc= 0.9526455393160008\n",
      "Test on valid set: loss= 4.817289352416992 acc= 0.5940111875534058 auc= 0.868\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.7410e+01\n",
      "\n",
      "Test on train set: loss= 2.051011800765991 acc= 0.8395920991897583 auc= 0.9040637693087931\n",
      "Test on valid set: loss= 8.121753692626953 acc= 0.41267627477645874 auc= 0.7946666666666667\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.8066e+01\n",
      "\n",
      "Test on train set: loss= 1.1602320671081543 acc= 0.9012625217437744 auc= 0.9355104002843333\n",
      "Test on valid set: loss= 5.686316967010498 acc= 0.5285865068435669 auc= 0.8726666666666667\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.6681e+01\n",
      "\n",
      "Test on train set: loss= 1.6085008382797241 acc= 0.8786014914512634 auc= 0.909066159287034\n",
      "Test on valid set: loss= 7.288601875305176 acc= 0.5040831565856934 auc= 0.8488888888888889\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2084e+02\n",
      "\n",
      "Test on train set: loss= 1.2969419956207275 acc= 0.8919553160667419 auc= 0.9459294648098597\n",
      "Test on valid set: loss= 4.948408603668213 acc= 0.612928569316864 auc= 0.8602222222222222\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0083e+02\n",
      "\n",
      "Test on train set: loss= 1.912160873413086 acc= 0.8487374782562256 auc= 0.9207840000954045\n",
      "Test on valid set: loss= 6.205747127532959 acc= 0.5016109943389893 auc= 0.8360000000000001\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.7116e+01\n",
      "\n",
      "Test on train set: loss= 2.0811421871185303 acc= 0.8278568983078003 auc= 0.9239371155880521\n",
      "Test on valid set: loss= 5.793570518493652 acc= 0.5299965143203735 auc= 0.8726666666666668\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2997e+02\n",
      "\n",
      "Test on train set: loss= 1.9646574258804321 acc= 0.8476043939590454 auc= 0.9138244317174611\n",
      "Test on valid set: loss= 5.62545919418335 acc= 0.5395995378494263 auc= 0.8586666666666666\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.0048e+01\n",
      "\n",
      "Test on train set: loss= 1.943692922592163 acc= 0.8522984981536865 auc= 0.9258855395116541\n",
      "Test on valid set: loss= 6.295435905456543 acc= 0.5569848418235779 auc= 0.844\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.5303e+01\n",
      "\n",
      "Test on train set: loss= 1.417543888092041 acc= 0.8837811350822449 auc= 0.9616328834468482\n",
      "Test on valid set: loss= 7.009904861450195 acc= 0.5126440525054932 auc= 0.8286666666666667\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1875e+02\n",
      "\n",
      "Test on train set: loss= 1.6999859809875488 acc= 0.8524603247642517 auc= 0.9478167287113186\n",
      "Test on valid set: loss= 5.466076850891113 acc= 0.5737112760543823 auc= 0.881111111111111\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.1981e+01\n",
      "\n",
      "Test on train set: loss= 1.514649510383606 acc= 0.8708319664001465 auc= 0.935438403443173\n",
      "Test on valid set: loss= 5.557941913604736 acc= 0.5776848793029785 auc= 0.8646666666666667\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3112e+02\n",
      "\n",
      "Test on train set: loss= 1.4638559818267822 acc= 0.865814208984375 auc= 0.9610576591690562\n",
      "Test on valid set: loss= 4.941564083099365 acc= 0.5856626033782959 auc= 0.8928888888888888\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.9529e+01\n",
      "\n",
      "Test on train set: loss= 1.6855171918869019 acc= 0.8607963919639587 auc= 0.9528388160323547\n",
      "Test on valid set: loss= 6.3637309074401855 acc= 0.5397166013717651 auc= 0.8906666666666666\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.2873e+01\n",
      "\n",
      "Test on train set: loss= 1.4939167499542236 acc= 0.8718841075897217 auc= 0.9510225662827863\n",
      "Test on valid set: loss= 6.031633377075195 acc= 0.5599023699760437 auc= 0.8788888888888889\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1506e+02\n",
      "\n",
      "Test on train set: loss= 1.4570385217666626 acc= 0.8752832412719727 auc= 0.9541213990914482\n",
      "Test on valid set: loss= 6.82932186126709 acc= 0.470244824886322 auc= 0.8853333333333332\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0121e+02\n",
      "\n",
      "Test on train set: loss= 1.4554215669631958 acc= 0.8694561123847961 auc= 0.970977102930432\n",
      "Test on valid set: loss= 5.127874851226807 acc= 0.5884918570518494 auc= 0.913111111111111\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.1157e+01\n",
      "\n",
      "Test on train set: loss= 1.3735798597335815 acc= 0.8812722563743591 auc= 0.9594449793621223\n",
      "Test on valid set: loss= 4.595029354095459 acc= 0.6992552876472473 auc= 0.8866666666666667\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.3851e+01\n",
      "\n",
      "Test on train set: loss= 1.4855620861053467 acc= 0.8755260705947876 auc= 0.9224891137802238\n",
      "Test on valid set: loss= 6.066267490386963 acc= 0.590272068977356 auc= 0.8575555555555556\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.6540e+01\n",
      "\n",
      "Test on train set: loss= 1.3099379539489746 acc= 0.8947070240974426 auc= 0.9624608975204019\n",
      "Test on valid set: loss= 6.757853984832764 acc= 0.5216869115829468 auc= 0.898\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.0269e+01\n",
      "\n",
      "Test on train set: loss= 1.6206926107406616 acc= 0.8700226545333862 auc= 0.9524112004163892\n",
      "Test on valid set: loss= 6.642581939697266 acc= 0.5421898365020752 auc= 0.8768888888888888\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1402e+02\n",
      "\n",
      "Test on train set: loss= 1.5100040435791016 acc= 0.8762544393539429 auc= 0.9578705303618223\n",
      "Test on valid set: loss= 5.609313488006592 acc= 0.6184154152870178 auc= 0.9091111111111113\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0988e+02\n",
      "\n",
      "Test on train set: loss= 1.453533411026001 acc= 0.8837811350822449 auc= 0.9528380141065735\n",
      "Test on valid set: loss= 6.774633884429932 acc= 0.5381854772567749 auc= 0.8686666666666667\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.3153e+01\n",
      "\n",
      "Test on train set: loss= 1.5858030319213867 acc= 0.8752023577690125 auc= 0.9420255562120268\n",
      "Test on valid set: loss= 4.182483673095703 acc= 0.6900960206985474 auc= 0.8864444444444445\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.0242e+01\n",
      "\n",
      "Test on train set: loss= 2.086905002593994 acc= 0.8327937722206116 auc= 0.9278602279597983\n",
      "Test on valid set: loss= 7.208812236785889 acc= 0.4669207036495209 auc= 0.8124444444444444\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.6907e+01\n",
      "\n",
      "Test on train set: loss= 1.751807689666748 acc= 0.8488993048667908 auc= 0.9491216550671673\n",
      "Test on valid set: loss= 5.952438831329346 acc= 0.5464502573013306 auc= 0.876\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0152e+02\n",
      "\n",
      "Test on train set: loss= 1.5821276903152466 acc= 0.8607154488563538 auc= 0.9557158671240268\n",
      "Test on valid set: loss= 5.551937103271484 acc= 0.5952138304710388 auc= 0.8733333333333334\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1469e+02\n",
      "\n",
      "Test on train set: loss= 1.7562134265899658 acc= 0.8587730526924133 auc= 0.9162881247688558\n",
      "Test on valid set: loss= 7.740578651428223 acc= 0.4575851559638977 auc= 0.776888888888889\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1150e+02\n",
      "\n",
      "Test on train set: loss= 1.4847313165664673 acc= 0.8837002515792847 auc= 0.9406059863614205\n",
      "Test on valid set: loss= 5.134709358215332 acc= 0.6242469549179077 auc= 0.8711111111111111\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4609e+02\n",
      "\n",
      "Test on train set: loss= 1.7625659704208374 acc= 0.8679993748664856 auc= 0.8962936738495657\n",
      "Test on valid set: loss= 5.419165134429932 acc= 0.5972709655761719 auc= 0.8817777777777778\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1551e+02\n",
      "\n",
      "Test on train set: loss= 1.4763175249099731 acc= 0.8771446943283081 auc= 0.9617983249305265\n",
      "Test on valid set: loss= 6.507625579833984 acc= 0.5377928018569946 auc= 0.8593333333333332\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.5880e+01\n",
      "\n",
      "Test on train set: loss= 1.769844889640808 acc= 0.8624959588050842 auc= 0.947413322128903\n",
      "Test on valid set: loss= 6.350487232208252 acc= 0.5491572618484497 auc= 0.8426666666666666\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3345e+02\n",
      "\n",
      "Test on train set: loss= 1.7050164937973022 acc= 0.8697798848152161 auc= 0.9246412160174104\n",
      "Test on valid set: loss= 5.781686782836914 acc= 0.5954764485359192 auc= 0.8673333333333334\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0675e+02\n",
      "\n",
      "Test on train set: loss= 1.5028212070465088 acc= 0.888394296169281 auc= 0.9542386929897745\n",
      "Test on valid set: loss= 7.175217151641846 acc= 0.5200253129005432 auc= 0.8488888888888889\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.4306e+02\n",
      "\n",
      "Test on train set: loss= 1.3759989738464355 acc= 0.8951116800308228 auc= 0.9579586925653821\n",
      "Test on valid set: loss= 6.285842418670654 acc= 0.5737524032592773 auc= 0.8988888888888888\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2743e+02\n",
      "\n",
      "Test on train set: loss= 1.395546555519104 acc= 0.8794108033180237 auc= 0.9708809050190423\n",
      "Test on valid set: loss= 4.878403186798096 acc= 0.6438418030738831 auc= 0.9064444444444446\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0343e+02\n",
      "\n",
      "Test on train set: loss= 1.4454047679901123 acc= 0.8896892070770264 auc= 0.9419318109384986\n",
      "Test on valid set: loss= 5.946177959442139 acc= 0.5947890281677246 auc= 0.860888888888889\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.4571e+01\n",
      "\n",
      "Test on train set: loss= 1.4820307493209839 acc= 0.8838620781898499 auc= 0.9530559984493554\n",
      "Test on valid set: loss= 5.925147533416748 acc= 0.5950468182563782 auc= 0.8933333333333333\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.9146e+01\n",
      "\n",
      "Test on train set: loss= 2.0429189205169678 acc= 0.8432340621948242 auc= 0.9381348822582976\n",
      "Test on valid set: loss= 6.912556171417236 acc= 0.5296475291252136 auc= 0.8231111111111111\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.9494e+01\n",
      "\n",
      "Test on train set: loss= 1.507851243019104 acc= 0.8812722563743591 auc= 0.9395333961997279\n",
      "Test on valid set: loss= 6.5188493728637695 acc= 0.5226148962974548 auc= 0.8595555555555554\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0723e+02\n",
      "\n",
      "Test on train set: loss= 1.532153844833374 acc= 0.8695370554924011 auc= 0.9477476447192276\n",
      "Test on valid set: loss= 4.458385944366455 acc= 0.6517050266265869 auc= 0.9039999999999999\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2276e+02\n",
      "\n",
      "Test on train set: loss= 1.349271297454834 acc= 0.8929265141487122 auc= 0.9443171996852137\n",
      "Test on valid set: loss= 5.8080058097839355 acc= 0.5574638247489929 auc= 0.9033333333333333\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.7907e+01\n",
      "\n",
      "Test on train set: loss= 1.6024969816207886 acc= 0.8654094934463501 auc= 0.94384873657679\n",
      "Test on valid set: loss= 5.256473541259766 acc= 0.5938482284545898 auc= 0.8957777777777778\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.4686e+01\n",
      "\n",
      "Test on train set: loss= 1.5249965190887451 acc= 0.8702654838562012 auc= 0.957979826658782\n",
      "Test on valid set: loss= 4.872406482696533 acc= 0.5673806667327881 auc= 0.8824444444444446\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.5641e+01\n",
      "\n",
      "Test on train set: loss= 1.5854510068893433 acc= 0.8738265037536621 auc= 0.9558437328067496\n",
      "Test on valid set: loss= 6.205639839172363 acc= 0.5697648525238037 auc= 0.8664444444444444\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.9816e+01\n",
      "\n",
      "Test on train set: loss= 1.6648188829421997 acc= 0.8655713796615601 auc= 0.9566603303423955\n",
      "Test on valid set: loss= 5.20320987701416 acc= 0.5843407511711121 auc= 0.8944444444444445\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.4107e+01\n",
      "\n",
      "Test on train set: loss= 1.32835054397583 acc= 0.8933311700820923 auc= 0.9667789148960064\n",
      "Test on valid set: loss= 5.460271835327148 acc= 0.6200596690177917 auc= 0.8893333333333334\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.2125e+01\n",
      "\n",
      "Test on train set: loss= 1.2696757316589355 acc= 0.8990773558616638 auc= 0.9604110495454681\n",
      "Test on valid set: loss= 5.3403520584106445 acc= 0.6172272562980652 auc= 0.9068888888888887\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -6.7677e+01\n",
      "\n",
      "Test on train set: loss= 1.0466340780258179 acc= 0.9109744429588318 auc= 0.9739225087639621\n",
      "Test on valid set: loss= 5.124672889709473 acc= 0.5948744416236877 auc= 0.9035555555555556\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.1984e+01\n",
      "\n",
      "Test on train set: loss= 1.035938024520874 acc= 0.9127549529075623 auc= 0.9742514967805395\n",
      "Test on valid set: loss= 5.31024169921875 acc= 0.6131054162979126 auc= 0.9148888888888891\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.5909e+01\n",
      "\n",
      "Test on train set: loss= 1.0704681873321533 acc= 0.9144545197486877 auc= 0.9746785311221654\n",
      "Test on valid set: loss= 5.166479110717773 acc= 0.6435284614562988 auc= 0.9126666666666667\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.7695e+01\n",
      "\n",
      "Test on train set: loss= 1.0977154970169067 acc= 0.9083036780357361 auc= 0.9703111545550749\n",
      "Test on valid set: loss= 5.1175384521484375 acc= 0.6510182023048401 auc= 0.9077777777777779\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.1142e+01\n",
      "\n",
      "Test on train set: loss= 0.9322600960731506 acc= 0.9244092106819153 auc= 0.9694158042503439\n",
      "Test on valid set: loss= 4.650081634521484 acc= 0.640534520149231 auc= 0.9297777777777778\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.9941e+01\n",
      "\n",
      "Test on train set: loss= 0.9374884366989136 acc= 0.9224668145179749 auc= 0.9749487690807017\n",
      "Test on valid set: loss= 5.581274509429932 acc= 0.6365790367126465 auc= 0.9122222222222222\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.8690e+01\n",
      "\n",
      "Test on train set: loss= 0.9379516243934631 acc= 0.9209290742874146 auc= 0.9693940882150835\n",
      "Test on valid set: loss= 5.421527862548828 acc= 0.5901416540145874 auc= 0.8822222222222222\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.8184e+01\n",
      "\n",
      "Test on train set: loss= 0.8598237037658691 acc= 0.9283748865127563 auc= 0.9684751585212783\n",
      "Test on valid set: loss= 5.088749885559082 acc= 0.6602160930633545 auc= 0.9011111111111111\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1157e+02\n",
      "\n",
      "Test on train set: loss= 0.9157325029373169 acc= 0.9219812154769897 auc= 0.9600297076705399\n",
      "Test on valid set: loss= 5.2226152420043945 acc= 0.6569772958755493 auc= 0.9042222222222224\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0325e+02\n",
      "\n",
      "Test on train set: loss= 0.9469875693321228 acc= 0.9206863045692444 auc= 0.9580944069953452\n",
      "Test on valid set: loss= 4.684275150299072 acc= 0.680018424987793 auc= 0.9006666666666666\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0648e+02\n",
      "\n",
      "Test on train set: loss= 0.9043737053871155 acc= 0.9232761263847351 auc= 0.9676718490198887\n",
      "Test on valid set: loss= 4.970661640167236 acc= 0.6436797380447388 auc= 0.9191111111111111\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.5552e+01\n",
      "\n",
      "Test on train set: loss= 0.8873167037963867 acc= 0.9267562031745911 auc= 0.9762143189490983\n",
      "Test on valid set: loss= 5.666949272155762 acc= 0.6179412007331848 auc= 0.9195555555555556\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0192e+02\n",
      "\n",
      "Test on train set: loss= 0.8376019597053528 acc= 0.9339591860771179 auc= 0.9724373155988868\n",
      "Test on valid set: loss= 5.490166664123535 acc= 0.6207640767097473 auc= 0.9408888888888889\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0097e+02\n",
      "\n",
      "Test on train set: loss= 0.8481668829917908 acc= 0.9282130002975464 auc= 0.9742563913306382\n",
      "Test on valid set: loss= 4.627511501312256 acc= 0.6773591637611389 auc= 0.9164444444444445\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0320e+02\n",
      "\n",
      "Test on train set: loss= 0.8279983997344971 acc= 0.9308028221130371 auc= 0.9701988722218935\n",
      "Test on valid set: loss= 4.517501354217529 acc= 0.6777746677398682 auc= 0.9255555555555555\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.3623e+01\n",
      "\n",
      "Test on train set: loss= 0.7634463310241699 acc= 0.9391388893127441 auc= 0.9727148382385022\n",
      "Test on valid set: loss= 5.331863880157471 acc= 0.6600010991096497 auc= 0.9268888888888889\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0113e+02\n",
      "\n",
      "Test on train set: loss= 0.81512451171875 acc= 0.9306409955024719 auc= 0.9731954441214224\n",
      "Test on valid set: loss= 4.848864555358887 acc= 0.6602111458778381 auc= 0.9259999999999999\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0474e+02\n",
      "\n",
      "Test on train set: loss= 0.808820366859436 acc= 0.9333117604255676 auc= 0.9628457683772087\n",
      "Test on valid set: loss= 4.867314338684082 acc= 0.6667596697807312 auc= 0.9315555555555555\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0632e+02\n",
      "\n",
      "Test on train set: loss= 0.8733009099960327 acc= 0.9282130002975464 auc= 0.9773637544346935\n",
      "Test on valid set: loss= 5.840402126312256 acc= 0.5876311659812927 auc= 0.9111111111111111\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.8354e+01\n",
      "\n",
      "Test on train set: loss= 0.8371461629867554 acc= 0.9329880475997925 auc= 0.9716157146975821\n",
      "Test on valid set: loss= 5.143181800842285 acc= 0.6796683073043823 auc= 0.9013333333333332\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0440e+02\n",
      "\n",
      "Test on train set: loss= 0.7912455201148987 acc= 0.9352541565895081 auc= 0.9762194573730753\n",
      "Test on valid set: loss= 4.717637538909912 acc= 0.6933768391609192 auc= 0.9431111111111111\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.4144e+01\n",
      "\n",
      "Test on train set: loss= 0.7920087575912476 acc= 0.9358206391334534 auc= 0.9738384987403723\n",
      "Test on valid set: loss= 4.599298000335693 acc= 0.6996980905532837 auc= 0.9426666666666665\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.6969e+01\n",
      "\n",
      "Test on train set: loss= 0.783897876739502 acc= 0.9336354732513428 auc= 0.9638749161002899\n",
      "Test on valid set: loss= 5.249111175537109 acc= 0.6451905965805054 auc= 0.9199999999999999\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.8309e+01\n",
      "\n",
      "Test on train set: loss= 0.766374409198761 acc= 0.9361444115638733 auc= 0.9656423306896071\n",
      "Test on valid set: loss= 5.061774730682373 acc= 0.6431156396865845 auc= 0.9326666666666666\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.9623e+01\n",
      "\n",
      "Test on train set: loss= 0.7595886588096619 acc= 0.9359825253486633 auc= 0.9739394054679653\n",
      "Test on valid set: loss= 4.521009922027588 acc= 0.680142879486084 auc= 0.9328888888888889\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1311e+02\n",
      "\n",
      "Test on train set: loss= 0.775542676448822 acc= 0.9351732134819031 auc= 0.9704037914690945\n",
      "Test on valid set: loss= 4.85328483581543 acc= 0.6800158619880676 auc= 0.9402222222222221\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -9.8928e+01\n",
      "\n",
      "Test on train set: loss= 0.7145830392837524 acc= 0.9399482011795044 auc= 0.9634458333616294\n",
      "Test on valid set: loss= 4.9640889167785645 acc= 0.6793017387390137 auc= 0.942\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0156e+02\n",
      "\n",
      "Test on train set: loss= 0.7184643745422363 acc= 0.9405147433280945 auc= 0.9711977761024386\n",
      "Test on valid set: loss= 4.984105110168457 acc= 0.6602951884269714 auc= 0.9497777777777777\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.0569e+02\n",
      "\n",
      "Test on train set: loss= 0.7558843493461609 acc= 0.9339591860771179 auc= 0.972845665206394\n",
      "Test on valid set: loss= 5.508777618408203 acc= 0.6398612260818481 auc= 0.9166666666666666\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1267e+02\n",
      "\n",
      "Test on train set: loss= 0.7465049028396606 acc= 0.9394626021385193 auc= 0.968916236233914\n",
      "Test on valid set: loss= 5.262976169586182 acc= 0.6572288274765015 auc= 0.9184444444444445\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2195e+02\n",
      "\n",
      "Test on train set: loss= 0.7117254137992859 acc= 0.9426999092102051 auc= 0.9701301494537115\n",
      "Test on valid set: loss= 5.353109359741211 acc= 0.655924916267395 auc= 0.9259999999999999\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2840e+02\n",
      "\n",
      "Test on train set: loss= 0.6957473754882812 acc= 0.9429426789283752 auc= 0.9705210629990214\n",
      "Test on valid set: loss= 4.952676296234131 acc= 0.6797839999198914 auc= 0.9202222222222222\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2434e+02\n",
      "\n",
      "Test on train set: loss= 0.7605406641960144 acc= 0.9413240551948547 auc= 0.9673073794989946\n",
      "Test on valid set: loss= 4.96890115737915 acc= 0.6800252199172974 auc= 0.9068888888888889\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1742e+02\n",
      "\n",
      "Test on train set: loss= 0.7234383821487427 acc= 0.9418096542358398 auc= 0.9638366341114812\n",
      "Test on valid set: loss= 5.517518997192383 acc= 0.6327135562896729 auc= 0.9331111111111111\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1561e+02\n",
      "\n",
      "Test on train set: loss= 0.7350003123283386 acc= 0.9412431120872498 auc= 0.9704178666327337\n",
      "Test on valid set: loss= 4.723040580749512 acc= 0.678955078125 auc= 0.9297777777777776\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2563e+02\n",
      "\n",
      "Test on train set: loss= 0.7535135746002197 acc= 0.9390579462051392 auc= 0.9736320071953054\n",
      "Test on valid set: loss= 4.944950103759766 acc= 0.6613637208938599 auc= 0.9197777777777778\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.1112e+02\n",
      "\n",
      "Test on train set: loss= 0.7716310620307922 acc= 0.9364681243896484 auc= 0.9756761630707432\n",
      "Test on valid set: loss= 4.715297698974609 acc= 0.6792142391204834 auc= 0.9104444444444443\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2069e+02\n",
      "\n",
      "Test on train set: loss= 0.6770387291908264 acc= 0.945613443851471 auc= 0.9774661477120263\n",
      "Test on valid set: loss= 5.014999866485596 acc= 0.6800001859664917 auc= 0.9397777777777778\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1677e+02\n",
      "\n",
      "Test on train set: loss= 0.7054100036621094 acc= 0.9451278448104858 auc= 0.9736032251889993\n",
      "Test on valid set: loss= 5.354539394378662 acc= 0.6599438786506653 auc= 0.9306666666666666\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2374e+02\n",
      "\n",
      "Test on train set: loss= 0.7154154777526855 acc= 0.9445613622665405 auc= 0.974532446604349\n",
      "Test on valid set: loss= 4.8551435470581055 acc= 0.6798135042190552 auc= 0.947111111111111\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2606e+02\n",
      "\n",
      "Test on train set: loss= 0.6761325001716614 acc= 0.9469084143638611 auc= 0.9710652340082605\n",
      "Test on valid set: loss= 5.338131904602051 acc= 0.637035071849823 auc= 0.904888888888889\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2250e+02\n",
      "\n",
      "Test on train set: loss= 0.6769590973854065 acc= 0.9468274712562561 auc= 0.9719720938004629\n",
      "Test on valid set: loss= 5.055738925933838 acc= 0.6798313856124878 auc= 0.9095555555555557\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2000e+02\n",
      "\n",
      "Test on train set: loss= 0.7083353996276855 acc= 0.9465846419334412 auc= 0.9757455780047666\n",
      "Test on valid set: loss= 5.275076389312744 acc= 0.6600258350372314 auc= 0.9215555555555556\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1809e+02\n",
      "\n",
      "Test on train set: loss= 0.6500310897827148 acc= 0.946179986000061 auc= 0.9783667292919151\n",
      "Test on valid set: loss= 4.622720241546631 acc= 0.6768904328346252 auc= 0.944\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.3642e+02\n",
      "\n",
      "Test on train set: loss= 0.6955243349075317 acc= 0.9440757632255554 auc= 0.974166665732505\n",
      "Test on valid set: loss= 4.802048683166504 acc= 0.6797040104866028 auc= 0.9235555555555554\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2598e+02\n",
      "\n",
      "Test on train set: loss= 0.715088427066803 acc= 0.9406765699386597 auc= 0.9693926786349797\n",
      "Test on valid set: loss= 4.7811598777771 acc= 0.6536751985549927 auc= 0.919777777777778\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2135e+02\n",
      "\n",
      "Test on train set: loss= 0.6866177916526794 acc= 0.942295253276825 auc= 0.9727445873193412\n",
      "Test on valid set: loss= 4.955718517303467 acc= 0.6591939330101013 auc= 0.9204444444444444\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1550e+02\n",
      "\n",
      "Test on train set: loss= 0.7139534950256348 acc= 0.9423761963844299 auc= 0.9812629846682194\n",
      "Test on valid set: loss= 5.257458686828613 acc= 0.6401487588882446 auc= 0.906\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1604e+02\n",
      "\n",
      "Test on train set: loss= 0.6268345713615417 acc= 0.9489316940307617 auc= 0.986609424925032\n",
      "Test on valid set: loss= 5.505565643310547 acc= 0.6394049525260925 auc= 0.9146666666666666\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.1566e+02\n",
      "\n",
      "Test on train set: loss= 0.6657392978668213 acc= 0.9469084143638611 auc= 0.9789813727642589\n",
      "Test on valid set: loss= 5.362620830535889 acc= 0.6479848027229309 auc= 0.9326666666666666\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2153e+02\n",
      "\n",
      "Test on train set: loss= 0.6778472661972046 acc= 0.9452897310256958 auc= 0.9798584501544834\n",
      "Test on valid set: loss= 5.063772678375244 acc= 0.6798416972160339 auc= 0.9311111111111112\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2630e+02\n",
      "\n",
      "Test on train set: loss= 0.6578031778335571 acc= 0.9473130702972412 auc= 0.9777158784672035\n",
      "Test on valid set: loss= 5.129962921142578 acc= 0.6745156049728394 auc= 0.9411111111111111\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1556e+02\n",
      "\n",
      "Test on train set: loss= 0.664130449295044 acc= 0.9477177262306213 auc= 0.9780207167605344\n",
      "Test on valid set: loss= 4.749216079711914 acc= 0.6797860860824585 auc= 0.9404444444444444\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1271e+02\n",
      "\n",
      "Test on train set: loss= 0.6395571231842041 acc= 0.9487698078155518 auc= 0.9779912604439651\n",
      "Test on valid set: loss= 4.740540027618408 acc= 0.6619982123374939 auc= 0.9299999999999999\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.1648e+02\n",
      "\n",
      "Test on train set: loss= 0.6162897944450378 acc= 0.9512787461280823 auc= 0.9791489893284137\n",
      "Test on valid set: loss= 4.634172439575195 acc= 0.6876831650733948 auc= 0.9222222222222223\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.0627e+02\n",
      "\n",
      "Test on train set: loss= 0.5881817936897278 acc= 0.9511168599128723 auc= 0.981859649242647\n",
      "Test on valid set: loss= 4.633413314819336 acc= 0.6984513998031616 auc= 0.9268888888888889\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1364e+02\n",
      "\n",
      "Test on train set: loss= 0.5714250802993774 acc= 0.9527354836463928 auc= 0.9826188558525747\n",
      "Test on valid set: loss= 4.902315616607666 acc= 0.680111289024353 auc= 0.9335555555555557\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1052e+02\n",
      "\n",
      "Test on train set: loss= 0.6190482378005981 acc= 0.9469892978668213 auc= 0.9799378854807956\n",
      "Test on valid set: loss= 4.566706657409668 acc= 0.681898295879364 auc= 0.9086666666666666\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1710e+02\n",
      "\n",
      "Test on train set: loss= 0.6791649460792542 acc= 0.9430236220359802 auc= 0.9796033574455624\n",
      "Test on valid set: loss= 4.614956855773926 acc= 0.6703511476516724 auc= 0.9233333333333335\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1700e+02\n",
      "\n",
      "Test on train set: loss= 0.6870572566986084 acc= 0.9429426789283752 auc= 0.9793367907567907\n",
      "Test on valid set: loss= 4.850339889526367 acc= 0.6424227356910706 auc= 0.9157777777777778\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.0739e+02\n",
      "\n",
      "Test on train set: loss= 0.6383125185966492 acc= 0.9465036988258362 auc= 0.980884493925869\n",
      "Test on valid set: loss= 4.674196243286133 acc= 0.658797562122345 auc= 0.9182222222222223\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.0327e+02\n",
      "\n",
      "Test on train set: loss= 0.5895546674728394 acc= 0.9513596892356873 auc= 0.9825545897812786\n",
      "Test on valid set: loss= 4.705209255218506 acc= 0.677791953086853 auc= 0.918666666666667\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.0178e+02\n",
      "\n",
      "Test on train set: loss= 0.592703640460968 acc= 0.9514405727386475 auc= 0.9826933651174109\n",
      "Test on valid set: loss= 4.908629894256592 acc= 0.6624482870101929 auc= 0.9151111111111112\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.0689e+02\n",
      "\n",
      "Test on train set: loss= 0.5965685248374939 acc= 0.9484460949897766 auc= 0.979952772349578\n",
      "Test on valid set: loss= 4.7315168380737305 acc= 0.673946738243103 auc= 0.9266666666666667\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1112e+02\n",
      "\n",
      "Test on train set: loss= 0.6886746287345886 acc= 0.9414049983024597 auc= 0.9825503651833154\n",
      "Test on valid set: loss= 4.781430721282959 acc= 0.6695493459701538 auc= 0.9224444444444446\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1259e+02\n",
      "\n",
      "Test on train set: loss= 0.6573365926742554 acc= 0.9439138770103455 auc= 0.9810921831988546\n",
      "Test on valid set: loss= 4.9062604904174805 acc= 0.6423422694206238 auc= 0.912888888888889\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.0274e+02\n",
      "\n",
      "Test on train set: loss= 0.6341730952262878 acc= 0.9458562731742859 auc= 0.9806005851216735\n",
      "Test on valid set: loss= 5.0703935623168945 acc= 0.653847336769104 auc= 0.9175555555555555\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1381e+02\n",
      "\n",
      "Test on train set: loss= 0.611965000629425 acc= 0.9486079812049866 auc= 0.9814491535070241\n",
      "Test on valid set: loss= 5.2058916091918945 acc= 0.6616902351379395 auc= 0.9097777777777779\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1465e+02\n",
      "\n",
      "Test on train set: loss= 0.6059560179710388 acc= 0.9515215158462524 auc= 0.9849751638612826\n",
      "Test on valid set: loss= 5.125370979309082 acc= 0.658866286277771 auc= 0.9046666666666667\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1668e+02\n",
      "\n",
      "Test on train set: loss= 0.6349419355392456 acc= 0.9512787461280823 auc= 0.984096305679758\n",
      "Test on valid set: loss= 5.097874164581299 acc= 0.6602796316146851 auc= 0.9191111111111112\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2287e+02\n",
      "\n",
      "Test on train set: loss= 0.6188442707061768 acc= 0.9513596892356873 auc= 0.9815445531750282\n",
      "Test on valid set: loss= 5.612515926361084 acc= 0.6400196552276611 auc= 0.9095555555555555\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2392e+02\n",
      "\n",
      "Test on train set: loss= 0.637338399887085 acc= 0.9507122039794922 auc= 0.9776275524153852\n",
      "Test on valid set: loss= 5.588903903961182 acc= 0.6400869488716125 auc= 0.9088888888888889\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2349e+02\n",
      "\n",
      "Test on train set: loss= 0.581384539604187 acc= 0.9529783129692078 auc= 0.9790336240733261\n",
      "Test on valid set: loss= 4.859745025634766 acc= 0.6826071739196777 auc= 0.9273333333333333\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2323e+02\n",
      "\n",
      "Test on train set: loss= 0.5933824181556702 acc= 0.9531401991844177 auc= 0.9819715115603882\n",
      "Test on valid set: loss= 5.108339309692383 acc= 0.6799997687339783 auc= 0.921777777777778\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2440e+02\n",
      "\n",
      "Test on train set: loss= 0.6215465664863586 acc= 0.9509549736976624 auc= 0.9820441747213973\n",
      "Test on valid set: loss= 4.69162130355835 acc= 0.6995834112167358 auc= 0.9213333333333333\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2502e+02\n",
      "\n",
      "Test on train set: loss= 0.5841399431228638 acc= 0.9532210826873779 auc= 0.9802719952229259\n",
      "Test on valid set: loss= 4.836453437805176 acc= 0.6800263524055481 auc= 0.9200000000000002\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2822e+02\n",
      "\n",
      "Test on train set: loss= 0.6318029761314392 acc= 0.9489316940307617 auc= 0.9799827748306822\n",
      "Test on valid set: loss= 4.992855072021484 acc= 0.6620935201644897 auc= 0.9182222222222223\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2474e+02\n",
      "\n",
      "Test on train set: loss= 0.621452808380127 acc= 0.9487698078155518 auc= 0.9803768038113414\n",
      "Test on valid set: loss= 4.887061595916748 acc= 0.6792835593223572 auc= 0.9095555555555555\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2241e+02\n",
      "\n",
      "Test on train set: loss= 0.5759194493293762 acc= 0.9546778798103333 auc= 0.9808809310168535\n",
      "Test on valid set: loss= 4.740487098693848 acc= 0.6846649050712585 auc= 0.9266666666666665\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2179e+02\n",
      "\n",
      "Test on train set: loss= 0.5904815793037415 acc= 0.9529783129692078 auc= 0.9805486258959801\n",
      "Test on valid set: loss= 4.677372455596924 acc= 0.6885326504707336 auc= 0.9268888888888889\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2085e+02\n",
      "\n",
      "Test on train set: loss= 0.5790698528289795 acc= 0.9533829689025879 auc= 0.9821086032942155\n",
      "Test on valid set: loss= 4.958591461181641 acc= 0.6800421476364136 auc= 0.9244444444444445\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.1906e+02\n",
      "\n",
      "Test on train set: loss= 0.5736549496650696 acc= 0.9525736570358276 auc= 0.980849792489104\n",
      "Test on valid set: loss= 4.991325855255127 acc= 0.6799148321151733 auc= 0.9246666666666666\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.2201e+02\n",
      "\n",
      "Test on train set: loss= 0.5624211430549622 acc= 0.953787624835968 auc= 0.9820519910984873\n",
      "Test on valid set: loss= 4.9289751052856445 acc= 0.6801050901412964 auc= 0.9182222222222223\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.2405e+02\n",
      "\n",
      "Test on train set: loss= 0.5627074241638184 acc= 0.953706681728363 auc= 0.9830411903609303\n",
      "Test on valid set: loss= 4.910849571228027 acc= 0.6804132461547852 auc= 0.9204444444444444\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2493e+02\n",
      "\n",
      "Test on train set: loss= 0.5477820634841919 acc= 0.9550825357437134 auc= 0.9817515265280221\n",
      "Test on valid set: loss= 4.922076225280762 acc= 0.6782482266426086 auc= 0.9135555555555556\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2745e+02\n",
      "\n",
      "Test on train set: loss= 0.5567597150802612 acc= 0.9548397660255432 auc= 0.9813086865976018\n",
      "Test on valid set: loss= 4.894956111907959 acc= 0.6810191869735718 auc= 0.9142222222222223\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.3037e+02\n",
      "\n",
      "Test on train set: loss= 0.5676443576812744 acc= 0.953706681728363 auc= 0.9820971843523811\n",
      "Test on valid set: loss= 4.798172473907471 acc= 0.6901045441627502 auc= 0.9159999999999998\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2999e+02\n",
      "\n",
      "Test on train set: loss= 0.5663800239562988 acc= 0.9544351100921631 auc= 0.9823457582722954\n",
      "Test on valid set: loss= 4.802305221557617 acc= 0.6991896629333496 auc= 0.9135555555555556\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.3018e+02\n",
      "\n",
      "Test on train set: loss= 0.5637466311454773 acc= 0.9528973698616028 auc= 0.9796182920873058\n",
      "Test on valid set: loss= 4.7811198234558105 acc= 0.6704155206680298 auc= 0.9166666666666666\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2733e+02\n",
      "\n",
      "Test on train set: loss= 0.6102043986320496 acc= 0.949660062789917 auc= 0.9794842370876976\n",
      "Test on valid set: loss= 5.247628688812256 acc= 0.6436261534690857 auc= 0.9082222222222223\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2560e+02\n",
      "\n",
      "Test on train set: loss= 0.5632997751235962 acc= 0.9542732238769531 auc= 0.9787912374882828\n",
      "Test on valid set: loss= 4.821435451507568 acc= 0.6637760996818542 auc= 0.9139999999999999\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.3333e+02\n",
      "\n",
      "Test on train set: loss= 0.6164454221725464 acc= 0.9505503177642822 auc= 0.9773874900748709\n",
      "Test on valid set: loss= 4.505399227142334 acc= 0.7161843180656433 auc= 0.9215555555555557\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2757e+02\n",
      "\n",
      "Test on train set: loss= 0.6372058987617493 acc= 0.9524117708206177 auc= 0.9801898967975762\n",
      "Test on valid set: loss= 4.631888389587402 acc= 0.7000522613525391 auc= 0.9331111111111111\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2275e+02\n",
      "\n",
      "Test on train set: loss= 0.5830749273300171 acc= 0.953868567943573 auc= 0.979320463918255\n",
      "Test on valid set: loss= 5.003938674926758 acc= 0.6789143681526184 auc= 0.9246666666666667\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2047e+02\n",
      "\n",
      "Test on train set: loss= 0.5770502090454102 acc= 0.9550825357437134 auc= 0.9805064895127108\n",
      "Test on valid set: loss= 4.734271049499512 acc= 0.6799986362457275 auc= 0.9222222222222222\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2093e+02\n",
      "\n",
      "Test on train set: loss= 0.5868452787399292 acc= 0.9545969367027283 auc= 0.9790466018063189\n",
      "Test on valid set: loss= 5.1285247802734375 acc= 0.679998517036438 auc= 0.9062222222222223\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2317e+02\n",
      "\n",
      "Test on train set: loss= 0.5850175023078918 acc= 0.9531401991844177 auc= 0.978118762466756\n",
      "Test on valid set: loss= 4.874815464019775 acc= 0.6814638376235962 auc= 0.9284444444444444\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2553e+02\n",
      "\n",
      "Test on train set: loss= 0.5348682999610901 acc= 0.9556490778923035 auc= 0.9824238496406588\n",
      "Test on valid set: loss= 4.7796125411987305 acc= 0.6801431775093079 auc= 0.9111111111111111\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.2498e+02\n",
      "\n",
      "Test on train set: loss= 0.5781358480453491 acc= 0.9534639120101929 auc= 0.984393800042275\n",
      "Test on valid set: loss= 4.769371509552002 acc= 0.6610559225082397 auc= 0.9004444444444445\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.3279e+02\n",
      "\n",
      "Test on train set: loss= 0.5980423092842102 acc= 0.9507122039794922 auc= 0.98230022602124\n",
      "Test on valid set: loss= 4.845897197723389 acc= 0.6619753241539001 auc= 0.9037777777777777\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "\n",
    "metric_idx = 3\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_CoSen(beginEncodingEpoch, endEncodingEpoch,  \n",
    "                                               codingEnhancementRate, mu,\n",
    "                                               train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'CoSen')\n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "\n",
    "cosenWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print and Plot the Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "beginShowingEpoch = 0\n",
    "\n",
    "# plt.plot(acc_train[0][beginShowingEpoch:])\n",
    "# plt.plot(acc_train[1][beginShowingEpoch:])\n",
    "# plt.plot(acc_train[2][beginShowingEpoch:])\n",
    "# plt.plot(acc_train[3][beginShowingEpoch:])\n",
    "\n",
    "# plt.plot(loss_train[0][beginShowingEpoch:])\n",
    "# plt.plot(loss_train[1][beginShowingEpoch:])\n",
    "# plt.plot(loss_train[2][beginShowingEpoch:])\n",
    "# plt.plot(loss_train[3][beginShowingEpoch:])\n",
    "\n",
    "plt.plot(acc_valid[0][beginShowingEpoch:])\n",
    "plt.plot(acc_valid[1][beginShowingEpoch:])\n",
    "plt.plot(acc_valid[2][beginShowingEpoch:])\n",
    "plt.plot(acc_valid[3][beginShowingEpoch:])\n",
    "\n",
    "# plt.plot(loss_valid[0][beginShowingEpoch:])\n",
    "# plt.plot(loss_valid[1][beginShowingEpoch:])\n",
    "# plt.plot(loss_valid[2][beginShowingEpoch:])\n",
    "# plt.plot(loss_valid[3][beginShowingEpoch:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9Kf7zhM1xpuX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy of baseline: 0.6432999968528748\n",
      "minority accuracy of baseline: 0.46200000047683715\n",
      "\n",
      "test accuracy of enhencement encoding: 0.6908000111579895\n",
      "minority accuracy of enhencement encoding: 0.541399997472763\n",
      "\n",
      "test accuracy of reweighting: 0.6826000213623047\n",
      "minority accuracy of reweighting: 0.5135999977588653\n",
      "\n",
      "test accuracy of cost-sensitiveness: 0.6539999842643738\n",
      "minority accuracy of cost-sensitiveness: 0.49040000438690184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(labels, preds):\n",
    "    pred_classes = tf.one_hot(tf.argmax(preds, axis=1), 10)\n",
    "    cm=tf.matmul(labels, pred_classes, transpose_a=True)\n",
    "    cm=cm/tf.reduce_sum(cm, axis=1, keepdims=True)\n",
    "    return cm\n",
    "\n",
    "def soft_confusion_matrix(labels, preds):\n",
    "    scm=tf.matmul(labels, preds, transpose_a=True)\n",
    "    scm=scm/tf.reduce_sum(scm, axis=1, keepdims=True)\n",
    "    return scm\n",
    "\n",
    "softConfusionMatrix_test = np.zeros((4, n_class, n_class))\n",
    "confusionMatrix_test = np.zeros((4, n_class, n_class))\n",
    "loss_test=np.zeros((4,))\n",
    "acc_test=np.zeros((4,))\n",
    "auc_test=np.zeros((4,))\n",
    "minority_acc_test=np.zeros((4,))\n",
    "experiment_keywords=['baseline', 'enhencement encoding', 'reweighting', 'cost-sensitiveness']\n",
    "weight_list = [baselineWeights, enhancementWeights, reweightWeights, cosenWeights]\n",
    "\n",
    "n_minority_class = 5\n",
    "minority_class_beging_idx = n_class - n_minority_class\n",
    "\n",
    "\n",
    "for metric_idx in range(4):\n",
    "    model.set_weights(weight_list[metric_idx])\n",
    "    y_h = tf.constant(model.predict(test_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "    y = test_y\n",
    "    softConfusionMatrix_test[metric_idx] = soft_confusion_matrix(y, y_h).numpy()\n",
    "    confusionMatrix_test[metric_idx] = confusion_matrix(y, y_h).numpy()\n",
    "    loss_test[metric_idx] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "    acc_test[metric_idx] = tf.reduce_mean(keras.metrics.categorical_accuracy(y, y_h)).numpy()\n",
    "    auc_test[metric_idx] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "    minority_acc_test[metric_idx] = confusionMatrix_test[metric_idx,\n",
    "                                                         minority_class_beging_idx:,\n",
    "                                                         minority_class_beging_idx:].diagonal().mean()\n",
    "\n",
    "\n",
    "for k,a,ma in zip(experiment_keywords,acc_test, minority_acc_test):\n",
    "    print('test accuracy of '+k+':', a)\n",
    "    print('minority accuracy of '+k+':', ma)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Soft-Confusion Matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [20,5])\n",
    "\n",
    "for scm_idx in range(4):\n",
    "    scm = softConfusionMatrix_test[scm_idx] * 100\n",
    "    plt.subplot(1,4,scm_idx+1)\n",
    "    plt.imshow(scm, cmap = 'Blues', vmax = 100, vmin = 0)\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            z = np.around(scm[i, j],2)\n",
    "            c = 'k' if z<50 else 'w'\n",
    "            text = plt.text(j, i, z,\n",
    "                           ha=\"center\", va=\"center\", size = 7.5,\n",
    "                           color = c\n",
    "                           )\n",
    "            \n",
    "    plt.ylabel('Index of True Class')\n",
    "    plt.xlabel('Index of Predicted Class')\n",
    "    plt.title(experiment_keywords[scm_idx])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAAEWCAYAAADcs2piAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAADW0UlEQVR4nOzddXwUx/vA8c+cxT0hECAEdynWQqG0UKdCnbq727fUqSv1/upOhSotpUJxd3cPSQhx15P5/XEXSCBysidJ5v195dvkdnfmSchtdp+deUZIKVEURVEURVEURVEURVECi87fASiKoiiKoiiKoiiKoijHUkkbRVEURVEURVEURVGUAKSSNoqiKIqiKIqiKIqiKAFIJW0URVEURVEURVEURVECkEraKIqiKIqiKIqiKIqiBCCVtFEURVEURVEURVEURQlAKmnTzAgh9gshTvVhf1II0c3x+YdCiCd91beiKI3z9flAcZ06hypK8yCEeEwI8amT+04WQkx1oe1kIUSpEELvfoSKoih2QogtQoiTG9n+txDiWt9FpHibwd8BKM2HlPI2f8egKIoCIIT4EkiXUj7h71icpc6hihK4pJQvatWWEGI/cJOUcraj7QNAuFbtK4riO0KI+cBUKaVTSV1fkFL2rflcCDEZ6CalvKrW9rP8EZfiPWqkjaIoiqIoihKwhBDqIaOiKIrSaqmkTfM0TAixVQhRIIT4QggRLISIEUL8KYTIcbz+pxCiQ80BQojrhBB7hRAlQoh9Qogra227QQixzXHcv0KITvV1KoT4UgjxvOPzk4UQ6UKIB4UQ2UKITCHE9bX2DRJCvC6EOCCEyHJMCwjx5g9FUZojIUSSEOIXx3t3nxDinlrbJgshfhRCfO14724RQgw9qolBQoiNQogiIcQ0IURwrePPEUKsF0IUCiGWCiEG1Nq2XwjxUCPHnu84tlgIsUcIcabj9SghxGeO93yGEOL5miH/jvPMEiHEm44+9wohRjpeT3OcK66t1UeD54nGzjFCiFuAK4H/OaYczGjgZ9tLCPGfECJfCLFDCHFprW1fCiHeF0LMdPxsVwghutba3rfWsVlCiMdqxfyWEOKg4+MtIURQreMedsR6UAhxw1HxuHIOjRNCzHD8/Fc5fs6L6/0lUpQWyHGOekQIsREoE0KMcpzHCoUQG4RjaoAQ4hQhxKZax80WQqys9fViIcQEx+dNnW+n1vr6GiFEqhAiTwjxpDh2OqqpvnOzEOIbIBmY4Tg//U8IkSLsUyUNjn3mCyGec5wvS4QQs4QQ8S70rShKI4QQHYUQvzre63lCiPeEEDohxBOO91a24/0b5dg/WAgx1bFvoePvbqIQ4gVgNPCe4/38XgP9nS3s92Ylwn5t9FCtbW5diwkh4oX9fq7QcS2ySAihq3XcqcJ+bfYYcJkjvg2O7fOFEDc5rlkKhRD9avWZIISoEEK08SQ+J459xPGzKBH2a7BxjteHCyFWC/v1TZYQ4g1P/q1bDSml+mhGH8B+YDPQEYgFlgDPA3HARUAoEAH8BEx3HBMGFAM9HV+3A/o6Pp8A7AZ6Y58u9wSwtFZ/EvuQO4Avgecdn58MWIBnASNwNlAOxDi2vwX84YgxApgBvOTvn5/6UB+B9IE9cb4GeAowAV2AvcAZju2TgUrH+0sPvAQsr3X8fmAlkOR4r20DbnNsGwxkA8c7jr3WsX+QE8cOB4qA0xwxtgd6ObZNBz5ynFfaONq41bHtOsd54XpHn88DB4D3gSDgdKAECHfs3+B5wolzzOHzUQM/2zAgzRGLwfHzyOXIue9LIN/xvRqAb4EfHNsigEzgQSDY8fXxjm3PAssd33sCsBR4zrHtTCAL6Ofo/zvcP4f+4PgIBfo4vpfF/v6dVR/qw1cfjnPUeuzXO+2BPMf7ROc4N+U53oPBQAUQ73gvHwIOOt63IY5tcTh3vp3q+LwPUAqMcuz7OmAGTq21b1Pn5lNrfZ3iOBcYHF/PB/YAPRwxzgdedqZv9aE+1EfjH4735AbgTcff4mDH++kG7Pc8XbBPV/wV+MZxzK3Yr0FCHccPASId2+Zjn+7YWJ+ZwGjH5zHAYMfnnlyLvQR8iP0awYg9eSRqHVf7fDT1qHgOxwx8DrxQa9udwD8axNfgsUBP7NctSY59U4Cujs+XAVc7Pg8HTvD370xz+FAjbZqn96SUaVLKfOAF4HIpZZ6U8hcpZbmUssTx+phax9iAfkKIECllppRyi+P1W7HfJG2TUlqAF7E/ua93tM1RzMCzUkqzlPIv7BcZPYUQArgZuF9Kme+I50VgohbfvKK0IMOABCnls1LKainlXuAT6r5XFksp/5JSWoFvgIFHtfGOlPKg43wwAxjkeP1m4CMp5QoppVVK+RVQBZzgxLE3Ap9LKf+TUtqklBlSyu1CiETgLOA+KWWZlDIb+0VR7Xj3SSm/cMQ7DfsN17NSyiop5SygGujm5Hmi3nOMkz/bc4D9jlgsUsq1wC/AxbX2+VVKudJx7vu21vd/DnBISjlFSlkppSyRUq5wbLvSEVO2lDIHeAa42rHtUuALKeVmKWUZ9gupxjR0DtVjT8I/7TinbwW+cvL7VpSW5B0pZRpwFfCX41xok1L+B6wGzpZSVjo+PwkYCmwEFgMnYj/f7ZJS5uHc+bbGxcAMKeViKWU19kSPPGqfps7NTflCSrlTSlkB/MiR848zfSuK0rDh2JMMDzuuVSqllIux//1+Q0q5V0pZCjwKTHSMgDNjT+52c1wzrZFSFrvQpxnoI4SIlFIWOK45wLNrMTP2B+2dHNcJi6SU7pwLvgMur/X1FY7XPI2vsWOt2JM3fYQQRinlfinlnlrfVzchRLyUslRKudyN76nVUUmb5imt1uepQJIQIlQI8ZFjyF8xsBCIFkLoHTcPlwG3AZnCPh2gl+P4TsDbjmFthdifPAvsT7Wakue42alRjj1jmoA9U72mVrv/OF5XFOWITtjfv4W13iuPAYm19jlU6/NyIFjUre9w9PaaYpedgAeParsj9guZpo7tiP0pcH3xGrGfR2ra/Aj7qJMaWbU+rwCQUh79mrPniYbOMc7oBBx/1Pd/JdC21j6ufv9g//ml1vo6lSM/0ySOPT83prFzqOGotmp/riitRc3vfSfgkqPez6Ow39AALMA+eu0kx+fzsT+4GuP4uqaNps63Neq8l6WU5dhH9tTW1Lm5KQ2df5zpW1GUhnUEUo/6+wr1//02YD8HfAP8C/wg7NObXxVCGOtrXNhXmit1fHzoePki7CPvUoUQC4QQIxyve3It9hr2kUGzhH26+SSnfwJ1zQVChBDHOx7KDwJ+0yC+Bo+VUu4G7sP+8CpbCPGDEKKmzRuxjzLcLuzT0M5x8/tqVVRht+apY63Pk7EPA34Q+xPo46WUh4QQg4B12BMwSCn/Bf4V9noRz2N/ujQa+4XBC1LKbzWMLxf7jVlfKWWGhu0qSkuThn1kSncvtf2ClPIFN4/t2sDrVUB8PRdDrvL0PNHU06Y0YIGU8jQ32k6j7lOp2g5iv1CpGa1Ycw4G+/Doo8/P7sjBPnWqA7DT8VrHhndXlBar5n2ehn0aw80N7LcAmIJ9OubLQAH265wq7NMza9pw9nybSa1RfY5rpzg34naHp30rSmuXBiQLIQxHXavU/P2ukYz9b22WY79ngGeEECnAX8AO4DOOej9L+0pzLx712irgfEei5y7so+c64sG1mGME8oPYEyN9gXlCiFVSyjlH79pEOzYhxI/Yr2uygD8dbeNJfE0dK6X8DvhOCBGJ/QHfK9inRe0CLnfU57kQ+FkIEecYZKA0QI20aZ7uFEJ0EELEYn9KNA373O0KoNDx+tM1Owt7Ia3zhBBh2C9gSrEPWwP7XMlHHSeDmiKjl3gSnJTShv1i6c1aRa7aCyHO8KRdRWmBVgLFjmJtIUIIvRCinxBimAZtfwLc5niyIoQQYUKI8UKICCeO/Qy4XggxTtgL97UXQvSSUmYCs4ApQohIx7auQogxTbR3DA3OE1nY56U35E+ghxDiaiGE0fExTAjR24m2/wTaCiHuE/YifhFCiOMd274HnhD2Qn7x2Kcu1BQv/RG4TgjRRwgRSq3zsCsc0y1+BSY7RlH2Aq5xpy1FaSGmAucKIc5wnCeDhb2Yd82CC0uxJzqGAyulfQp4J+y1FhY69nHlfPuzo7+RQggT9ps54UK8TZ2fGuNp34rS2q3Envx82XHtEyyEOBH73+/7hRCdhRDh2BMv06SUFmEvaN5f2KcnF2OfwlNzr9To+1kIYRJCXCmEiJJSmh3H1xzr9rWYsBf5rZlOXtOmtZ5ds4AURxKkId9hn3VxJUemRnkUX2PHCiF6CiHGCvtCDZXY71Gtju/rKiFEguM6sNDRVn3fl1KLSto0T99hv3Ha6/h4HntBzxDsT6+XY59mUEOHPVN7EPv0pzHAHQBSyt+wZz5/EPZpVZux16zw1CPYh/Qtd7Q7G+drUShKq+C4OT8X+1DVfdjfv58CURq0vRr7fOP3sD913o29ULAzx67EXsD3TewFiRdw5OnUNdiLY251tPszR6YouMqT88Rn2OdKFwohptfzPZRgL3w8Efu57xD2c13Q0fs2cOxp2P9tDgG7gFMcm5/HXj9jI7AJWOt4DSnl39jPxXMd39dcJ7+X+tyF/ffgEPZh299jT7orSqvjqGtzPvYHVTnYn/A+jOM61vGEdi2wxVEHBuzFLlOlvfaWS+dbR9LnbuzFwDOxF1DPxvn34EvYk7uFotYqMk5+r572rSitWq33ejfso+/SsScsPsf+93Qh9nNAJfb3GtinTv+MPTmyDft1T80DmbeBi4V9ld13Guj2amC/41rmNux1uDy6FgO6Y78uKsV+Pvs/KeX8evb7yfHfPCHE2nq246jLV4Z92tPftV735FqxsWODsI96zMV+HdMG+/kb7Is2bBFClGL/2U501CZTGlFTgVpRFEVRlAAlhHgFaCulvLbJnRVF0ZTjqXwh0F1Kua+19K0oiqIEBjXSRlEURVECjBCilxBigGPI8XDshft+a+o4RVG0IYQ41zE9MQz7stubsC9n26L7VhRFUQKPStooiqIoSuCJwF7Xpgx7rZwpwO9+jUhRWpfzsU+tPIh9msJEN5fbbW59K4qiKAFGTY9SFEVRFEVRFEVRFEUJQGqkjaIoiqIoiqIoiqIoSgBSSRtFURRFURRFURRFUZQAZHBl5/j4eJmSkuKlUBRF8YU1a9bkSikT/B2HJ9S5SFGaP3UuUhQlUDT385E6FylKy9DQucilpE1KSgqrV6/WLipFUXxOCJHq7xg8pc5FitL8qXORoiiBormfj9S5SFFahobORWp6lKIoiqIoiqIoiqIoSgBSSRtFURRFURRFURRFUZQApJI2iqIoiqIoiqIoiqIoAUglbRRFURRFURRFURRFUQKQStooiqIoiqIoiqIoiqIEIJdWj1IUrWwuLObMdXuxOL7uGmxkwbCeGAzqV1JRFMUd8/OL+SYjj1KrjQmJ0VyUGINJp57NKEpLJq02bOUWdKEGhL7+9/v64nI+Tc/hUJWZcXGRXJUUR4RB7+NIFUXRUnV5Kft/e4OK3Uuxhneg3Tn30a5nP3+HpXiJukNWfO6LA9k8uudgndf2VJrpsGgzmScPRAjhp8gURWlprly/mzkFpYe/HhQewnt9OhGq15EUbPJjZNp6ae9BPknLpdxmA2BVUSnTMvP5eVA3DDp1TlWUlkZKScmCdErmpSGtEqETRIxpT8TY5DrXUT8fyufhHWlU2SQ2YHVxGV9k5PLf0B5EGdVtgKI0R1U7FyC/uZCuOgsCsBTqqPrmD7aNeofeZ070d3iKF6izdUtRVQpZWyC+F4RG+TuaRh2dsKltzIqtLDyhrw+jURSlpTp/zU5WFJfXeW19aQWjVm4//HXXYCNzhvQg2GT0dXhOs5aZKZ6dSsWWPIRBR9jx7YgY1R6ht9+YHays5sO0HKps8vAx5TbJptIK/skt4pw20X6KXFEUbylblknJnANIsz1RK4GS+ekIk56I0R0AqLLZmLQznYpa54ZKmySrysyn6Tk82LldnTbN5mJy8+YibdXExY0hKCjRZ9+PorQoGeug6CBs/hki28EZL4AWD6Wrq+HFBEwS0B1p0qS3YZBVhM59FPPYCzCagjzvSwkoKmnTnFjM8MfdsOknkFYIT4SR98Lsp8FWXf8xIQkw/g3od55vY21AZmVVo9t3Vph9FImiKC2ZVcpjEjb12VNpJmXJFi5LiOHtfp18EJlrbNVWst9dh7W4Ghw3XiWzUzEfKCbu6j4ALCksxSAEVcg6x5ZZbfyrkjaK0iIVz0s7nLCpIc02SuanH07abC2trPfYKin5K7eoTtImN3cumzbfjRB6pLQBVrp2+R/Jydd77XtQlBZnxUfw9yNw1N9jlr8PwTFw+xKIat90O9VlsHMWRCZB8vFHXn8xAag//6MTkBySz8Gdm2nfb4j734MSkFTSJtCV5sAHo6Ess55th2DWo40fX5EDP18NPx+9QcAVP0GP07SK1Ck5lU0nZf7IKuS8xGjvB6MoSotisdm4c2sq8/KLKbbKpg+oZVpOAdPmFXBfpzY8lNIuYKYUla/NwlZmPpywAfuNWcWOAsxZZRgTw4gy6OtdVcAAxJnUn3lFaYlspfU/rLOVmZFSIoQg2qDHKus/F8bWmhplNhezafM92Gx1kzx79r5ObOxIwsN7ahe4orQ0Vgt8OxH2/tf4fpUF8NYAeHgXhMY2vN+M+2HN50e+1gfBzXNg1tNOhdPu57HQr8ipfZXmQ1UoDEQHlsPXE+C1HvB6t/oTNh6T8N3FsHeBF9puWL/I0Cb3eWD7AR9EoihKS/L5gSw6LNjI7zlFLidsansrNZszVm1v8EbH16r2Fx/zNB1A6KA63V6r5+TYCAz1PHYz6ARXtIvzeoyKovieIaH+6yl9XPDhmjadQ4PoFhqM/qjTQ6hOx80dEg5/nZs7ByGOvSWw2cwcOvS7dkErSkuz5ht4Lq7phE0NaYHZk+2f75oHs5+D9LVQc83xckrdhA2AtQo+HAV75zTZvBCOm/vV3zj5DSjNhUraBJqvL4DPz4C986Asy/v9/XiN9/uoRefESialtmNvUBRFURqSXl7JY3u0S25vKa/i1Fp1b/zJEBcChvpG/QgMMfY56yadjh8HdaWNyUC4XkeEXkeoTseUnh3pERbs24AVRfGJ6HO6IIx1r6mEUUf0OV3qvPZV/850DQkiVGc/NwTpBHckt+H0+CP1D6U0O6ZEHc2Gzdb4tHZFaZWkhPICmHGX68eu/w4mR8G3E2Dx6/DpKfBMtP21ygJt4lv0qjbtKAFDjZsOJO8Mhvw9vu2zshAWvQGj7temQJZGUiuq6BSiimgpitK0U1bt0LzNbeVVPL0zjWd6dNS8bVeED29L6aJ0ZO358TrQR5owpRy56eofEcq6kX1ZU1RGhU0yLCqM0AaW/1UUpfkL7hFD/PV9Kfo3FUtOOYb4ECJP70Rwt5g6+yUFm1gwvBebSivIrbYwKDK0ztQogLi4McCxSRudLpiENmd689tQlOZn73yYcR8U7HPveJsP6ne26e39PhSfUkkbf7NZYc7zsPV3KPBxwqbGnGfsHyPvhdOf9U8MRzlUZVZJG0VRmmSxWCixeWcq00cZedzfuR3RflwWVx8VRPyN/Sn4cQeWoiqQYOoUSdzEXoij6u7ohWB4dLifIlUUxdeCukTT5vboJvcTQjAgouHp6UFBiXTt8iB79r6BzWYGbOh0wbRNPIfoqKHaBawozV3WVvhuIlgq/B1J4y75wt8RKBpTSRt/2jANfrvF31EcsfRtWP0FPJbm1zD0QC81pF9RlCaMXLaFvU4UN/fEhweymdQ1yat9NCWoUySJDw3FVmJGGAS60MBdnlxRlOYpOflGYmNHcejQdKy2KtoknEl09LDD9XEURQH+eyrwEzYnPwamMH9HoWhMJW38RcrAStjUqC62z6l8Ig8M/vn1sAKGACkCqihKYDpz1Q6vJ2wAPk/P9XvSBuxPyvWRJn+HoShKCxYe3pNu3R7xdxiKEpgsVbBnrr+jaNjg6+DctwKq3IWiHTXh3V/mv+LvCBr3fJy9wJYXxBy9jEE9hq3Y5pW+FcWvpISD6+3zoatK/B1Ns7a+1DdPuopVYXRFURRFUfbMA2n1dxT1u3M1nPe2Sti0YCpp4y+py/wdQdNeTYHKMs2bfadX04U98y02Cs0WzftWFL/J3wfvDoEvz4ZpV8Nr3WHVp/6OSnFCtUrcKIqiKErrVJYHS96Gadf5O5KGJXT3dwSKl6mkjb/om8kw95e1nxZwy5YDTu138yY3q7IrSqCREr65wL7SQHUZVBXb50TPegIOrPB3dEoTdKgnV4qiKIrS6mRuhHcGwtwXwBagtWwece6+SmneVNLGHxZOgT2z/B2F8+a9rGlzzp7yFhVpP8pHUfzi4FooywZ51IgNcyWs/Ng/MTVzZ8VH+qQfI2DQqaSNoiiKorQ602+3T2e3Vvkvhvg+DWwQMCkTQqJ8Go7iH6oQsa9JCXMDY1ltp634AE6Z5JeuKyxWQgx6v/StKJqpKABR3++xhLIcn4fTEnzRvwvXbdrLP7nFXu3nmqRYr7avKIqiKEoAqiiAnB3e7SO+L+RuaXyfq36C6A7ejUMJeGqkja/t+NvfEbguyH8Z3Ie2qyF/SgvQfmj9T2mMIdDzbN/H00J82b8Lh04ZRKLBe3/KHuri/5WjFEVRFEXxMZ0BbF5cpdIUCXcugU6jG96n3WCVsFEAlbTxPUuAzodszDlvatpcggs3WL/kFGnat6L4RUg0jH0KjKFHXjOEQHQnGHy138JqKTaMHuCVdjsHm4gxqgGpiqIoitKqlOfbaxF6U3UxpK2C6/+Exw6BMazu9pMegVvneTcGpdlQV6O+1mcCcIO/o3DNP5Og+yrNmvuuXwqnrd/r9P6fpWVzY8c2mvWvKH4x8i5IGgQrPoKyXOhzvj1hYwpr8lClabMHd+PUtbs1bfPd3smatqcoiqIoSjPw03VwcL33+8naCsnDwRQCjx/0fn9Ks6VG2viaTg+Dr/F3FK7J2wmHNmnWXP8Y1wqILigo0axvRfGrlFFw2Tdww99wwm0qYaOhflHhHDplEE+mtKGTyfPnEbd1iGdodLgGkSmKoiiK0myUZMGB5d6dGlWj68ne70NpEVTSxh/OexeunA6GZnRDkL7ab12f7GKSR0s7d73KnLk9mTO3DwfSvvNbHIqiOOfOzkmsOLEfh04ZxJ0dEzC60ca8Id2Z3F3NIVeUQCTNZkrmzqPghx+o3LrV3+EoitLSVBaB9MGqkWFtIDbF+/0oLYKaHuUv3U+BJzLgqwtg39y62wzBgA4s5Z738+Ae+GAElGd71k6ktsU4I3WCYpt0at+L2/l+9RazuYyFi2rXybCwa9eT7Nr1JOPG7vF5PIqiuO7Jbu15slv7w19XWm3ctHkfs/PrH713VlwEb/dJIVKtWKcoAak6LY3UK6/CVlaGtFoAQdiIEXR4522E0Z0UraIoylFiu4CtUuNGBVDrvieqI9ylXekJpeVTSRt/u/Y32D0H/psMVcUw6AoY/SDoDfDrLbDxR0CCLgiEDqxOFjI2RcAtCyAiHv63CwrT4K1+7sf53aXQtj/cPA/0nl8YPdG1Hf/b5dzczXPX7OD93p3oF+m7qSR1EzZHbxvOSaNX+iwWRQl4exbAv5Mgoh1c9i2s/Rr+eYTDFyjxveCOZaDz7+DOYL2OqQO7YpWSjEozaeWV7CivxKTTcUZCFAkmddOnKIEs4777seTmgs12+LWyZcvI/+474q691o+RKYrSYlTka9te13FwxY9gqYKszRDXFcLite1DafFU0iYQdBtn/zjahR/bPxpiqYbqUljxGaQuhrhuMPoBiG5/7L5ajJQ5tAleSIKncjxu6poObZxO2uwor+bUNbu4pl0Mr/bq5HHfnjKb8/wdgqIEBrMZXqh14ZG9FV5se+x+udvh2ZgjX5/yJIx5yPvxNUAvBMkhJpJDTJwY57/pl4qiOM+clU3Vrl11EjYAsrKSwmk/qqSNoijaeL27dm0JPVz9q/1zvQGSj9eubaVVUUmb5sxgAkMsnPIw8HDj++r0oDOBrdqzPm3VsOxDGHGbZ+244evMAp7vmoTJy0OgKys9nEqmKK3BV+fBvgXuHTvvOUg6DrrXk6xWFEWpj8Xc4Gg9afbw2kZRFAVg6aeuH2MItq8ObKmCrb/Vej0EbpmvVWQ+l5WVxbx588jMzESv19OrVy/GjBlDUFCQv0NrlVTSpjV5/BA8p0F9mH8n+SVpA3Dmmt3MPaG3V/vYvWeKV9tXlGZv3xL3EzY1frwGHs/QJh5FUVo8Q1IShoQEzGlpdV4XJhOR48f7KSrFVVJKhPBBkVdFcVVVOcx60IUDBEQkwt1rj6wGavkYDq6B8PYQm+yVML1t3759TJ06FavVWuf1pUuXsnTpUjp37sxVV12FXq/q//mSWj3KWTv+sQ+Xe6kDzHwYSmuNxjiwAj49DT45BfYt9l+MTdHr4ZrpGjTkXAFhb9haUeX1PkpK1jW5T3V1mdfjUJSA9f1lnrdhLvW8DUVRWg0hBO1ffw1daCjC8aRXhIZiTE4m7qab/Byd0hirtZytWx9hztwezJ3XjTlzu7Fo8SjWr7+VvLyFSOm/60pFOeyldq7trw+C25cdSdiAfRZE8ohmm7BZuHAhX3311TEJm9r27dvHc889R25urg8jU9RIG4D8ffDzTXBoPUhpr+g9+gEYfA0IAe8MgfzdR/Zf9TGs+Qzu32Z/Wpy2/Mi2r8ZDm75wx1KffxtO6XIK3LUa/noY9s5zv53yAgiNaXo/L3huVzpPemk53vLy/ZSXH2hyv6qqbEymzl6JQVECXnX9qy+5xBDqeRuKorQqIQMH0vW/WRT+9hvmjAxChw4l8rTTECaTv0NTjlJensryFWcjZX2r8EiqqzPJy88kL382wcEdOH74nxgMET6PU1EAeMmNJIvBaK/32WWM9vH4wapVq5g7d27TOzq8//773HXXXcTFxXkxKqWGGmmzZz68MwgOrgabBaQVCvfDjHvg5xtg64y6CZsaNit8N7FuwqZG9hZY9bmXA/dAfHe4+rem92vM+yO1icWdrtNzyavWfv56ZeUhli0/HTA3uW9IiHeSRooS8P55Upt2JnyoTTuKorQqhrg44m+6iXZPP03U+PEqYROAMjP/YNnysQ0kbI5VWZnOzl0vejkqRWnAzEegqsiNA4X93rEF2LFjBzNnznTpGCklH3zwAQcONP2wW/GcStp8P7HhbVt+tSduGpK5tuFtcya7Foet4WFoXrFvkWfHlzm38pO39F2ylexK5y4GnLVj5zOAc/8OBoNaGlhphapKYPk7nrcz/Bbod77n7SiKojRj5eXlbNu2jd27dzc6HaE5sVotbN12v8vHHTo0XftgFKUp5ipY5eZDJGmDTidqG48fWK1WfvrpJ7eOtVgszJgxQ+OIlPq07ulRUoKlovF93F1tqdKJ6QNSwtuD7CN7agy4HC70wRPoGfd5v48mGABP8tMDlm3n4oRIJrZPYER0OHoPC9sVFNQzakpRWhtLNfx0HeyZB0FhcNpzMOhy+zK77gwfPlpCHzj7Nc/bURRFaYZKS0tZunQpGzdupKysDL1ej16vR6fTcdFFFxEVFUVMTAwGQ/O8RN+6tYnVTBtka3oXRdHaC23cOEgHxiC48BMwBmsekq9lZmZisbh/R5aXl0dVVZVaVcrLmudfhOYgJKrx7eZqeCHh2Nc3fg9CDxe87524ahSlNb1PU3bOgh6nu3141xATOyo8mOYkJT9nF/FzegERJh1/DO9F7/AQt5uzWovdj0VRWoLyAng15cjXlnKYfhus+xaCo9DkorrTCZ63oSiK0szYbDb++OMP1q9fX+d1i8Vy+IZp6tSpGI1GdDodp59+OkOGDPFDpK6zWErZvfs1KqsOUlK82a02pGwZ00yUZuTlbu4dFxQOdyyHqPbaxuMnWiSI1UpS3te6kzbeXHJw8HXHvpa6FBa/BRmroTyv4WM3TPV+0kZvcH8UUY1p18CTh9w+fFJKW67f5sE8yJp/P5OOkgoLZ/+ylt3XjHRrxM327c+6sLfKJCst1LuD63891cPplLWNeUS7thRFUQKQzWZj3bp1LF68mOrqarp06UJWVhbZ2dlNHms22+vq/fPPP0RHR9O1a1dvh+uRjIxpbN/xmCZtzZnbn3FjN2nSlqI06vuroTLHvWOrSlpMwgYgMTHRo+OllKSlpdG5s1qgxZtad00bj5cYNICugdomI+62/zdrq71g8Rv94YuzYNe/jSdsfEVoUJPF2sTUsiac0Sba8xjAnrwJMWDOKufbrZkuH55x8EcyDn7l9P4pne5wuQ9FaRYq8r3bfo+zIKKtd/tQFEXxIyklX375JTNmzKCgoICysjI2bdrkVMKmNrPZzOLFi70UpTYsljLNEjZ25Rq2pSgNWP4p7PjD/eNjOmkXSwAQQiA8GMggpeTbb7+lrKxMw6iUo7XupE3Gusa365qYp9imF9gaWGnot1tg2fvwwQjY+TcUB1hlbbP/31g6nba/frbEED74d4fLx+3cOdml/VNSbnW5D0Vp9ZJHwRU/+DsKRVEUr9q3b59mq6kUFwf2tO30jKn+DkFRXLPqa/jnQfePN4TAuMmahRMoevbs6dHxFouFDRs2aBSNUp/WnbTZ3kS1a30Ts8eyG5m3u2c2/Pu46zEBCB/MWtNpNPcww7M3aKzBiV9BKet+NMAWYaS40PUVpWy2Kpf21+vVylFKC9XQyEEtHFgMKz/xXvuKoigBYOvWrZq1lZKSollb3lBdrfXozPEat6cotexaADPvdu0YQzBEdwK9CeK6wUWfQL8LvBOfH1188cUet7F3714NIlEa0rqTNjFdGt9uLvWwAzenX0kL7PjXw76bYArVpp1/H/Xo8H8Gd298Bynt059qPmpeO5oQICVd20R4FE9TTKYOXm1fUfzqQi8nVf56CCqcWFlPURSlmQoJcX9BhKONHj1as7a8oX3S5Zq2N27sO5q2pyiHbZ4O357n2jFCwGVT4b6N8GQO3L0Gep/rlfD8TYtixJWVrj84V5zXupM2jdZk8WKRYmd8fym8NxTKcr3TfrVn9WgOO9jEFLMmJIc1cnFTk7CpzdbA6jVWG6LcyuRz+7ocQ3TU8U7ve+LIeS63ryjNhvDBn4T/c/79piiK0twMGzZMs7aio6M1a8sbwsJSiI7WZkXAcWP3aNKOotRRVQL/Pgc/X+v6sbFdodup2sfUQnla0FhpXOtO2qz9rpGNnhYp1kDuLnitK5S6Wd28MVaNsqEWH2dV9Xr7P03tqVI2+387FFsZ3CnGpebM5hIqq5wrDpiYeKnmdXgUJSDk74XPzoCfrvF+XyUZ3u+jlcrLX8TyFeNZuGgo69ZdT0VFmr9DUpRWJzIyktNOO83fYfjMkMHf0q3rJAyGSIQwERHRz6Xjx43doxI2ivayt8Pab+C1vrDsddePD46Gm+d6d6XhABMeHu7R8dLjBX6UxrTuJb+l1d8ROOfL8XDXSn9H4Xv1nCiDKyuJ3JtDdo/2oBOO5A3otxfx3WVDXWq+qiqPxUuGO71/v74vudS+ojQL1eXwwaiAKE6uuC819VN27zlyjsovWMjSZacwfNjvRES4PgJRURT3nXjiifTr148ZM2aQnp7e4qcNdOp0M5063Xz46927Xyf1wAd+jEhptcwV8MOVkLoULO7MKhAQ1QHuWA5BniUxmhuTyeTR8Rs2bOC881ycgqY4rfUOG7BUwZAb/BuDPgiEEwWBc3d6Pxa3NTBdyQWN5rBrZW2DqqvokJ3JK3EWTs2TmDYXYNxUQMdNRfx2zkC6J7hWz2bd+uud3nfUqBUuta0ozcaCV3ybsAmJ811frYTNZmH3nlfr2SLZtPleAKzWanbveZ2ly05l5arzycr6Sz0VUxQvioqK4qqrruJ///sfoaHu1RGcPHmytkH5SNeuD9K7V33npLqiolx72KYoTZrzLKQucS9hE5oIJz8Kd61qdQkbgPx8zwqLW61WqqpcW9xFcV7rHGmzaArMeQ6/ToE6+3UYdIW9hsRLncHWyMkl0IfmfXk+XPe724eH6aC0kdxP99Q9gGDMuhUM7dGFs669lbMAm01isUlMzqxAdRSzuZSysi1O7x9kine5D0VpFtZ969v+LlfLfmtpztyujW6vqNjHgoWDsVjKAMvh1zdvuZuE7DMZ0P99L0eoKK2bTqfjvPPO44cfWs+5TwhBUtJFhIR0ZO26hosV9+j+rA+jUlqsigJY/TnsXWAfYWMzu96GzgR3r4AQ18osKHW98sorPPXUU/4Oo0VqfUmbXf/Zs7D+9EgqhESDuQqWf9R4wgagzwRfROW+/fM9Oryh2sIACMH8i8/AWliI8cpzEcYjSxLrdAKTzr2E1rp1Vzm9b5/eaoiv0kJJCeVeqJnVkHGTIdn5KYlK45pK2NSwWIrqfT0n519KSrYSEdFHy7AURamluLi4VSVsaouJGU6/vu+xectdx2zr3PleIiN7+iEqpUUpzYGPRkFFoQd1NoV9Ge9WnrDR6/VYrZ6VDrE1elOneKL1JW2m3+nf/q/7y56w+WI8pC527phLvvBqSP5W3sR2fXg4eg+LYx2tpHST0/u2a3e6pn0rSsDY4KMbiS5j4cofQW9sel/FKc4mbBonyc7+WyVtFMWL5s+f7+8Q/Cox8SwSE/dgNpeTkfEVen0YSUmXo1d/DxQtLHrNnrjxpE5pVHs4Vy03f+655zJ9+nSP25k8eTI333wz7du39zwo5bDWV9OmLMu//X95NrzWw/mETZdx3o1HK1+c4+8InLZ7tytV5J2oOaQozdUaXySEBVz9q0rYBCiDwbVaYIqiuCYjw/0V85prTZv6GI2hpKTcTseO16iEjeKZ6jL4+1F4tSus+MjzhWUu+xaMwdrE1owNGjSIM844Q5O2PvnkE8rK1AIXWmp9SZtA4Eri6JpfvRODKVLb9lIXgdXS9H5+lpO7yKUVDYYP+8uL0SiKn1WXNr59woee9/FYRuDX5Wq1BO3aXeTvIBSlRevQoYNbx7WkhI2iuE1K2DANPhgJU3rBb7fC24Ngxf9Bea7n7Q+5HpIGed5OCzFixAgmT57M+PHj6dKlC3q9+w+v3333XQ0jU1pf0sbgXgV/v2g72HttD7lO+zafb+fyIRsKisFmo3vZfvqX7EQv6yZ+tM57b9x4nUv7R0R00zgCRQkg3U+nwfXbTnkcpt/mWfuGUDCFedaGcozSsn0atCLo3eslTCa1mpeieNOZZ57p0v5Go5Hx48d7KRpFaWbmPg9/3gdZW6Ak0z6tuyzb83ZN4XDtTDj3Lc/baoGGDRvGNddcw5gxY9xuo7LS3RpDSn1aX9Lm1Gf8HYHzLv3Me2339sIFgayGQteGAb8853sWr76Gf9feyq8b7mHT0gsYm7f88HYtf0G3bX/cpf1DQ/tr2LuiBKAT7oDQGOokboQOBl8DY/7nefuWcihM97wd5TApbaxYcapbx0ZFDSWp3aV06nQHJ41eS1LSJRpHpyjK0YxGI5deeqnT+1ssFioq3FiuWFFamooCWPYemJuqfukioYOIdpB8grbttkCjR48mIkJNow4ErS9pc8It/o7AOee8CbFdvNd+x+O90+5bLhS0LEjn7R0v0aUinVBbJRHWCmItxXy69SmSKzIBMBm0qylz6JBry5L376eKkiktXHgbuHWxPUkT2R4S+9mnRGlZkO/gGu3aUti56wW3jhPCRFzcGHr3foluXR/EaNR4iqyiKA3q06cPTzzxBGPGjKFfv34EBzc8jthgMNC5c2cfRqcoASp7O+hN2rcrbfZROzv/1r5tN1XuLiDzlZWkP7aIjMlLKZ57wN8hASCE4MEHH3TrWJPJC/92rVjrS9oAjPPzkt+NST4RJqXB0Bu82483a0z8dLNz+73dlxBbFTpknZf10srlmX8CMCxKu6kVNluVS/uHhydr1reiBKyo9nDeO/DAVrh9CQy8TNvzQ7tB2rWlkJ+/xK3jpKwmN3e2xtEoiuIsg8HAKaecwsUXX8z9999PUFDQMfsYjUa6d+/udh0cRWlRIpPAWu2dtqtL4eB677Ttosqd+eR+uhlrQRXYQFZaKZ6VSt6POwCQUlKxPY/SpQcx5/tnFJ5O53rKQI3Q0VbrW/IbYPS9EBIFf97r70iOEDrofylc8GHzL9q55UfodAIMv7HhfSbHAhBqPXa+Y5C0kFSVA8BTXV2vk6MoSoAQeojp5Jeu9+7dy8qVKzEYDIwePZrExES/xKG1iIh+lJfvcutYk1HVr1GUQBAUFMSjjz7KwYMHWbp0KYWFhZhMJgYNGkS/fv0Qzf06UFG0ENPJvvKkxYPaKMffAeu+PnbhBWOY365Pjpb/6+56X69Ym0362qPq9/wBQd2jib++H0Lnu/NEcHAw5eWuTVMbOHCgl6JpnVpn0gZg6HUw+zmo1KDyuBZumAUdh/m2T30IWL2Usf3rAdj4M9x01NBDiwWeP3LjoMd2zKGluhDyjJHck9yG7mEhHodSUZHD0mWuzlttvW8NRXHKA9vhjV6N79N1nG9icdi+fTt//vknpaV1L842b97M8OHDOfvss30ajzf07vUCWVm/uXycThdCx47XaR+QoihuS0pK4uKLL/Z3GIoSWKxWeK4N4OGqtJGd4bTJsOVXe10cWXPPIcAQBH0v9DBQbdiKXJsJULWrkOI5B4g6zXdJJ3dG2px00kleiKT1ap3To2q0c6H+ireVHPJ9nxOcX/raLelLYXKU/eOZOPt/n2/8SW+lMGLW6Tlu/NM81jVJkzCWLhvh8jFBQWposqIwucj+Ud/rO5yYC36Ka8W/3WWz2Xj77bf54YcfjknY1Fi5ciV5eXk+iceb9PogBg38Bmf/fOv1Eeh0QXTtch+xsSd6NzhFURRFcZXVDPuXQOoyWPAmPBeLxwmbM16BB9bbkzM3/gsdhoPOaP9IGgQ3/AtB4RoErwGD67fjpUtdW/jFU65O2Rw2zMcDEVqB1j2c4Nz34J0B/o7C7s8HoM+5vu2z3wT4xUd9SedOvsHSTHBkMiOSOmrSbVnZfjiqZo4zevVyr9inK7ZnFvPy39vsRb5O70m/9lFe71NR3FJf4qY0q/FjdEZoP8gr4Rxt/vz5FBQUNLnf119/zRVXXNHsp0rFxY1k3NhdVFRkUF2dz+o1F0I9oxb1+jAG9H+fyMgBGAxqbrmiKIoSYPbMg5+uhcpSwOp5e3G94O4VdV+LSbEnbiqL7KNtQmI870cDZVtyKPhhB5hdv0+RFRr8rFwwYcIEXn75Zaf337ZtG+PHe2Gl4lasdY+0ie0ExlB/R2FXnu04YflQoM6ZPk+7lWtKSre4dVx8nPeWASwoqaL7YzM58+1FzN+Zy7wdOZzz7mKu/nRF0wcrSqBoP5Q6S4XXIeC+zT4LZfny5U7tV1RUxAcffMDMmTO9HJFvhIS0JyqqP52S618VsVfP54mNPVElbBRFUZTAU5oD319uT6ZokbC5ftaxCZvagqMCJmFT9F8qBd9sdythU6PqQLGGETUuODiYO+64w+n96yu0rnimdSdtAC74xN8RHLHwVd/3eeWvvu+zMadMhpRRmjUXHNRds7Y8VVpp4cnpGznuhdmYj30ozqLduczZ5odpcorijq6nQGh90x11cM9aiGzrs1CsVtcu9latWsXOnTu9FI3vdev2MD16PIPREIMQeoKC2jGg/0e0bXuev0NTFLdIi4XC338n+623KN+40d/hKIriDRu+A4sGtTUvnWYfEdzpeM/b8gFbtZWSOZ4v6Z37pXsPpt3Vpk0bzjnnHKf2Pe2007wcTevTuqdHAfQ5B3qdD9t/93ckEO6HIfvdx8GZL8M/k3zf99Hu3Qwx2kyLqrFx0zUuH5MQr/00tbu/W8uMjZlN7vfavzsY19t3N7uK4ja9AW74B767DIoz7Cvg6Uxw0ccQ28WnoXTr1o0dO3a4dMx3333H0KFDnb4ACXQdO1xFxw5X+TsMRfFY5Y4d7L/kUmS1fanfvA8/wtS9O12m/4bQ6/0cnaIomtm/1PM2dEZY/hb0OdPztnykcme+Ju3IcgvVBRWYYjxftMXpPqVzI4N69WpioQrFZWqkDcDEryHlZH9HASOcH3amqaE3+aff2iYXaZ6wATCbXS882rfvFE1jmLYy1amEDUBlfUNwFCVQxXeHu9fALQvgupnwvz3Q4wyfh3Huue4lWlevXs3zzz+vcTSKongi9cqrDidsalTv2kXm5Mn+CUhRFO/QYslqmxky10PWVs/b8rKiWftJf2Ix+VO3a9Zm8Yx9mrXlDH/VBPwzu4Bz1+zi7q37MVvsdVKXFBTTecEG2s5bT9t56xm6ZDNlZg8LWAcwlbSp0dkHq2oYgiGsff3bht3qvxozBqN/+gU44d76i5xqRKdzfU6lXuMneU/97nxtj2tG+G75PkXRhBDQphe0H2wffeMH4eHhPPDAAwg3zqEWi4XJkydTUaHBEG1FUTxSuWMHtgZWgCv+Y4aPo1EUxas6jtSmHZ0BCvZr05aXFP6zj5K5aWBxv4ZNfSp3Nr0Ig5Y6duzY5PLfAwZot8iP2Wajy/wN3LQllVXFZfyUVUjHRZt5dfdBLlq/lwrbkZ9nerWF7os3825qFl1rJXN6LNjAC7szKHNxKn2gUdOjagy9EeZ5uGLQkOvh3Lfqvma1gKWy7rJyebth6sVQeMBeEOu6v6FND8/6bk7CEuGm/yDG+wmK+PhTyc7274VelQvnCLNV25O5orQWkZGRTg/brc+UKVN44oknNIxIURRXWfIbnjYgLS33CaqitErHXQGzn3R+f50BpAR51IW1tRoS+2gbm0ZKlh+kYkse1bsKvdOBxbcj9IUQXH/99Xz22WcN7nPhhRdq1t+5a3ZSXs+13Rtp2fXubwNe2Ft3dkOxTfJuWg7vpuXwdf8UTo+P1iw+X1JJmxphcTDqAVj8hvttHH0SAfuTZ3143dfiusG9693vxxtOfQZmP61xo0c99e46Fi7+HEKiNe6nflarlcJC11Zkio/XpnDn3pwSHv5pI2sOFLp03Ov/7qBTXCin9WmLXotho4rSgkkpWbZsGUuWLKGsrMyjtiwWC0uXLmXkSI2e/CmK4rLQoUMb3KaPj8ecn485NZXqffsJ6taV4P7964yws5aWYquowJiQ4ItwFUXxRFg89DwbdvwNNPLQJSgSrFXQ61zYPcex2pQjWWEIgV7j7ct6B5BDn2zAssc3qztJq0TofXfP0LGj9uUsGrK+tFLT9q7ZtJ/vB3ThlLhITdv1BZW0qe3Up6HP+bDgFagohN7nw7+POH/8yY96LTSvG3WfffrWf0/ZT4xaGHYDDL8VynKg7QAI9u0bZN36a6iurj8T25C+fV7zqM/le7KZ+Mkqt4832yS3TV0LQKeYIA4VVVHl+LsUbBBMPq8flw3r6NY0EEVpaebPn8/SpUsxm82atDdr1iwiIyPp16+fJu0piuIaWdXw9Yc1O5vdI0+0T8mUEnQ69FFRJH/2KTaLhdTLJtpfd4i69BKSnn3WF2EriuKuCz+2L2hwcC1IHVgcD2Ciu8F9ayB/n33qU0IviGwHBan2e5U9c8EUBsNughPv8+d3cIz0pxdDle9GztvKq9FH+HaJ7SuuuILvvvvumNcfeOABn8bhjvu2pbJhVH9/h+EylbQ5WtIguPz7I18ffwu8PQiKUhs/rvPJEJnkvbh84YTb7B+WavjyXEhf7ll7qz6HjT9B/4mQ7Pun10VFrsdvMLj2ltiYXsi0lamkFZSzJb2QvArthimmFtS9eK20SCb9uonV+/N4/dLjNOtHUZojs9msacKmxs8//6ySNoriJwefbGSqRE1Cpua/NhvWggL2XXhRvbsX/fgTpp69iL/yCo2jVBRFM0ERcP1f9kLC+XuhTW+I63pke2xn+0eNmE5w6Ve+j9NJme+u9WnCxs73D3J79OjB008/zc8//0xaWhp9+vThzDObxwpeWebmWdtGFSJuik4H92+E895rYLsJzvsArg2AJcO1YjDBxZ9o0JCEqmJY/TE8GwOV5Rq06ZysrHle7+P5mVs4770lfLsynYW78jVN2DTm57UHKSivbnpHRWnBPJ0O1Zg9e/Z4rW1FURpm3rNX0/ZyXnxR0/YURfGSxD7Q+5y6CZtmxlJhxprhvWuThugjTD7vE+z1bS655BIeeOCBZpOwqfHzIW2WXfclNdLGWYOvtn+U54NOD8FRPg/BZrVhzivHVmnBmltJdUEFttxKgvvGEd6vjbadRSfDVb/D1PO1a3NKN3j8oHbt1aO8PJtly0e4dWxU5Gin991xqIRPF+13qx8t/LImjZtGN98/bIriqbCwMM1H2dQoLvbNPHRFaS2kzUbF+vXYqqoIPf74w6uP7Lv6GipXuT+luEnNfLUQRVGaj+K/9/un34VpRJ7kuzozLcF92w5wYZvoJlfC0oKUUpOyFipp46rQWK93YamyUL41h+JZqVBihiYWTKhcl0shOwge2ob4i3tqF0i3k+HxbHhBo4SQ2UvZ59JsKn68jM3ReyiOcvxKu/HmGDr0S6f3/XtzZtM7eVFRhVpFQ2ndjEYjRqPRK4mb/v2b31xnRQlUpUuWkn777chqxwhRnY42j/yP0mXLvZuwURRF8SFLiUY1QV1U/Nd+QgbEY4wO8Uv/3jQ0PJjVGhcjBvut9dKiMkbFRGjedo38gmXs3PkcZWU7MRiiSO54AykptyOEe4kiNT0qgNiqbaQ/vYRDTy+jeNpuKGg6YVNb5epsStce0jYoYxA8uEvbNrW0dyFVb/VgWXKqPWEjhFsJG1dV+3k+5OBOMX7tX1ECwUknneSVdl2tbaUoSv2shYWk3XzzkYQNgM1G9ksvUz5/vk9iqNy2zSf9KM6xSUmR2YK1nmV8FaW5sVgs5P+xi/THFlG9rcBvceR93TLPc5/179z0Tm7z3jmouHgjGzbcRFnZDkBisRSyP/UDdu1+ye021ZWpn1ktVjKfXwGV2iQBCn/aRWjfeBACnUnveYO/3gYbv296P3+QEr6fyI5uYUiBh8ka135Wlw3vyP8t0HbuvbPaRZo4qbtazlRRRo8ezZw5c/wdhqIoDcj/9luw+abeW0OK/pxJcO/efo1BsZuakcszew5SarVhEIJrk+J4rnt7tSKm0iylv7ICCgKjxqQlr8LfIXhFYnAQesAbj8qDdRrcJzdg7753sNnqjryy2SrIyPiWLp3vw2AIc7lNNdLGj/J+3knmE0s1S9gAIOHg08s4+NRS0h9bRPHKDPfbmveStgmb0zUuCFiUBuYyCqJNGoyuMbq0d6e4cMJNvn/7jOkex1/3jkGvUxc4zZ3VYiZj+1ay9u5G+vmmpjl79NFHNb3gj46O1qwtRWntqtPT/R0CBdOn+zsEBZieVcD/dqZTYrUhAbOUfJqRy6mrdvg7NEVxic1mI/3RRQGTsAHQR/qnGLEv/Dekm1faDdV77z6urGwn9Y3kEcJAVZV7JTbUSBs/yftlJxWrs7zbiQ2Kf91L8b+pdHjSjSW3F7+hUSA6OO9dGHyVRu3VsN+oGazSlVlk9bckQl0+ZtMzZ9Lryb+psnh3iO+EQUm8dGF/Qkzq7dpS7Fmzgr/fm4KUYLVasFmsSGlDCEHHPv24YNJkDMaW+wdYS0FBQTz99NMcOHCAqVOnUl3t2UXUbbfdplFkiqJEjBtH8W/T/RqDzMsj+6OPaXPrLX6No7VpO2+9U/ttKavk1d0Z/K9be+8GpCgeyPpwPeb9Jf4Oo0ERZ3Tydwhe0ycynPd6dOCundo9BOhgMtA7LFiz9o4WHtaTysqDHJ24kdJKUFA7t9pskSNtzJXaFyzSksVsoWKVlxM2tZVZSX9+ievHWTXIIN+1BiYXeCFhA0R3hOAoUg6U26dKeUBK15d+E0Kw/bmzmHbLCVwypD1Xn5DMn3eN4qUJfT2K5WjT1x/kq2Wpmrap+J7VYmHniiXM+ewD/njjJarKy6muKMdaXY20WUFKpM3Ggc0bef/Gy/0dbrPTsWNHIiIinB51ExZ27NDU6667juBg7/0RV5TWJmLcOAzt3LtA1VLem2+y/QT3VpZUvO+NtBwyKvxTxFVRmpL51uqATtgAhPaJ93cIXnVx+3juT9amNIQR+H5QN69Oy+zc+R50uqA6r+l0IXRof5VbU6OghYy0sZjN7FixhH/eff2YbVe8+Cbtunb3Q1THshRXcejddfYVoXyt1EbBrL3EnN7Fuf2/vwopNZh19P4wuOBjGHCJhw014KrfaP/ZOAqjKjmUeNTN1uHgQ9HrBVar9qtXCSE4vkscx3eJO/za23O2a97PRwv2cNsYtcR3c/XNY/eTvcf5gt6WqiqmXHYOAA9O+9NbYbUoQgiuueYafvzxR7KyshBCNLiy1B133EGbNm2wWq1kZGSg1+tp166dT5Z+VJTWRAhB13//4eDjT1A6axayyn835rKwkKLZs4k69VS/xdBaODvKprYhy7exfVQ/oo0t4tZEaSHK9xVgPRTY9WJiLuvRKq5fHunanjcP5HjUxtiYcL4Z2BW9l+toRUb2Z9DAL9i56zlKS7djNEaTnHwTnZJvdrtNIV0YoTB06FC5evVqtzvzhj1rVjD91eca3eeSJ18kud8AH0VUv+zPNlG9q9CvMQBEX9yD8KGJje6T+/75xGXP124RJqGHJ7JA71rdGKdVlVL+3wPkpf9GTlwQBZEGMNqLS5lMbRg5Yj6bt9xDbu7sRpsZN3aPJuGkTJqpSTu16QTsfWm8Jm0JIdZIKYdq0pifBOK5qCFTLj/P40KcKnHjmqKiIqqrq4mLi2Pfvn1899132Gw2EhMT1fSnAKLORa3PnvPPp3rHTr/1r4uOoufy5X7rv7VwJ2kD0CXYxNIRfbQNxknN/XykzkXay/t5OxWrPUsSeIVRR1DHCPQxQUSdnoI+KqjpY1qI+blFTNy0z61jh0YE8+fQXhpHpL2GzkXNOp1dnJPN76+/0OR+Pz33GDe/9zmRCW18ENUR1jIzpYvSqdxfFDDD6gp/3kl1cTmxY+tfQi1711ZtEzYA0grbZkC/CzVstJagcELP+ZiKvGso2/YwVOcAgoSEU+nT+3X0+iC6dnmgyaRNaekuwsMDY1TW0aJCvJTwUrxq+W8/abJyyoe3XcPQ8RMYeq6X3kMtTFRU1OHPu3btypNPPunHaBRFqVG9a7df+7fZ1DLTgWxvZTWVVhvBXiwQqijOsFVbAzJhY0gIoc3dx2mzQnAzdHJ8FIdOGcSMgzncvMP5xXZOiwnn64HNe8ZCs07abJo3y+lVV/754G0ufarpBI9WShanU/Sne5lAbyuflU5IzzhC2kces23T1JcZ60HbR0+pOvx14QEPWnVOXNwoRo9ahsVShk5nQqc7kugIDumEpKZ0cf1WrDyfcWO3ej1Odzx0Rg9/h6C4wFJZyUd33UBlSbEm7ZUV5LNg6ufs37iOix9vfGShoihKwNLrXUpkGzp2xJKW1uB2XWQktmLnz7OxV3uhvp6iqWKLhWC9KsSvuE/aJFW7C6lKLUYfaSJ0YAK6YNdueQ++EDgj8to8OARDVBDCoEOo1WMBODcpgTek4AEnihMvOb4XXUObf73CZp3K3rpontP7pm3ZgM3mjVXej1W+OSdgEzY18t7dQH1T47Lzyl1rqNNJhz9tqAZOuc0IJYdcDdFtBkNYnYRNhcXK/y29F0l9i6/V5vlc+7u+W+NxG0dLjAziiuEttyp8S/TeTZdrlrCpLXXjOvIy/L+ErqIovlFZZubQ3iLKilpGkdawk092ar+Yq66i9/ZtWHo1PpS9x5LF9N6+jbg773Cq3bK5zl83Kv7RJkglbBT3SbONnI83kjd1KyVzDlD0514yX1pJdbrzMx4OPDIdWeGH+qP1MPWOwZQQis6kVwmbo1zRPp600f04PyGKeEP9I49u6xDfIhI20MyTNqX5eS7tLxodZ6ENc24F+d9pX4jWG7I/3XTMa3E9jqPYbHJ+MabUhRAc0+D+QkC1MGHN1aZejDum7fydrubF6Gh8pA3A5s2TPOrr382eJ6fiw00IQK8TTBiUxH8PjPFqhXNFWwe2bMLaQAFcLaz/V/uaSYqiBBZpkyz6cSdfPrKEGe+u55vHl/HPR5uwmH3z8MlbrLm5Tu1nyXNuP2G0P6AxtnFu+nvVzp3Iag1WxlQa5G49mxoWi0WbQJRWqWTZQaozSpHV9hF90mxDVlnJ+257vQ+rj7b/7rcQ1ggQHtwidwhx/9ijtLm2n2ZttURGg4GP+nVm8+j+TO6aRIgjsRUsBJO7JjG5ewc/R6idZj09Sqc3YHPh5C58UFm7dEkGeF7CgrBxHQk/vh3lKw9hiAsmdFAbqjJKyX1vveeNO5j3FHHo4/W0vWXQ4deGX3YjPzy0kOtSVmDS2U9uTeYLKgsoJphIjl1qXUo4JGMoD+5CkmaRu6YwZwYd64mtRpXVSLk5lEhTMYeyfqJNm7G0aXO6W315uPI4AFabZO9LZwOoZE0zkZuWyq8vTaYkz/vznzfPm8W4G271ej+KovjPpgXpbF18EKvFhtVxmbN/cx6Lf9zFyVcGfiHFhkgnr9kiTjnF/sl//zm1f9RFF3Ho6clN7ygE1qIiDAnaLB2r1LWt1PNVdjot2kzGKYM8D0ZplcrXZIH52BsxW0k1lrxKjPENJ1Ss5eVQFgKhBveuv8MMxF3Xh5COUcdsKtuQRcG0nS7dIxp7xrgeQyt2W3Ibbkv2bf1aX2rWI22Gjj/f6X2NwdplPRtjznZxelEDosd1whgZRNSpnQg7LhEhBMEdImj37AhN2q9h2VtCVcGRmKMT23LRCx/xp+Uy/kjvTVG1k6NujNFYGvh1ettyIWLIddoE7Jb6T7xmm4Evt0zk7rkv88iip7hv/ossOTiMlesfRkr3Mm9d24R5EihgH2EjhFAJm2aiOC+Hrx660ycJGwBLdRXp27f4pC9FUfxj3X8HsFTX/TtkNdvYvvwQVqsGT4b8JOzEkU7tF3nmmS61qzMYwND0c0ih06GPUTdC3vJWapbHbViB5YWBsXiH0rI0dVVd9Pvv6CI7IFwZZaOz15zp8PJoOjw5ot6EDUDYwEQ6vDiaDi+PJqhHtFNNJ1zZ2/k4lBavWSdtTrzsagxBzs1TM1dWkJ950MsRgbXS82kR8XcPanDeot5kIPYGbZdDzHm1bh2W+OQULnrpI87/bDnRURFOrSQVNfRiZlhPoFra5xRKCVYp+Mc6lL9tw0lMafjJoJSStWvXMnPmTLZv347NZmPz5s0888wzTJ48uc7HSy+9xN9//8369euxOVnMMC5xApXUXQ5PSvh666UszjgeqzRilUbKLWF8seVKtuR0ZvO+pU61fbR7xnleMHjCce09bkPxnb/fe8Pnff774ds+71NRFN+pKqt/RIq0SqzVzTdpE32eEw/bDAaEyV7XpPf2bU63HTJuXJP7mLp3RziR3FHcs6nE85E2AD8czNekHaX1CRuaiDAee3urjzShj2v8nlEfE4uscK30RrvnRmJKCHXpmNjLmhgtqYM2dw9stStEKfVr9n+57vnqJ96YeK5T+35x3y3oTSbu+mIaBoOXllCu50ThDH37UKLP70ZIcv0Z2tpCe8SRH6yDSo0u3CSU7ysktHP0sdvuWAZvOJGIOOkhqiILuO3PqUzQLcGKjl9sY1hs68v9p/ZE10AS6vPPP+fAgSMrS61atQqj0Yi5gZogVVVVrFixAoAZM2Zwyy23kJiY2Ghol3Yfz0d5s+lW9R96rFjRMXf/KJYfHIbk6BOijk82X8uawkx+vLPpb/toZ/dvx9Dkvaw+UOT6wQ73n6pWimpOsvbu8nmfxdmeP81UFCVwtesWxYEtx964hscFYwxuvhfyQV0621eQsjZSm8fNpErFwoWN72A0En3RRW61rTinR1gQ+yqqmlj0oWlDozwftay0TuEjkqjcUUD1gWKkxYYw6ECvI/aqPk2OYI88/TSy37yV4IgkhOHIw1776PtjR8Abu0Si17t+PtaHGUl8eCg5n27CVmAvMm/oEE7c9X0xmPQIo+/O8WZzEWZzPjZbNWZzIeHhvTEaj11dWPG/Zp+0cXUKibW6mrevvABTSCi3ffwNRlNQ0we5IOqcLuS/t9GpfUWEgfhr+xHUIcLlfpKeHMHBx5e4fFxD8j/aROjLo4/dEJkIjxyA/xsBJRn1HzzsFgiN4fKRMbSNvZVJv44kp6SKmFAjU87uzUVDOtZ72OTJk+t9vaGEzdGsViuffvopDz74IMHBDWfPg/R67h75Nssy17E/ex7hpkj2rMvB1sBAM4s0sjLNyPR1GS6PehFC8PMdo3jy9418s6zhZUobU2G2EhbU7N+arUZweATmyoZrJnmFmjqnKC3aiRd1J3P3aixmK9Jmf8vrjTpOvqJns586G3vNNeR/8UXDO1RXYy0pQR9hvzbqvX0b23odO02g9igci8UCFY2P8jDExhJ13nnuBa045b5ObVmQX0KF7UjaJliAXugoc2Gp9yuT4rwRntIKCIOO+Bv7Ub2/2L7kd4SJkP7xTo1aETodCfdfTO470zH1PBthCAZpw5K/D0NCz7r7Rhppc9MAt+M0xoWQ9Mhwt4/3hNVaxYEDX7Fv/1tIefTKhHpSUu6ia5d7/BKb0jDhTCXtGkOHDpWrV69ucj8pJct+28X62elIGwSFGRgxoStdB7chOEz7ES4f3X6tyytJgX0O9P3fTtc8nvRJixrfIUhP0pPHo2tgeTJn5f+3j/I52i3/2+6FkU1njKWEBa/App8hJAbGT4F2rp208vLyePfddz2I9Fg9evTgiiuucHr/rxbv4uk/d1D/DFcJCNpFBbHs0VPdjqnKbOX7VQfIKKggyKgju7iKGRsOUlFPgbTaFjx8Mp3ivPeUSQixRko51Gsd+ICz5yJf2LVyKX9MedGnfXYdegITHn7Cp30qitbUuahxRTkVrP03laz9xcS0DWXwGZ1I6Oj6Q55AtO/yK6hct67B7dGXXEy7555zuj1LTg67Rp/U4PaIs86k7WOPqQLEPrAov4RHd6Wzp7yKUL2O69vHc2W7OK7fvI/tZUcecJgEBAlBSa0EjwBmD+5OXz+MtGnu56NAui5q7qxVVWS98jbVBwqJOPsk4i48E2mTVGzOwXyojJD+CZjahfs7TLdUVGSwYuXZWK2ljeylo23b80lsM564uJMQovmO7myOGjoXaZ60kVLy5aNLKC+sZ0lFAUPPSmH4uZ01fVJktVh468oJbh174ePP0XnAcZrFAlCdWUr22/VfjIggPW0fPx69RvMUm0wQuSCofywJV/bVrL2jWa1W3n33XQoLC73S/vDhwzn77LOd3j9l0kxqEjT1Mehg94vjtQnOYeehEs54a2GDQ4ejgnWse+rMBqeTaaG5X5hAYF2cFGYd4rN7bvJZf0Kn44Y3PyK6bTuf9ako3qDORa2XtaKCnccNbnC7LiyMnmtc+7nWNxqnhiu1cRRtmG0Sg6g7Ij6jsppCs4V2QUZiTfaHuGarlfn5JXQINtE7wrXaIPUprbTw5uydbEwrpEfbCB4+oyfRoaYmj2vu5yN1LlKaYjaXsGjRECSNTE+tRacLJsjUlqFDf8JkivVydEqNhs5FmhYirq608NlDi+pP2ABIWP33frYsbGCajZv0BgPHneVcXZuj7Vm1TNNYAEztwmn3+PEYu9SaE2gU6KODaHPXIM0SNgDhpydr1lbVJu8Vftu6dSvPPfec1xI2ACtXrnRp/8QII43VkrfYYPw7i9idpd0qBj3aRvD+FQ0nCd+5YqhXEzaK9vRG305lkzYbUYltfdqnoiiKlvQhIcTedluD22VjNW8aYOpb/0OnsDENj8BRvMeoO7YGSPtgE30jQg8nbACMej2nJURrkrDZnV3CoGdn8dnifaxKLeDbFQcY/Nx/rEkt8LhtRWnu1q2/xumEDYDNVklF5X62bvufF6NSnKVp0mbe1O0NrnhwmIQF3+9k5V97teyasdfd6tZxiV26aRpHDX2EicRbBpL09Ajiru9Lwk0DaPu/YRhdrDDelOixnTRtzxveeecdfvzxR5/09dZbbzm975wHT2lyny0Hizn9rYUcyNNmKXeAswckMf3OEzmuYxR6YR/Rc3rvNsx+YAxjeqih281NRGw8QWG+Hcpts7l+Q6MoSuCymm0c3FXIoX1FSJunZVybhzb33oMxuZ6ad3o9EWPHutxe119+JvSEE+q8Fn7qqSR/9JG7IQYca2k15euzqdiShzTbsJRUk/3hBtIfW0z644vJ+XIztsrW+/fhhi9XYTnq/WOTcNNXq/wUUeuzbfjxbOvVu85Hzuef+zusVk9KSUmJczVXj5aXNw8pW+95JVBo+oh47/ocp/dd9cd+SvOqGHu1dmvQ9xp9MtsXzXfpmA69+rJ33Wp2rljMlvlzQdrQGYycdef99Brp+dMZXYiBkJ7eHVIWPDCOyg2u1/Txtm3btjFt2jSf9llYWMhnn33GjTfe2OS+4cFGBDS5yoFNwmO/bWTqTSc0safzBnWM5rc7R2nWnuJflz8/hS/vb/ipsZb0RiN6vSpUrSgtxb6Nucz+fIv9b5EEY7Cec+4cSEJyy6hf0xAhBMmff8H+yy7DVl6OrKhAhIaij4wk8dFJbrXZ6ctGChw3cyWL0in6dz9CJ0AIpJRgdXw4VG0vIPPVlSQ9eUKzL1jtjgP59RejLig3U1ltIdik/na6o3zjJg4+/DCWrCxM3brR7rlnwWZDGI0E97CveJr3409kP/VUvcfnvvoaua++Vue1NpOfJm7iRK/Hrtht2/aoR8cXFKwgNnakRtEo7tD07GWzuvZ0aNuSTE6+sic6nTYDfsbf9RC5+/eRm5bq9DGf33fsCB2bxczMt19l5ruvc/ZdD9L7xDGaxOct8Zf3IX2DdrVttLBr1y6fJ2xqpKWlkZ+fT2xs08myT64ezE3frG1yvw3p7i/hrbR8cUkdGHnZVSydNtXrfY28+Eqv96Eoim8U51Yw65PNWGoVqDdXWfn9rXVc98qJGHy49Ks/mDq0p9t/syj++2+qdu0muHcvIs48E12Qtit7NnfV6SUUz0oFi0Q28ahJllsoX3WIsOHtsFVbkdVWdGHGVpnEUTyX9c475P/fB4e/rtq8mf0XXHj4axEcjKFNG8wHDrjUbvbkZwgZMIDQPn00i1Vp2KGs6R4db7ao+yB/0yxpYzXbmh6yUI+8jDJNV0O49vX3+eDWqykv1GD+qs3GX++8RmhEJJ00LlasNV1MELaCo5dt849p06axbZt/i/69++67PPHEE02uhnVqX+eKucaEar/qmdKyjLhwIqt+/wVzZePLznpCbzCS1EixTUVRmpftyzOx1TMdymaTpG7Ko+vgNn6Iyrd0oaFEX3SRv8MIONImKZi+m/I1h3ChDAUA5VvzqNieT+W2fJD2RTCiz+tK2JBE7wQbILrEh7E3t+yY1+PCTGqUjRtsVmudhE19ZGWlywmbGgduuJFey7WvLarUJaVESrNHbYSGdNYoGsVdmtW0qShzL2EQGtl0RXdX3f7RN5q2N/vzDzVtzxtib/Deqk/O+vTTT5k8ebLfEzZgP0G98MILpKWladLeg6f31KSdxthsjS8FrgS+MVfd4NX2hU4QEh7Z9I6K3xRVmNmeWcSinTl8v/IA87dnU1plwWK1seNQCbuzSzFb1XtdsasoNtc7SlnaJJVlnl1kK81b1kcbKF/pesIG7NOkKrfmH36YKqusFPy0k9J1WdoGGWC+un44Jn3dEUV6AV/dMNxPETVveR997NX2pRcXJ1GOKC7e4HEbVnXd4neapZ3/fNf1XwhDkI6wKO8Mge037gw2z/lXk7YKMzOYMvFc2nTqzHkPPE5UYuA9qQhOCAM9bv1x95TFYuH555/3fcdNsNlsfPbZZ9x///1ERUU1uN8lg9vx09rMBrdHBes5f1B7b4QIwGv/bOejhXux2CRC2J8U7c8twypBJ2DCoPZMuXSgGtrcDAw87SxW//kbhYcOeqX9mHbtietQT+FOxa+sNklReTV3f7+OJXucry8WrBdcN6oz/zujl1o1rpVK7hvL9hWZWKrqXhBLCe17xPgpKsXfLAWVWFK1W7myRuGPOwk/LvCuYbXSMS6Uzc+cyf/N3826A4X0ahvOvaf2IFSNsnFZdXoGRb//7tU+RKi2i7Mo9Us94HnyrbBwAdHRaiqbP2l2FsvPdH11nZ4neG/Z2jNuuVuzpA0AUpK9fy+f3nMjfceezmk33RFwxUCjzu9K0a97fN7v66+/7pV2w8LCuP322zlw4AAmkwkpJd999529+J4LfvzxR26++eYGt7926WB+Wjuzwe1VLtZqcsV7c3fx/vwj/2ZSwp6cI0N7bRJ+XZdBudnCh1cN9VocinZufPtjti6cy/ypn1NRVKhZu9Ftk7hg0tOatedLW5dmsOavVIxBOk66shdJXaL9HZImbDbJ/83fzf/N30N5tesZ80qr5MMFe/l44V7mPjiGlPhwL0SpBLJO/eNpkxxJdmoxlmp74sYQpKPn8e2ITlQ3NK1V5e5C7zQswZxbjjG+5f5umQw67ju1h7/DaNaKZ80i4777wcsjwNu/965X21fs8vLme9xGZWWG54EoHtEs66DT67C6+ObesuAgIWFGjj+vq1Zh1HHP1N9456oLNG93y9xZbJk7i8Fnnccp192iefvuihie5FHSxtS94dEoDbHZbFRWVrrdZ2NOP/10wsPD6VOrSNnTTz/NBx98QFaW80N8MzI8O9FUmb33R+u9ebud2u+fzVnsyymhc0LLXk2kpehz0lj6nDQWm83GPx+8xbaFcz1q76pX3iIxpZtG0fmOlJKP711w+GYU4LdX7YW/O/aJ4exbB2AIar6FVt+bt5v/m7+bSg/PETYJJ7++gP+78jjO7p+kUXRKc6DTCc67dxA7Vhxi54pD6I06+o5qT+dB8f4OTfEjoxcTdlV7ilp00kbxTPmmzWTcc6/X+wk55xwiRqrViLxNSonN5nnN0zZtxmsQjeIJzWra9B7p3qiZ1X+lUlbknQK6RqORu7/8ySttA6z9+w82z/8PKSVWi8XxX7PLI0G0pO8U5vax0ee4njyrqvJe8eP+/fvX+/rNN99MRIS2yYuY0Ibzl97813TlZu+UKQuZ+NESL0ajaE2n03H2nQ/w4LQ/GX/P/wgKDUNvNKE3GmnXvSeG4JA6+weF1R1pERYTx4PT/myWCRuAGe9uqJOwqS1tawEf3buAj+6dx5p/9ze7+dJWm+TjhXs9TtjUdse369hxSPspEUpg0xt09DkxiQkPDObcuwfR5bgENR22lTPEhTS9k5uMSe5fJyotX/oDD3i9jy6rVpLy+mtN76h4TAiBEJ6N0dDrI4mNHaFRRIq7NBtpM+qSHuzbkEdZoes38ev/O8CJF3fXKpQ6TCEhPDjtT7586A7y0tyrbt6Yfz94m38/eLvB7Ve+8jZtU7wzkqg+7W4fTPok15f/1kWaMCW6/oc8yEvLcg4bNqzBpeANBgMPPvggkydP1qSvjMIKyhq4sayxYHsmY3o5t9KUK0KMOipcuOlbvq+Q1/7ZxsNnqhWEmpteJ55EzxGjyM/MwBQSQkRsy3+SnrY1v8l9LFWS5b/tZc+abC7631D0Bs2eJXhVaZWFKov2RcRu/XoV8/83VvN2FUVpPsq9VTBYgKmDGrGrHFGxZw/pt92ORaOFO5yRetbZdP37L/QaP4BV6qfTRWO15rp9/EmjV2sYjeIuza6O9QYd1744kjNu6UtEvGs38iUF3pleU9t1r/8flz37qtf7Odq3j9zLO9ddRtYe56bBaCH6FtdXkpJu/ibodDo6derk3sENGDRoEOPHNz0MLzg42Kn2Gkr+1MgursSkb3yfu773vPJ6fc7s7/oItQ/m7/VCJIovCJ2OuPYdW3TCpqK0mq8fX8L7t7k2JSw3rZTdq5vPyiYRQQYigo2at7s/v8Kp1aVy8/O5442pnPXk59zx6idkpfm+npmiKN5hLaz2TsMCrMVealtpdkpXrGD/+HN8mrABsObmknrd9RRt3Mi2gYPY1qcve6+/ASklRevXs61X7zofu846i7yvvvJpjC3Flq0PeZCw0TFk8M/odM13GntLoukjTaETdBucyJWTXRtCtWdNDlYvPLE8Woeefbjujf/zej9HM1eUMfWx+5hy2TlMuewc9m9a59X+wrvEEnmua4kUndH9N+R1113n9rFHO+uss5gwYYJT+44ePdqp/ZKSGq8R0T0xAksT9ZhKqrzz+zljfcOrVjXEBnSeNJNZm7yzQpGiuGvpL7v4/KHFlOS5PuJSSti9JtsLUXmHTif43xk9CTZqPzJo2qqGL6DPfXs+KZP+ZOirS/krO5pt5kT+ym/HiPe3smDhHM1jURTF94I6R3qnYSEQejX1TgFpsZB23fV+679qyxYOXnoZVFWBzUbVsmVs792HgxMvP2Zfy779ZL/08uEkTkVqqh8ibp4OHZru1nHduj3OuLG7iI4+TtuAFLd5ZfkjvUFHu+6RZO4qdvqY759bwZWTR3h9Hndc+2Ruee8LPr7LfyeqX55/EoDr3vmUuETvrKAVeWIyxTOcP6nFujk9bd++fcyePdutY2uccMIJREVFMXz4cPR655NHI0aMYO7cuVitDSdUDAYDJ510UqPthAcZuOuUbrw+a6fTfWuhtNKMxeZexRwJ3PLtOk7qlsrXN6l5por/mSstrPvPs6d1QaHaj1zxponDk4kINvL6rB2k5ZdjtUlNamDN3ZbFW//tJLfsyBPxSwa3479t2RRW1Jzvav+tFNiA+//KZu1oCaoeiqI0a8G94uyPVT0omSWMOmTt6dcCTO3D0YebPI5Paf4yHnvM/rSkGdp/xpkAdJzxB+HdvVNeQwtVFRY2zktjx/JDmKushEaaiIgNolO/eHqf2K7JmQDacO/fOLHNmRrHoXjKa78tJ1/ey6X9i7Iq+b/b51FVafFSREdEJCTwwA8zMIX6t3r+l/fcxFtXX0RVRYXmbVfnurYEe1An11eO+vDDD/nqq688Wp3pkksu4cwzz2TEiBEuJWzAPu3pllsaXr3LYDBwwgkn0KNH00s/3jW2O2FG397oaJGgXLg7n5RJM8kq0v53SFFcMe+7HR630fek9hpE4lvjB7Rj3kMns/vFs9nz4tl8fcNwJg5rjyfv7rk7cuokbAB+WpvZQMKGw6/lE0F1vm+HuSuKoj2hF8RMdO06+mhBXaMQRh0YdYggPfrIIGJdvDZXWiZraSklf8zwdxgeSzv3PLYNP77Rh7e+Zqm2sn52Kh/fv4BP71/Iyj/2UZRdQXlRNblppezbkMf8b3fw8b0LKcwupzCnnPxDpV6ZcbJg4VC3jw0OVitZBhqvjLQBiE0Kb3qnenw5aRG3vnWKxtEcSwjB3V/8yKo/f2XhN597vb+GWKureO+6S+g1agzj737Y/XasVvJ/2EHVpjy3jj/4ykqSHhnu9P5//vknhw4dcquv2vr2db3+Tm2JiYncd999zJw5k9TUVIKDgxk4cCCJiYkkJycTGen8EOOfbj+Rs99Z3OD2PVnFdE3UbshyWJABgTarUx3/0lyePLsXN57ku6LXilJbVZnndRLadXU9eRxIdDrBST0SOKlHApPP689NX65k8Z6mizFrSZi8t+qMoii+EzYggaIZe7CVmF0+tsPL9unj1QdLqU4rQR8dRHD3GIROjcJr7YpnzyHjrrv8HYZ2iovZ2bcfBAXRfdVKDCb/jSTbuuQgC7/fidXS9BA5q9nGt08tr/NaXPswLnxoMKYQz0cdl1dkYrEUuHXs8cPne9y/oj2vjss6796BLh9jqZTY3Jwy4o5h51zIg9P+5IqX3iK6fQef9Xu07YsX8MaVE7BUO3/jYymvJuuD9aRPWkTm40vdTtgA2AqqKJjhfLHk1asDp5J4dHQ0V155JY899hgPPPAA48aNo1+/fi4lbAD6JDV+w3j/D9rWIioordR0OfHn/tru1+XmldZtxEWBO0TZH4KNeqbePILESF9dQEriRQnGiAQf9acoirfFXND4eTVmYs9jXqtJ2ACYksIJP74dIT1jVcJGoSozs2UlbGqrqmLXgIHsvvAiv3S/aUE6877Z7lTCpiF5GWX8+JI291ebN9/j1nEGQxvCwztqEoOiLa+NtAHo2DsOd4YSWC02dCbfVqpu16UbN77xIRaLhbevnODTvmtIi4W3r76Q4y++nFGXXNnovvm/7KR8lbYrrZQtySTm3G5N7vfyyy9r2m99tmQUMenXjew4VEKYycCks3py2XBtV6lyOabMUk3bO+3NhZq2B/DgtPW8MVEVDVN8LyLGudXcGjLm2jYaReI/KZNm+qlniUDywVWD/dS/oijeENInjtAT2lK+vJ6RzToIG9SGsEHN/9yp+EZqPUV+Wxrz1q3suvoaun/ztc/6TN+Rz8LvtamLWZRdQcGhMmLahnnUTknJWreOG3zclx71q3iP1ysghUe7/pTR6OOETW0GgwG90b/FMFf8/D0Lf/imwe3V6SWaJ2wOt13S+PLr06ZNo7JSmyXa77mn/izw+rQCxr+7mE0ZxVRbJQUVZh75dTOnT5mnSb/u0nq2aW6Z60Oem7Jsr/ujrRTFE/9+stmj4/uN6KdRJP7hv4SNXZhRz7C+xz51r0+l2cryvXlsSCv06chWRVFcFzuhO9GX1R1xo48NJvGZE/wUkdIc5X71FdYs79w7BBrLqlWY830zNXnbskx+f3O9pm0WZLlWl1RLERHOXUcovuf1pE37njEuH/PLa2u8EInzzr3/Mb/2D7Dqt2msnvFbvdsKpjs/jclVeVO3NbjNarWybVvD210RHx9PbGxsvduu/WxFva/vzCln9mbXl8gOVN548/Vv37xrgijNU1lhFWlbPbtAev+2ubx/21yNImptBCd0dW5a1MyNBxny/H/c9NVqrvhkOSe+Mpdtmc6v9Kgoiu+FH9eWDi+PPvzR7n/DMPr5AaPSfFiLish55VV/h9GokNNO07S9PePP8XrJgNz0UuY1ct/krvj27tWFVVo2rydtjjs12eVjDu0p4pdX/FczpeuQYVz/9se07d6ToLBwElK6YArx/UpTC6Z+xmf33nrM67Zy762wZc0sa3Dbc889p1k/BkP9M/MsVhtFlQ2PablpqnvD/QLRnSd30bzN+05TGXLF98qLPS9CXKM5Jm78PcoG4ONrhjS5z96cUh78aQNlVVZKqyyUVVvJLKrkik+WY7Z6sLawoiiKErDKliwJ6OW9Q88+m5R336GDhlOaZEEB2/t4tthJU9b8vR+p8TD88NhgQjSohWcwRLt8jNFY/8N0X1m3P5cxL88mZdLMRj+6TprJql2tY9RYbV6taQMQ7eacvEP7ijm0r4i2nf0zciC2bRJXPj/l8NcWSzXLf5nG9sULKMr2fNUkZxUeyuCNKybwwHfTD78WNqQNxf8d8FkMAK++qm2GvqHsd3ZJVZPHfrpwNzed1HTtHXc0VoIpSOMU54Nn9ubd+Xs1bXNTeiF9krRb4UpRnFFVrv1UP8V5L1/QB52u6RPUtFVpWKzHnuHMVsmiXTmM7ZXojfAURVEUf9LpAzZp0/GnHwnv3x+AiGHD6L19G9tPPBGZp8H0JinZfsIIei1f5nlb9di3MUfzNkvzK/n4ngV0G9qGzgPj6TqoDXqj6zcgw4b+zrLlY1w6ZuQI7/ycGiKl5LFf1/P9qoMuHWcFLvnsyOCO724czsjuLX8RBq+PtNEb3O9i5vsbMFdrv269OwwGE6Muu5qb3v2UWz9suN6MN0irhSmXnXP464iTk8HgpVUAQo7N423evJnycm3nV7Zr167e12PDms4uP//XDtJySzSNp0ZwI2lMb/y5++ueEzVt7/HpmzRtT1GcsWe99hcuzcn+l8f7pd9wk46NT45j4vGdndo/t7QKSz01bKSUFHihxpaiKIrif2GjRvk7hGN0+fcfem/fdjhhU1uvJUtImTEDQ/v2EBHhUT+ysJCimdqPhj2wNQ+r2XuJsN2rs5n79Va+eWoZpQVNP9A+WmhoB5KSrkJK5/J1QwZPb3AWhNamrz1AyqSZdH70L5cTNvW54rOVh0fhvPvfdg0iDExeT9oARCW4t6pIZamFj+9dgKXae9OB3BEeE8MDP8zweb81iRuhFyQ9PYKgbtGa9xGcXHdk0+7du/n5558172fAgAH19290rgj16NcXUm3WNqG3P7eMikZ+1aq9MHugT1I0MfUkytzlwUqDiuK2sChfLWut1PjlthFsfvYsIsOc//s6tlciofUU+rfYJMd38e+waEVRFMU7dEGB9ze65J9/Gt0e0r0b3efMpveqlfTevo2UP353u6+DDz5E6m23u318fbYu9jzZ0BSrGcoKqpjz5Va3ju/d6xmyNzU+Y6VHj1cZN3YP0dHHJs+8Yciz/3Dfj957wDxlzh5SJs1k1oYM/tmcyZrUAq/XNvIVnyRtJjzowTKkEj66Z2HA/cCFEDw47U/ik1N82u+Uy87BZrOhM+pJuKk/iY8M0/Rf0djxSPGr9957j6lTp2rXeC2hofXXCLK6sJJJjyf/oVrDhF5mUYVmbbli3dNn+OaNqCheEuvh0pS13fnhWM3a8iVfj7bZmVXq8jFn9E2kV9sIQmoNtQ416bl6RCc6xPi+bpuiKIrifdWpqRDs3gN0bwkZNsy1/Xv0IOntt9zur3z+fLLefNPt449W0EgNUK2l7yigKNu9GQ+5m+Pqfb3mtrpjh4vcDcslXy3ZS8qkmeSV+2YGzS3fr+e2qWu5+rMVjJuywG/3eFryyb3ipvkZHrfx1WNLNIhEe9e+9h4PTvuTB36YwYPT/kSn9/5y5W9efh5zv/4UAGNMMIkPDcWU7NnwwRrlK+z1eiZPnkxubq4mbdYnMbH+2gnL9+bhysSvHk/9S34Ty5Q7a/r6pn9PMwu986Zf+cQ4l75vRQkkVRWBMY3V3/a/PL7Ox76XzuaJ8b290td5g5JcPsag1/H9LSfw5Dl9OKFLLGN7teHdy4/j8bO9E6OitATmKgvLpu/mh+dWMPP/NlCc2/wv/pXWRR8djbAF1lDs/M+/wJyV7dIxUWecQcSFF7jf50cfu33s0aSPL9r/+nCjW8dZK4xU5BnrnSIVH3+hh1E5p8ukmTw9Q/tVtpxRXm1lf14Zt0/178rUWvBJ0qZCg5VFygqq+b875mLVeEqMVoSwv3u7DR/hk/7WzZzOJ3ffBIAxNoQ2dwwi/q6BHrcrTHpeeOEFj9tpsh9R/9muqMKM0cV6PYNfmENqnuc1bv7Z3HSB6QMF2tb2qREXFuSVmjmK4gtJ3aPQKuv4/m1zSUtL06YxP1qfVsj4dxbx4l/aX6j0bRdBWJB70yqDDHquOL4TP9wygs+vG8a43okNno8VpbUrK6ri0wcXsfafA+RllLF/Yx7fPLGMmR/Yb6AO7ipg5vsb+OH5Fcz6Ygvblma6VX9CUbzJEB9P6PDh4OaDZWE0Ejyw4XsMfXw8PVavovf2bUReWDcRoO/Ysd5jSufNY/+ll2Krcu390uHFF0l8+mlw8+9W7udfuHXc0bQcYeyM/IPlFOW4fg/SeUgvdk3vTOG+MKTNPsLGaobcLXEMHPCaFyI9QkpJyqSZ+DtdaJOw9WAxB7304N1XfJK06TWy/qKzrpI2+PDuBVQG8Eol4+96CJ3eN4WcirMPsebvPw5/HdwhkogzO7ndnkRiOrsdZrP/fr7DUmLdqvg75rWFvPTXFo/6LqlseqpV/yTvrGa2YIdrTxsa4q361IrSmKiEUBJTtBntB/DHC7s0a8sfpi5PZeJHy9iaWYILMz6dMrZHPDPvPUnbRhVFqddf/7cRm+XYN/H+Dbn88NxyZry7gf2b8shLL2PXiizmfr2NqU8uZeEPOwJuWr/SurWf8jqhQ4a4nrgJCsLYvj0JDz/U4C5ho0ahD7eXV2j/4gv03r7t8EeP/2aR9N67iKOnZ1mtWEtKKJk1y9VvhdjLJ9J721ZCR450+dicV1+lbPdul487msXi+0EEv7+5zuVjzrrtOaISY0mb14GNn/dk0xc92PN7L8699m8vRHiEzWaj86N/ebUPV1RbJav2Nu9FM3yStEnqFk2bTtpd0H/2wCKW/75Hs/a0pDMYuP+76QSHu/79Xvr0y0S1dS3BtfyX7+t8HXVyMklPnQChzp+UZa3/bfliiXeWSXJSQkQQd43tTpDe9ezDRwv3c9qUeW733S4qpMl91h4ocLv9xhwo0Cb7W8+1paL4xPg7PR/p1xKc9+4inpi+mUovVAWfcskAPr/heM3bVRSlftmpDY/izcsox1LPCgVWi2Tb0kx2rmh69K6i+Io+KopOX39F13//JenttyEystH9jSkpRJx5Bu2efILO038jfOhQgo877pj9hMlE4qRHGm3LkpqKtBz7YFSWl1O1y/0ESqfPP0PXtq3Lxx0451wOTpnidr8AbbtEe3S8O0ryq1g/O9WlY4JCw7jhjalc/NjLnHLtHVw06WVue28GYVH117rRQqXZSpfHvJsUcse9P27CFmDTBF3hs/qnF08aipYDUNb8ncrfH2/C6odMpzPu/Oz7pnc6yo/PTKLoUKZLx1iqjx0Vows1En6i88kf4fifDh3JtgSipPcLUhYXFze47Z5x3fn8+uGE1bPKSVN25ZSTMmkmQ5/71+U35n2ndqepXNFVn610OSZnzNzged2nGsVlnk9HVBRXhYSbiE8Ob3rHFqzLozPZmNHwuc1TnRNa989XUXzBUm1l1cx9TH1qmQdt2NgwL13DqBRFG6YO7Yk643R6r1xB17lziL7qKvRt20J4OIbkjsRcczXdFiyg2z9/0+Gtt4i++GJ0jlEyKd99S/ydd6CPiUaEhhI+dizd5s3FEB3deJ9duiCCgo55XYSGEtS9m0ffT+dpP7h1XNEnn7KtV2+yP/3UreOHnpni1nGeWvLzHtbOci1xI4SgY98BHHfGOST3G4jQeff2v9eTja8M5k+BmExyls+SNkIIrnhW2yeEe9fm8OFdC3j/9rmk7/DOCAhP3Prh117vI2XgsVlvgJA+CW61p0fHUEsXr4+2eeONN8jJaXiY2ond4rn1pC5ut59bZqHLY3/T/6m/KSpveL5sZlEF+3LLkFJyydCO3Hdq9ybbPpCj/U3Ziv2FmrU1+rW5mrWlKK447fq+GEytcy20x3/dqPlUqKO9Pbt5TxtTmq/12et5ZeUrfLP1G8zWwJ2i7ilpk0x/cx1r/kmlKNuzEbDV5dqtbqko3mBKSqLdE4/TY/48eq9eRfdZs2j72GMYE9vUu78QgoS776bHsmX0WruGjv/3Poa4pkdshJ90EsaEBDDUenqv16OPiCDi9NM9+x4SE4m58w63j897fQqFf/7p1rFC55+aBMt+DczZJgDXfabNwkECMOkFw1JieOuyQUSF1B35YdSBwc3LzVEv/ut5gH7gm+IrDpGxXiraJO3z/K55cSQRsYGzpF14TCzhsXGU5ud5rY9Trr2l3tersl1fDrZGB1sc7WzRZOoL67x+4YUXUlhYyNy5nicFpIRb3/yJQyKeJyeOIizISMfYUDrH239HHvhxHb+uPehxPyXVNgY+O5spFw9gZ3YJYUEGrhiWTIXFxu3frmFXVik6IYgKMfLWxEH0caJmzc1T1/Dv/ad4HFuNs99aqFlbAEWVgTn6TGn5YtuFceUzI9gwL43sfcUc3FXo75B85ue12o2Wa0hxZcu9WW7J8iryWJi+ECEEYzqMISY4xt8hOc1qs3LJjEvYVXgkYThl9RQ+O/0zhrQdQnpJOj9s/4FVh1ZhlmYGxg/kqj5X0TW6qx+jdt+BbfnkHyzDavZsCL1OL+h8nHsPzxSlpRF6PZ2+/46sF16gZNZ/SCmJOPlkEp94Al09I3Bc1fbuuyn45FOodm+keeZjjxN9zjkuH5eYEsGhvd4bXdvcDJr8F4WVrj+9EkCISU/vtpEkRQczunsCY3omkBh55J7+nAHtmLM9m41phXSIDeXcgUm8O2cXXy9LpcLFRYrSiy2k5ZTQMUG70i2+4NOkjbfN+mwzFz081Ov9VFaYObS3CJNRR/6hUjL259C2dwide3Yg8qg5ord+8BVvXXMx1iptlqWuIQGbwcSbb76JNBhBwPjx5zBs2DAASpc3PM1KStnoaiF69HTu1oVTR/Vl586dJCcn07NnTwwGA1JKVqxYQVlZmUfxW6WgQIZx0BbK7d+upWbpmYEdIrnlpK6aJGxqe/DnI0vlvfGf/eJTcGRAUYXZyvVfruKxs3o12daOLO1WkMotrWDrIc9XvlKUQBEeE8SJFx4Z7py5M5Nf33BtBaXTb+yrdVheZ9AJvL1mzKVD61+FQwlcv+36jRdWvIBO6BAInl/+PJNHTOacrq7fIPjDq6terZOwAbBKK7f8dwvvjn2Xe+fdS6X1yPXNroJd/LzrZ3To6J/QnyljppAYlujrsN2Wta8Yc5VrNwA6vcBmPXKjYjDqCIkwMeR09xeGaK6klGw5WEx5tZWiCjNvzd7BvtxyUuLCePrc3sSFBREZYiQq1EhqXjmxYSbiwz2/aVcCnyEmhvavv+619nuuWsmOgYPcO9jNZM+JF3fnl1eb/1LSWrjzm5VuJWx0Ai4e0oGHTu9Jm8iGB14Y9DrO6NuWM/oeqWE06axeDOgQzcM/rafcxUT76CkL2f/yeJfj9SefJ22GnJ3Mmr8OeKXt/IOeJRLqs2t1Jst+3UNJ/rFvaOm45bdhYe2mPVSF5tJ/QF8mTJiAvlZ19vu+/pnUzeuZ+/mHCJ2ek6+6gV9eetqjuASgs1QTlJlKZbJ9Ss/MP/8kISGBlJQUaGRYbmMJG3tCB3ofjKNj16507Vr3aZkQgioXl+erz0JLF9JlNEevE7whvZh7v3e9Oro7jj61WK2S/XnldZI53vbwjxub3klRmrF2Pdpx54fteP8250boGYP1mEKa3/OE58/vy/0/uf5+DjLoWP7oOFbsz+O2b9Y2uF9ybCgXDe7gSYiKjx0sPcgLK16gylr3b+bkZZMZ1nZYwCYzpJTcP+9+5qbNPXydc7RqWzV3zbkLs6x/9JcNGxtyNnDqz6dyfpfzufO4O2kXrs1Kot4UHhOEqxcBVz5zAof2FZGxvYCyomo69o6l98h2zfI85om/NmVy/7T1VNVThH1rZjGXfbzimNcF9unwH149hPCgIz+vP9Zn8PhvmympsmDUC24c1ZlJZ/X2ZvhKM6cLCiKoXz+qNm9242D35tm07RLF8ed1ZsUf+9w6viWZucX1lZmGJEfzwVVDGk3WNEYIwfgB7ThYWMELf7n2cBBgXWoOx3VqPiMifV584ITzuhEU5nqBWWeEx3ierbfZbEx9Zinv3zaX92+by6xPt9WbsIEjBXz1GIko6YmpIpbNm7awYMGCY/bt1G8Q17/xIde9/j4z3nnV4zjt/YOhvBisR54K/fzzz/ZPrO6lHYQQIEEU2bBa63/a1NDrziqVJjJs0dga+PXz1wpI1VYbheXVXHl8ss/6lGg/H1at+q0Eojs/HOvUXxy9XtChd/OZPlLjgiEdaR/j/IWHAEKMel6+sD8xYSbO7NuOC45LOma/cJOO/53Rk/8eOAmTuxO4Fb+YtX8WNln/07/ZB2b7OBrnVFoqGfLNEOakzWkwYVOjoYTN0X7f+zun/3I63279VosQvSo6IdjlpzbfPLGM/z7bytYlmaRuzmPxT7taXcJmU3oRd3y7tt6ETWMksHh3Lv2e/peUSTMZ+PTfjHl1Lvf8sJ6SKvvDR7NV8uGCvVz/+bFJH0WprfNPP0ITRZHrE3nppW73OfTszkTG+740x7QXvbMwijvenOV6wgTguOQYtxM2tU0c3pG4MJPLx13wQeD8DJ3hl78qN00ZQ+qWPOZ8vZWKIu3m6OcfLD/8NDeqbTDn33scETFNL+MMUJRfztTHlrvdt0AQXtqV/NBcVq1axdixY+vdb+vixVR7OLXomL6lDYkehDg8bakwt5BCUwG7g9PYG5xBclU7TioeQrA89pfaVllM9Z7ZWHN3Ikxh6CKT0EV1JPXOnXT58MZj9k9KSiIjw7X6DfLw/0GxLQi/riveiOEpsVw6rCNbM/JZm95wXaClu7MZ2a3+Qm2uuOnEFObvdD073ZjzBh5746cogeDO/7OfF21WG7M+28KetUd+9w0mHcYgPefePQi9vnkmJ5Y8Mo7Fu3K494f12GySp87pTWiQgad+30JeaRVGg6CrYw51x9hQbhrdhSGdjiSo3rzsOB48rSdTVxwgLEjP9SM7Ex7cum7+WhKzzVxv0sYmbQFbzPfGf290OhnjqpdXvUybsDac1uk0r7TvCWmTzP92O1uXuLaCp2L31O+bNGmnqMpGUVX9BaDn7cyl95N/M7JrPG9fNojwEKMmfSothxCC3suXsa2Xa6Oy2txzt0f9XjxpKJ8/tNijNlyVe8D92qVamrZiD2/P3evycXoBXy7dz+eL99EmMoioECM7s0sJNugZ17sNr108kBAnVxGOCDYy58Ex3Pv9OhbuzgUZqHeZnvHb1WCnvnFc+8KJfHjXfK+0X3Sokq8fXcbx53Vh6Nkpje5bkl/pUcKmhs5mAqmjuoG5kdkHivjvs80ERd0CIgybJQdz2S8g3V+hwGYMste0AZCS4BB7kur5Dh+yPWwfeqmjUl9NsNXEl22m89q+B2hvSXTsLpEV+ZTNex4slSDtI2isWUf++G4f+Tm9ltatBD5x4kSmTJniVHwnn3wy3bp140B6Bu/MWIVNCLZb2mDF89FWUcFGijQuzDll1nYuHNIBRONvjZ9XZ2iStLn6i1Uet1FbiFHH8xf007RNRdGaTq/jzFv6A1BdYeHg7kKMJj3tukej89NqDFoZ1T2BNU/WvSk9o5/z00I6xIYyyYnaWoFm68FiPlq4hz05pQzpFMstJ3WhfbRzD01aqpM7nszHGz8+ZnSqTugY03GMn6JqmJSSjbnenbL74vIXAzJps335IXauytKsvfdvm2sfXdhK7MnVvjxBfSrMNuZsz2bAs7NY9fg44sIDZ/ERJXBETJxIyQ/OLwVuyTiIMTbW7f5Cwk0IHTQwsLLFstlsPPLbdreOtUrsq9IAh4qrOFRsn0ZcYbby58ZMZm05xB93j6JX28hGWjkiOtTEVzceT7XFxu7sEs5+x7kkWl5xOXGRoW59D77m18eZNm+vjwqs+GMv1VWNL7s44531mvUXWdCHjh2OLRY57YUV/PTiavSm7ghdOEII9MY2BEXdhDCkON2+PPxfgRQ6KpPqHnvhhRcCsDl8F2adhUq9PYFUqa+mSFfKpE5vY8NmH/YsoGrbH2CuOJywOaa//Hy29e5D2cojQ8giIiK45557iImpNYVB1v9vuX79ejp06MDIE47H0HkoKy2dKEabN4fWCRuA7FIzf6w/yNZDjVeD35JZ5HFfo1/8T9NM8LUndGL5Y6cSEayePrV05qIi9px3PtsGDGRb7z5s69WbbX36cuDW25BuFtTzF1OIgZT+8bTvGdPsEzat1cKdOVz4wRJmbDjI5oxivlueyplvLWRvTmA8CfSX7jHduaL3FQTrgw9Ppw7WB3Nd3+voHNXZ3+Edw2Lz/hLVeZXeW03TE5vmp2OpbmV3XBpKifPS6rANsEmY+LHnD1uVlqnDZNfqhpqSPS/yH9TKpkQC9H3yb6+1XW2VXPf5KmQD95cNMRl0pMQ7fz664ANtlij3Bb/+hhmdHPbkqU/utS+pnNQjClOwjv0bCw5vS+oRRcEhbVYDEghM1dFUbY8gJ62EhI72YfCfPrCAqnIrUh5bBFgII8bQk6ku/hKdwQBCYDM3noyQgBSCiuQe2ILDDidMTj7lFLp3705OWU69yQCbTpJrLKSIEvaEplMVaWVg9lagiQsVKTlwzbUkPv00sZdPBCA2NpZ7772XFStWMHv6dMz1LNknbDaKi48kPz6/bhgT3l/K1szAXh5v0i8baKoI+Y4sz54q3fL1KtKKtbu51gl4ZoIaYdNS2SoqOPjoY5TMmQMNnR9sNsoWLGD7gIHgOJcEDxlC+xdfwJSkpswp3iGl5LHfNlFZ66RptkksVRZe/mc7H1/t/RUdA9n9Q+7ntE6n8fe+v9EJHWd2PpO+cYG5OppRbyTMEEaZxXujJoL0gblSkKsrRil1/e+MXlz7+Qp3yym6ZVd2GU//vplnzlfXPor7gocMQR8V5XE7g05LZvl016cJuSu5r/sjg7TwwbwdVHj5tFlQXs3WzGL6Jtn/fbZlFrM5o4iOsaEc3zm2wYV1Qk0GwoP0lDpxXj9Q0HwedPo9LXjGLX359+MtPunr4M5jR0fU95pnBJYiIz++sAqQmEwWqqsNgGjwl0voogAjFz32DD89+1gTrduTNkLaMGXuJ7hTV06//Br69D1yEVhmbvyC69oeT2I12LigywUM+jYUWeXczyDrhReIvmACuuAjw1EHDBjAjnffZX9KCjZD3V8nISXRtQqCmQx6/rp3NHdMXc1fm7Ubhqw1Z1eN23WogO5tXS+Yeqi4gllbs10+rjHX+LB4suI75uxssl97neIZM1w70GJ/Yl65fDl7xo4j+euvCBs+3AsRKq1dUYWZrOLKY16XElbszfdDRIGnX3w/+sUH/o2l2WamTWgb9hV7byWUS3u6X/DTm7oNbsPqv/dr1t61L53I/O92kLY1j5AIE8ednkzX4zyfUh2oRnWP581LB/HkH5spqvD+iK0aXy1L5dYxXUlq5VMxFfdVrl+vSTuDT+9E2rZ8MnYUatJeDWEAedRbKr5jGOfePUjTflz1yr+7vd5HlcXG+e8tRi8E0aEmiirN6IRAALFhJm4Y1Zm+SVEMS4k55h670tzyEvF+T9p0G5zIupRUsve3jGHUos7aPcKRsGmCrKbLsHEk9x3gZB92xupKLr32BpK696yzvWNkx1rZnaOOlQKz3kKwLpjLel9G5MUdKPz6XbA6kWm0WKjasYOQgQMPv2QUgn6bt5DesSNmnQ7pWDZPb7HQ7mAmlz/4YJ0mMgorAjph44qz3l7K7pfGO72/lJKLPljK2gOFmscyeUJ/zdtU/Ktq3z72XXgRssL9mlc1DlxzLUmvv0bUOedoEJmiHBFs1Dv+7h37iD1KFQptVn7a8RMHSg64fXyH8A6kl6Y3uH1M+zE8MOQBt9v3pkGnJ2uatJn61FKsZvt7oji3kv8+30LB2eUMPStFsz4CzXnHtefcQUkUllczb3sOXy3bz4Z0rR+MHuvzJft4Ynwfr/ejtFBWK+UHDhCa7NnDT6ETTLh/MPkHS1n++172bczVpBqutECP4QmcdkN/pJQNDgDwpT/Wu7YYjScsNrAgySqpqvN6WXUFz87YilEvCA828NrFAxnbq83hn49O6GhyJkkzExBLdFwyaTjHndnBrzGYQhqeqmUw6mjXLZKTLu9Ot6Ft0BtcecM0vq+UEoNJzwUP3QVAVFvXpjHsWXXsEoh6nZ4Lu11Inetoaf/Q6/SEGkJ5csST9I7rTeLDNxAy/GQQzk1V08fUHVlizS8guLKS02b9R1JGBgazmaCKCnpt28bw5csJPWrI4Yq9gTmf3R0WCaVlVU3vCEyZtZXOj/7llYTNlcPVKJuWxpyRwd4JF2iSsKlx8KGH2X/9DdiaWc0bJbAFG/WcO7AdQUctRx5i1HHTqMCr26I07Nddv2JtoL6dM9JL07km+Ro+Of0T5l86n2njp/H48Md5adRLzL54Nu+d+h56nW+mxbsqKMSAFqFFjYSgUMPhhE0Nq1my+q/9VFf6bhSKPwghiAkL4sIhHfj9rlFMv32k1/vUB8BNrBJ42r7wvNP7pp5+hmb9xiaFc/btA+jQM1qzNneutK+0GQgJG4Aps3b4OwTAfmtbbZXkl5m5+evV3PndWmw2yT+bM7G6WAunOfD7SJsaIyf0YM/qHIpznbsJ1rz/i7qRkBzBulmpmEIMHHdaR/QGA8YgwY8vrCZzdzGZu4sRetFg0d2GNfwmE0Jw8SMnHv768mdf5cNbrnKqVYlkRf4qRsmrj3kjP3PiM/SL68crq16hylZFiCGEOwfdyYA2A+gT1+fwvHKdTkfKF2+T/8uvZD3+eKP96eLiMB2ViTbGxSKAsLIyRi1ZWis2+3d9dFwxoccuOd6c9XtuNvtfbni0zW3frOSfLdou6V1bRJCeFy5Uo2zqYy0poXz1anQhoYQOHYIwBMzprklZr78OVdqfCyuWLWPvuefR7d9/NG9bab2en9Cfwgozi3flYjLoqLbYmDg8matHdPJ3aIoLPEnY1Pj6wNd8feBrADZdu4k+8c1nBETKwHj2rs31qI2ipYCoPzEjBOQfLKNtF8/rZzQXgzrFsH3yOEa+upD8cu8sJX/T6C5eaVdp3mIuughT586k3XY7srjxWpoSyPn5FxIuvkiz/rNTSzRrC2Dd7P0cd2qKpm26y5e1q5xlkzBvew5/bz7Eq/9sx+rkYkeBkQZzTkDdxVz9/Im8f9tcv/Q9f6o9axgcYWD8XQNY/VcqO5YfO41Huv2bWnNc3V8PvVFHbNKRKtdhUdFc/sKbfP/4/Q22ZNHZKAm1EFylY4ZuETN/HMvvF/xOpKnusmiX9LqES3pd4lR0sRddCDYrWU8+1eA+Xab/dsxruqAgjD16YN65s87rAgg7/dhlPUd1j8ekg5a0SMPKfXkM7xxX57WTXpnDgYJj6zxobdH/TvF6H81RwQ8/kPXSywijEaREBAeT/MnHBPcJ/BsIS04OJbPneK19c2oqldu2Edy7t9f6UFqXEJOez64dxsHCCg4WVtA1IZyYsJaVoNdKYWUh32//nuWZy+kQ0YGrel9F77jAeC/2je3L7kLt6hT0/6o/m67dpFl73jbmsp4eJ22ABqdEWK2S0KjW8b5ImTTTJ/3cNbYbCRGBWdxa8b+wwYPptdI+I2Fbr8bPszlPPKFp0kbrUTHr/j0QMEmbe8Z243+/BN65vcJs5fHfNlLoQl0tH62JpImAmB5V2y3vjPZr/5UlFn55aW29CRv3SU6Oev+YV4WAcdf2Rqev+8+Q1K07N773eb0tbU0u5odT05k58hA/jsugPMxKbmUud8++2+MoYy+5hI6ffYoxJQWEwCqg1GigbGA/um5YjzEhod7jOn/3HaZeveq8FjpmDB3ffPOYfY16Hd/cdILHsQaSL5fsr/N1yqSZPknYRATpiQ5TFytHq9y6layXX0FWVWErLcVWVoY1L48DN96EbGJlNn+rTktjz/hzGl4hSiPpDwRmXQmleUuKDmFoSqxK2DQgtyKXCb9P4NPNn7I2ey1/7v2Ta/6+hjmp3kvSuuLPvX/6OwS/Co0KImVAXNM7uim2bRiRcS27YG7KpJk+S9h8c8MwHjq9Z9M7KgqQsmhhg9tqqkns+f3Yh9Pu6jWinWZtAVSUWFjz735N23TXRYP9W9KkMa4kbADuOqWrlyLRXsAlbYwmI3d8cArn3jOApB5RRMQ195tSezGZPhMv5sY3TqL/yR2ITQqj86B4LntiON2HJtZ7VHRCG2585zNHC/bHNnvblrGmVyEWg8RikMha/3prc9ZSWu16MWcpJbtWL2fRD1+zddE80mzVFN19G/8M6MKs/l1Y3LMjC2U531x3GVVl9a9KpQ8Po+v03+jy10w6fvIx3ebPo9NHHyL0x6YvbTYbV3yy3OU4A1lkyJEBaxPeW+SzfheqUTb1KvjxJ2Q9dVtkdTVly4+tARVIMp94ElsTw3i1YN63H0tey6kvpSjNwccbPqaouohqR+F/m7RRaa3kmWXPYLX5f6ULK/6Pwd+Gnt0Zg1H7S2O9QXD+fYM0b7e1evGCvozu0XJX41K0F5KQADpdg7WBBTD/4/c062/khV2JaqNtknb5b3ux2fw/VaGgIrAfgLpi4rDmM407oKZH1RBCkNwnnuQ+8QB+mzLVFINJh8VswxSsx1xlRdb7PhKAYI9tHN1CjZw0sYfT7UcnJjJh0mSmvvk4IdUGNnQvxGpoeHpWuaWccFO40+3npO7j60fuqb9Gj5QgBFJvH95XYq5m4fgzGP3R5wT3rP97COrShaAuDc8tXrs/jws/bFkJG4BbTrJnac1WG+vTvX/DXSNGjbKpl7WoCBr4o2YrC9xV6qTVSvnKlT7rL/OZZ+n4zts+609RWruFGQux2I59ClhprSS9NJ1Okc3n4rGlSkyJZNSl3Vn80y5sNhv1/HM1qcugeLIPlFBZYsYmJTGJoZx9+wBCIlr2CDRvjrDRC4gNNfLKxf0Z21vbEQxK69F94wZ29qu/DmRZkJHCsGB2rlpOj2GezwjQ6XVc9ewI9q7PYefKQ1SUmDm4q9Djdr95fBnXvnRi0zt6UXhQQKYP3JIQ1XxGP7acn7qPCR2MvaY3HXrFIBB89lDjIywKs8rd6qfrcUPZcLqRA+VpVJkaz67arM5nX6WUDSdswD53qxarXse+iBA6XX893RcucKmoq5SS41+cTXZJy1u15sYTO9ElwZ4oyynxXRHti45TFy0NiTh1HKXz5x+z8pK0mAkdNsxPUTWtYt06n/ZXOnu2T/tTlNYuKiiKjNJjl0q12qyEG51/4OItkcZIis2+e/AQqPqObk/3YYns3ZDDvG+2Y7PUvU7SGwUp/eLZs67+RQai2oRy5q39Kc6tRG/QER6jHrDUphP2oqHOCjbomHxeXyaqlTIVDxkMBqrCwjCWlyOkRId9PoRVCLa0tw8UmPH68zw4Tbupol0GJdBlUAIWs5VP7l/gViK4ttIC/yzYU1uwUc+Fx7Xn13W+W/pbCcDpUfUZcVFgVobPzyzFFGxg1cx9Te7ba0Rbt/v5+pIfqAqyNVni+sxfz3S6zbRtm+tN2ASZLfRJz+Gk7Qc4fncGCcVHpkRVG/TIqirKljk/Wua92dvp/OhfLTJhA/DV0lQyC+3JgVgf1nHYmlmCbIHL2Wkh8vTTCe7bFxHiyJ4LgQgJIf7OuzDEea9egadsVVXgyxWuAmCIraK0Jlf3uZoQQ92negadgSGJQ4gL8f+5qX1Ee5f2/6j7R16KxP9MwQZ6Hd+OzgMS6kyXEgIMJj0jLuyKwXTsJbTBqKPn8W0RQhCVEPL/7d11fFX1/8Dx17m1LjZGw2gGjA7pkJAUCRFRscBARbFQMfiqiK0YWD+VsjAQpUFEkO4Bo7thxHq3zu+PSYzVjXNj2/vpg8eD3fM5788bhMvu+3w+748UbPLhTMEGIMtqZ80B2c4rtNFkw3o2VivHqcgQ0k0GTocHs7ZWRZLDgq88sN69drXm8xqMeu5/txNBoSVjvcTEgQn0a1S8HyDPGd3W1yk4pVj8yWnWPY6DW89xap//PAFS7bBh7mE2zD2MIbDoDuE/vLaedoNqEd+2otNzhQY49gTOho2FhxbSM64nBy4dYMe5HZQPKU/zcs3RKbm/uUhLzntCQoDFSofdRzHY7OiA0GwLEYdOczwqlLPhwZj1euw2G5mJiQTWjy/yA/Bj321kzrZTDv86iyOrCm0m5Wzf0ykQHqiQkuX5YkrSqTTWHUymdY0Yj89V3ChGI9W++ZqU+fNJmT8fXVg4UUNvJbh5c1+nVqigxo3B6uYjGCfZMzLQBQd7dU4hSqs+1fuw98JeZuycgUlvwmq3UieqDm92fNPXqQHQsXJH9l/cj9nu2EOWB/Y+kOt0qISpV7cdFKdTowrT/b76bFpwmO3/HMeSbaNqfBnaDqpFeEwQvR5IYP7niVdOibHbVNoMrEl0Jd+vmvKFQ5P6FLlFKjZER6oZMi2OPzQw6hWqlJF/p4Q2FEVB36wpW/btzrOr4LI/33udKl99R3BYeL7XXWUM0HPvOx1RVZWju87z94xdpCb7fuWMKwKNepYmaXloj/c1qhLl6xScojjztL5Fixbqhg0bPJhO4X7/cAPHkvyncOOKmCohtBtUh8xUM9GVQylTIaTom4AHFz3Ivyf/LXJcr7heHEk5wo7zO67OGRjDj/1+JDb4atO21OSzfPHwPbnujT9+jqrnLnF9+2CbAnZylmXpVUCvB0UhtGtXKr/zNoop7wqTMylZtJroHydilFRd65Xl67tbOX2foigbVVVt4YGUvMbX70WekLp0KcefGJtvE+XL6m7exO6mzTSbs/qC+QTGxWkWTwhnlNb3ogtZF9h1fhexwbHUjPSfkysuZV9i0JxBnM88j0V1rNHkY00fY0jdIUQGRHo2OT9lzrJyeHsyNqudqvWjCQ4v2X1rilJY0aZz9Ug+ubs1d3y1ls1HLzocM9ikZ8nYTlSM9GzvieL+flQSvy/yFFVVee+2foWOMQQEMvrr7zEYjB7JwW6zM2X03y7dO/qzrtom4wJvnRLnKY91rsHYmwo/Bt5XCnovKhYrbS67eUwLpo5fQdq54tu1+sSxZL78YjYmexCZxjTCy5t4cswd6A2F71T75MZPaDKjSZHx5x+an+e1c1nnGPrHUJYNXXbltdAyeVdoxKRm5CnYAKiKgvHa9ay2nBMm0hYt4sCgg9T4fTaKLnf+X60sesuYcI/d2TXGwq+l/b28wIKNqWlTan7/HUn1tP0H5tT4F4mbMV3TmEKIwkUFRtGmYhtfp5FHREAEv/T/ha+2fcW3O7916J7JmyczefNkqoVX4/nWz9O2YvFabu4uU6ChwFNAS6NDk/oUeM1stdPv45UcOJPqVMwpw5t7vGAjShelgBU217JmZ/Hh8FuoltCEAc+8hCGfB9Tu2P7PMZfuMwb4vrNJ0wkLfZ2C2/y1YFMY3/+fd9KI1zoQXi7vm7fOoDDyg46MmNSGvo81pvPtjp/S5C0qKkY1gJiMyhyN2M2BMlvIOqZnzOQJPPn3k3yd+DUXsy7me69er+eDzh+4PPe5rHMM+3PYla8VRaHbyNG5xmQZ86/h6QopDpj37uXsx3mPyLP5sF9GVKCe0ID8yk8ly4t96/s6BaEhfZmCl2mWvfeeAq+5I3vPHo/EFUIUTxEBEYxtMdbp+w6nHObRpY/yf4n/54GsREnwy6aj7DmVihM7owCYseaQR/IRpdvdH3zh0LjDiVuY/twYzec/knTepfuGvOi7AzVUVeVSpoULmd7dyq+1Kbc18nUKLil2RRuAOye0Yej4lpSpEEJomQBa9avOqPc7YQo0EBoZRLX60Zw66H/bqJT//jsddpiD0VvYXn4Fsxq/iTnFzuJDS/h066f0/a0vR1KO5Hv/jdVuJHFEIm+0e4OKwc73xtmevJ3lR5df+bpxt17c9PATBP63Z/NA2Uis158aBdiLKEgnfzolz2t33RDndH5F6VY3hvgKYYWOGdSsEptfuYntE26iU52S2+8lNEBHzdjCfy9E8RLSt2+B1y58/4NH5rSnpHD0Cec/oAkhSi5FUehfo7/T95ntZqZsncKl7EseyEoUZ9uPX2T87B24sj74333ShLikUVWV7Ix0jiVt5/juJOz/reD3pugKjn+OOn/sKJ8/fA/pl7R7bzt9wLnPqQHBBoa/2oqoGMfaamht5ppDxL+0gMYTFjk0vui1TL4RHqDQq0kVX6fhkmK1PepaMZXDGPZy6wKvV0uIYddq/2uCezRiFwvrfoVVf3WL1+q42QRZwshUUrHYLLyx7g2mdMtbCLmsb62+9K2V8wEv1ZzKupPreGr5U1jVoiufH23+iE5VOl35ukGnG2nQ6UYADi2Yz8HxLxCTkQWATlWxo2B0oO9RUv0G1N2eiO6/bVLVYkJoVCmcbccLelOy55yzpzhWN9z/ei+S07NoNXFZoeOSTl6db8OhCw7FLo62T+jl6xSExs5/+mmB1zJWrfLYvGnz53MyIoIKr7zssTmEEMXL6x1eZ82JNZzJOuPUfUadka1nt9KxckcPZSaKm6PnMxj82WpsLm7pDi4FK6dLgwunTrBu9iz2bVhLVmrezwbBkVHUbdOelv0HEZZPCwdfS0s+y2ejhnPnm5OJjXP/VGNnDoD1dQ+beYkneGH2jqIH/uenkS249Uv/7K+0bUJvX6fgsmK50sYRtZrF+mWZ79+4X3IVbABQINOYs8fXjp01Jws+UntX8i76/tqXptOb0unHTqw4tgKbasOmOlalLmxclNVOBYsdg13FYFfRqThUsMlJ3M7u+g04+fnV5Ya/P9Kevgm5jzqvFBHAE8ZjvLjxexRVRVHt6OyF5z5/TDv0eh39Pir6g+vOk6nUf2kBh5PTiQjyTPMwXzv4RvF9wxEFsx51bX+zFi7++KPP5hZC+KelQ5dye73bnbrHrtpLbVNikb+v/z2Ixer6lvknu9fVMBvhbekXL/DrpFf4eswoti9bnG/BBiDj4gU2z/+DLx66m/VzfvVKbsYg508lmzn+SU3mjmtURpM43vDqn0kOj909oRt/J+U9odgfLHm8g69TcEuxXWnjiBFvtGPquKJPXPIWFZWLQQU8tbqmwGQo4H/LP0f/YfRfV/vQnM86z7MrnkWfb/vg/N3f8P4Cr5ni4go8/s5RF99/n6x/llN95kwUReHj4c2ZlG3lwNk0yocHUjbYwN8dJvJBvb6M2fIzXY5twmC3syuqKh81GcShiNzLFf95ujNVo3OWAp5OdexYvAyzje7vLefj4U0ZNW2TW78ef2PSOdZATRQ/0Y89yvFRD+R/UafDfPYs8buSNG9GDDj3yEcIUWo81/o5nmv9HAAvrHyBhQcXkm3P/99iBYXooGgSYhLyvS5Kpx0nUrC5+E9Mx9rRDGtdVduEhNdsW7qQpf/3qdPbn/6Z+TWHEzcx6Ln/YbfZ2DBvDqbAQBp373VlRb8WAgIDsWRmOHWP3aLNYTgn9vpfG4+CnHPw8xdAQEAAn6485LlkXLTphRspExbo6zTcUmJX2gCERgYw+rOuVI6P9Eh8nSHvh+cKtSJ48ONODH6hJfqAvNcDrEVXdbPsWfyQlLeHxSN/PZLveBs2VAd2CuvQ0admwZ39gxo2IDC+XpFxipK1cRPpa66uFgoNMNCociRlwwJ4fsYaRrZ/jOF7ltDl2GZMdhs6VOpdOMw7Kz6hTGbu/aKXCzbOMttUAvQ6BjZ1vvePP+tQ2/+WjApthHfsCEEFnJBht7O/Q0fOfPgh8buSiN+VRL2knYTfcot3kxRClFoT2k7gvkb3EWmKRIcOg5LzgMmoMxKoD6RaeDU+7/65PFgQuSRUisCgc/zPhB7o1bA8q57twrT7bvBcYsKjdvy9hMVffORyv5rD27bw8b1D+eCOW1j53Tf89fUU3h/Wn7W//aRZjhnpaZrFclbquSyHxhn84LQo1ccP9no1KIdJ7/q/K/tfv6nYF2yghK+0uezmMc0AWD17L5sWHNUsrl6ncON98SQuOwYKNLmxKjWbxQJQrkoYD37YBYDM1GymvbgaS5aVuPMN2RO7HlUp/C/A6+te52zmWR5t9igA2bZshwozhSmvC2DI1FaEhFVgdJvxtKyQtwN5lS+/4uQLL5C68L/j3BQFdDrCbr6ZtOXLUZMdawh36q23qfnrL7lem/33dtauSeLxvX8Rf/4wBvXqclkdYLTb6HNwFdPr59+vRQGnfgdGfLOB/a/fxGsDEnhk5gb+2lP8m9l9cnszX6cgPCh+8yYODh5C1vbt+V5PnvIZF2b/TmDt2mT8849m8xoqV9YslhCiZDLoDDzU+CEeavzQldcyLBnsTN5JmCmMOlF1pGAj8ri3fXV+WH8Ea3b+H97HdqtF+zqxbD5ykXLhAXSLL0egUfrYFGdHtm9l0RcfuR3HnJmZ57WVP0yjQt36VK3f0O34drPZpfu2/72Ehp27uTzv5kWHHB7b+ubqLs/jrvjx88i0OvfZM27cXM3zmHJnC7YevcDNnzjX3zE8UMe2V0pOD9BSUbS5rM2A2rQZUPvK1ynJmUx/YbXL8QwBeuq0LE+dluULHRcUFsADH3QGoP/BBG5e2gerIbvInjtfbf+KBxs/iFFvZNEhx7p1F0hVOWPL4ISiYLy0j4cW3M1jLZ7kroR7cw3Th4ZQ+cMPsNtsZKxdiy05maCmTTH996EuqXsPOFp04cty8mSe12bO3cibKz8lyJqNLp/yi8lupXrKyStbNVrH5T4C+dmedZi00Lkjimu+sIAQk45Phzfn/+5pza+bjvLKnJ3YVTvp5uK3JSQwoGT26RFXVf95FruaNEXNyv8pjP3kSTLy+fvljsqfFdz4XAghChJsDKZF+Ra+TkP4sUqRQfzyUFvG/7adjYcvoAIGHQQZ9bzSvyGDmud8f9msalThgUSxseDT9z16ItSsCeNoPfA22g+9w+UYqeddO3IbYOGUD6jdqg0Bwa7tBlj16wGHxzbq7JuTjjxRfHHFkP/eHxpXiaJGTDAHzjm2nW3b+K6Ehxawer2Y8v2aKx8Kjw7igQ87orhY0M9MtZB20bHlbZdVrV6OzfdvIEhX9B8ku2rneNrxnC80qC9cPs7botORrdMxecO7pGTlv6dSp9cT2rYtEf36XSnYAFR7+y2H5gpqUD/Pa3UObCXEmoUeNd96VbbOyJ6oqhhUG4pq54cH2+a6/mCX2rzStz5OrLIFIN1sZ8Q366n+3Dwm/JFEhsVOhYhgvrmnJb0alnMumA+teaZT0YNEiaB6+fjLoFq1vDqfEEKI0qNe+XB+fqgtByf1YcP4bix4vCObXupxpWAjSo7dq1aQmuz5RrRrf/0Bmxv9ZWaOH+vW/Ev+z7WHXc5uNdKyh4+jen+w3Otz5ufmxhV4e0jjK1/PG+P4qYRhIcV/O9T1SnXRBsAQYODhT7ry4CedqNwgwun75326zaV51921jmB90f1tFDWnQrHqhBtH/qpqvg2GsxWFdj+2Y+KaiQ6HCm7SxKFmxWUffTTPa21OF9x93IaCWW/gr8rNaHNiO/fuXpjvMuu721fnwBt9ODSpDy/cVMfhvC9LybJis6vsO5vO/VM30CIumpn3t+bBjnFOx/KmXvVjKF8m1NdpCC8JbtXKa3NV/f47r80lhBCidIsJDaBWbBhGfan/CFIiLfnaeyt3N8z9zeV7090sLF06fcql+1b9steteb1h5ynf9foBGNaqCocm9eHDYblbQgQa9Xx3f9HfH9cqG1Iit+rKO+Z/9Ho9Nz/anNGfdaX5TY53qj97JI3Pxyzj13c2cnSXc0vt1t6xtsgxU3dMBWDBoQVOxXbIf3+gv9/9Pb/u+RW76tiRjGWffqrQ66G9ehHcuHGe1+vVyLuqRQXsKOyPqMjMOt0Zve1XHt76KyNqFl0hHdm5NovHun58m82u8uqfOxn+1Vo+++cQ97St5nIsR5kKeQ+pVib/X3OlyECm3NXaQxkJf1T5ww/A4Pndq1V++pGQpk09Po8QQgghSrbMtNQCj/T2BNXu+lHy7opv59rq9y1Ljjk8NqpCyVst4ojXbi64X1HbWmX59PaCv2/VAb+PbueBrHxPijb5uGFALZr2dHzJpjVb5eS+S8z9ZCs7Vx53aq4vun1R6PW5B+eSbknHqlqdipuLA9XGl1e/TKsZrdhwakORY2PuvZeQrl3zXjCZqPTRZCq/926+98V99CGQe6eXAuhQqXPpOA/umEPL07uItGRQ9oECjj6+Tu3YcPb8r6dDY4vyzarDmsQpTICp4L14i8d24Z3BjQj5b4xOgcHNK7Hy2Xx+r0WJpg8Npc76dYR0dHwpqLPKPvMMoY0aeSy+EEIIIUqP3f9qd0CCI8rXqF30IE9QFJrc1Nfp2/74ZLNT4weNa+70HFpwZo2KQQef3a7d95K1y4agL2IVXu9GFXlrUEKeIkajyuHsm9ibkMCS2f+zVDUidkbbW+oQGB7A6ln7Hb7HZlH59+d91L2hAnqDY/WwNpXaFHrdarcyd793mkFl27O5d+G9rLl9DcHGwrduVfnkY1LmzuP8zJnY09MI79OXyCGDOTt5Mudnfkf4oIGE1K+PqUoVFJMJAGNkJJUmT+bEY48VGPfyG0VQXce3PplMBg5N6kP98XPJcKO2VZiW1aJ4e3ACJ1OyiQkx0f2DFS7FSc228f39rbn32/VkWnOeEESHGFj5VBdMBh2DW1RhcIsqqKpaIpf2Ccfpg4Ko+sXnZGzdypH7R6KmpmoS11CtGjX/mIPuv7+XQgghhBDu2rzgD6/O98sbLwMQGBrOQ1/OcLj/S0a6699PBYSE8sCUb136Hv1I4gXn5goIcHoOLfyvf31enLMz32uv96/L7W1qkm62EWzUo/uv0WjAj9so4IA4p5xKyXZo3K0tqzKkRRU2H71IcpqZplUjiQn1ze+Xt0jRphDNbqxGuaphzH53i8P32O0ql85kUqaiax3Fr9euUjteXfuqJrEcoaLSfVZ3GsY0pElsE9roahE69Q/sazejDw+nzD33EDl4EIqiENG3DxF9+wCQPPM79rW9uhwtc82aKz8PHzyYiq/+L+eeHt05VaUK9iJOoLKcPImxQgWnct/5Wh8+XrKbd5bsc+o+RwxoWom4smHElQ1j9X739sG2qRVD0muFH0EnBRtxWXDjxtRbv46UlSs58cijBZ4q5ZCwMGpIwUYIIYQQGjt/wvGtP1rKSkvh/WH9GTjuFao3Lfo0u89G3enSPE/++KdL95nNZr58bKVT9yg+3AtzZ9vqpGZbeGvh1f47emDvxF5XCmOhAblLCEa9jmyb+9vVUrOtpGZaCAsqerWMoiil6tQ52R5VhEq1yzD6M8e3p1jNdgJDnVuWVTWs4B46r7V/rdB7w/RhTs3liBRLCqtOruLL9Z9wcvRjZM1bjO3cOcwHDnB64kTOvJn7BClbejpnXi24sJTy888cHvM4qt2OLTW1yIINQPYBx4/Du9Yj3epyaFIfXuxd16X7C/LG3B1Xfl4zxvWCXKfa0VqkI0qh8Pbtqb1yBbHPPUdQkyYogfnsddbp0FWqRPl5c4nflUTtzZuIGDSI4DZtqDj5Q+LXr0MvBRshhBBCaMiX/WUu+3XSK9iKOH1TVVVUq/PL8p0p2JizMln502z+mDyLi2dTnS7YAPR71Lfb1x/uUodDk/pc+bF/Up9CVzKN6lhDs7kNenlwnR9ZaeOg0Z915cuxyzFnFL32KzjcuQ9F3/X5jvY/tM/3WlENiFNt2myZyI/FCM/cpwcb3P2XnV4bVcjM5ML33xP9wCgMUTnVzeSvvykyVuaiReyq38DhuU3V3GsKfF/HWtzXsRZ/7zzNyBkbsLj5b0maRWXC74m8fHMCsRFB6BWwuXAM+7f3SkNh4Tp9aCjRI+4iesRdqHY7mTt2kLpgAemrVqOPjKTMHcMJ7dr1ykotQ1AQFV8vvPArPCv7UAopy46gGHSEdatKQAU5BU4IIUTJYrWYfZ0CAD+8+DTDJ75X4PX/G+NYz8xrOVOw+ef7H9m04ACGwKZAMIfH/wsYnVtBr0CV+Bin89SSqqpsPHSBsEADdSuEFzr2x/WH2aXRiVNlwwIIMkl5Ij/yu+KEke914pMH/ypyXPKJVKIrOr4C5ufdPxd47dU13tsalYei5BwXrodvu+k4FG3j4UWgmExk796D4YacAoT1lGvH3hUm6/RpTJUdbwZdkM71y7F3Ys4Wrj0nL9Lv41Vku1JtAb5ZfYTxfRug1+vY/nIP4l9Z5PC9oSYd2165SbY9Cc0oOh3BCQkEJyT4OhVxDWtKNpYjqRgrh3Ju+k6sx9OvXMvakUxgo2hibq/vwwyFEEIIbRlMAeiNRmwWi0/zOLV/T4HX9m9az6XTJxyOFRgezoOfTXN4/Lkjh9k0fxeGoBtQFNeb4T70SWeX79XCEz9s4rctJ3O9Fh6oZ/JtTelc7+pJwDuPX6L3R86vIiqIQafw3f3ycLsgUrRx0oMfd+KzR5YXOubE3otOFW0+2PyBm1l50JUig8rfzfS02mujxdFMjBWv9puJuvdeLv3yi6bTHr/7HsITt2kas06FSHa/3huAf3afZdT0dWQ5uUKy/ovz2T2xD0GBRg5N6sPGg8mMnLae85k5K7ACDBAeYCDIZKBnw/I8fmOdEtvFXAgBx8Y515Q8a1sy2Z1TCXDi3wghhBDCnymKQkBwKBmXnGu26y3Z2dnMfnOCU/eM+uQb9HrHPyov/WYqhsAWbhVsRn3Y0eGGylqz2+20eG0R5/PZVZKSZePubzcQXz6U+Y/nHHeuRcHm4U41OHIhk/gK4YzqUAOjgwf5lEbFumiz8vhKvtr2FWa7mYG1BzKo9iCPr2TQG/S0GVKD1bMK7rlSpV4Zh+OdTjmtRVoeF5UGQ1bYqHMKVKuV/f1vptbaNRgDAgiqWQNjjRpYXOxDky+LBavZjMFD/Tc61i3LrtdyVuD0ev8vkk5nOnRfth0GfbqSXx7O2c7WvHo0m16+ySM5CiH8U/rm06RvPIN530WX7r8waw/lx/jmKE8hhBDCE2LjqnNoq++LNseSdlA5/mpLhtnvvs7+daudjmOzWDCaHD+RKPV8Kij59Bt0lA6MAb75aD54yko2HL5U5LikU2n8s+cMISa9W/MpwLu3NmZgM/d3VZQWxaZoY7aaeWr5U/xz/B/sqp1AfSCZtqsftBPPJTJ953Rm3zzb44WbZjfGcWTrOY7vSclzzRSiJ7Kc441qX1//upapeURYhspbX9sIyQLD5d4wWVnsa9yEqIcfovxjj1Fr3lxOvPQSl36a5VBMFRWFwv8/7W3ajPgd293Mvmjzn8hpNK2qKh3fXMrRi4UfN7fxyCVqj5vL/Z1qcF+HGiX+iDkhRA5rmpnTb29AdfNcS1sR7zFCCCFEcdPoxps4tHWTr9Ng7kfvkJZ81u04gSHO9aBr2Kkt6+enoOgjXJpv9KeOH3yjpZrj5uLMdzUPzdjIi/2c3+b9VPc6dKwTQ2x4IOUjgpy+v7QrNmuQ+s/uz7Jjy7CpNlTUXAWbyw5cOkCjaY0Yu2wsaWZtGiIVZMDYFtRvXzHXa5Hlg7jnjfwbChdk+5nCixImvH/Si3pdy5ebNtgJyr6mYHONC59OIalFS1KWLKHi//5HpQ8/dGiOogo2ANhsnP/pJ4fiaUFRFFaM60adskUX3SzAlOUH6PDGEk5cyMRud61PjhCi+Dj3ZaLbBRsAY9XCm/oJIYQQxU3Nljf4OgUATQo2UZWqOH1P6wE3YzMnoqrO9/Wp16680/e4w2q18cU/+6n7vHMFG4B0s50+DZ3PV9EpNKoSJQUbFxWLlTbrT63nePpxh8cvPrKYxUcWM67lOIbXH+6xvLrcUY8ud9TDZrOj0ykurfA5ay78jcWM9t3YK4VUQq/oSbekMyphFLc3uB2ALae3MHrJaC6ac1YQXf7lNDysYirsb3RaGscfeZTjikJ4//4Et2tLxqrVV6o/dlQUHCzUXOf0Sy8T2rs3plDvnbiy6MnO1H9xHhmWogsxmTZo+2ZOc+pmFUP57J7WxIa5sTRSCOG3rKczNIkTfUe8JnGEEEIIf6HT6ahYN54Tu5N8nYrbbn/tHafvUXQ6HvlyHJ8/+jV2azSKPhTQA/oCPyNe3nlw453eOaDAYrXR/NVFpGS7d6zuhUwr1coEcfi8Y+0lAvQKPRuUK3qgKJDfr7Sx2Wy8ue5Nl+6dtH4SH236SOOM8tLrdcXmVCADBuYOnMvcQXP5+7a/rxRsAJqUa8K/w/+F7Kpc+8s5FaVgc+SXp6qk/P47GRs2EtytG7qwMJTAQPQN3Tu+e3+Llm7d74qdr/Z2+p5NJ9Jo9fpSLDY3zxcXQvidrENF7/V21JmTZzSLJYQQQviLG24Z6usU3NZiwGACgx1vdXEtg8nE6M8f5NGvhvDARz1QFAOKoqBS8INgU4jnP0Pa7XZavr6Y2uMXuF2wgZzn8suf6erQ4/gAg8LDXWpRK1YOYHCH3xZtjqQcoeP3HWkyowm7L+x2Oc4XiV9gtmq/WsUbDBovhKoYXJHNIzaj1xXePEo1x6Dar/41nNtKh8WZVLKzyVi8GHtqKmpWFvbth13M+KqkG9q4HcNZK5/p5NJ9D0xfr3EmQghfUq12kqfu1Cye7dN9msUSQggh/EVck+YEONkLxp807zuQTsPu1iSW0aTnvvc7cLmyUVDhJsOcpcl8BbFabdR4fj5nU7X7PFwtOqeodXBSn3yvB+phUNPyPNGtNn882oEx3epoNndp5ZfboyatnsTMPTM1i/fKqleY2HGiZvG0kpKdt5Hxtaw4eR51ERYOWejQuLrBfdhLIjmdW+BIrMLkfjoenG8nLBOnNzkp5FSYL79ZubJNiosXnb/HTZXLhPJi7zq8Om+PU/f9teuchzISQvhC1v6LqFnavh+rqlpsVmgKIYQQjlAUhZufHs9Pr4zzdSpO0RkM3P3eFKLKVdA0bmCQkdFTuvLxg0sL/vxj0X4NxaMzNvDHds+cUFy/Qu6i3KFJfTh4NoWXZm+nS71Y7u1QyyPzlnZ+t9Lm2eXPalqwAVh42LFihbcNmTPEa3O93/79fF8328xk23KfZPJG35uwnx6O3RqKajei2vWsrVyXo71vc6XcAuQUai6/WanX/OeM4y+Md3F2193XsTajO8V5fV4hhP9IXX4MJ9+uinTmo83YM5xvViiE8KEzu2DPQrjkeJ9FIUqbABe3FvlKwy49GDP9V80LNtey6LIL/NyTEux64+S/d51i0fZTV77eezqFuHFzPVawAZg3Ju9OhOplw5k+sq0UbDzI71bazDs0T/OYFrt/fmN8IuOEV+ZpEt2EbjW75XrtwMUDjFo8itMZOX+pIwMieaLZE/y460e2nzyDWW2P9eBDKHoztaIq8M6gtjSuEsX5ti05/eSTLudyuXATNWokpvLlOf/d91gOHkQxGlHtdjAXvHQv5ZdfqPT6ay7P7aqnezXgyZ7x1B0/H4sD20CDDH5XCxVCuMF8ULt+NpdZTqZz4de9RN/hneaDQgg3ZF2C74bCyS2g6MFmhoZD4OaPoIgt50KUNgc3F582Afe+/wVRFSsWPdBNccMMHJ95tfEwXN0u1XxkWafjPfLdJv7cdlLTHB2xYIxzpyQL7fjVp0u73TMNXEOMxaviq7Xpfafn+tpsMzNozqArBRuAi9kXeXn1yySe2YcSkExghdmE1nwfQ+hO9p27xBcr9gNQpk9vgnvd5HZO9qwsTv/vVSz79oHNhpqVVWjB5nrZJ06QuWcPqs3943cdodPp2DuxDxue60qYqfC/Nm8ObuSVnIQQXqLxKpvLMTN3JKNapXG5EH5vzqNwdB1YMsGcllO02ToTfnvA15kJ4XfKVff/1RYBIaE89MVMrxRsAG7p0IP9ddYAuXccbG82j97xPZ2KtfHAGa8XbG5tXpkDE3tRr0KEV+cVV/nVShudzjM1pMeaPuaRuO5QVU98CshrTv85eV77v8T/w6rm359BZzBfPTlKsRJQdjGgMi/JSnJaAtGhAVR7/32S/loG2dn5xnDEpWnTix50vcBAdrdti/38hVwvhw8eTKXXXnU5F2fERASR+L9eAIz9fiO/br26JFEHTBzYkP5NKnklFyGEdyhBBtRMbXvaAKCCaldd3nYqhPACcwbsmgtqPg+JEmdBQDj0fc/7eQnhp+IaN8MUHIw5I8PXqeTx2LSfMQYE+mTu98e+wK7zu3jzr/ewY+XpzmN5NOZdp+Pc9n8bPJBdXkYdfHpHM7rX99y2MeE4vyrawNWmtVq5re5tDIsfplk8rYxdNtbjc0QHRFM9qnqe17ed21bgPdf3xVQUCIhZhuV8e5JOptK+dgAAdTZtZE+DhprmW6SsLOxZeTusp/z8M0GNG1FmiPd6BAG8N6w57/nfHy0hhMaihtTh/DTtTo+6liLbKYXwvVeueXr8ynXbIS2ZYC+kaLtpGjQYANU7eiQ1IYqjEW9/yrRnHyU7LdXrcxtMJqzXrd6PqlCJez/43Ou5XK9emXp8M/gLt2JYbJ598D9/THviZUWN3/G7os2svrMY/OdgTWI93OhhHmr6kCaxtLbk6BKPz9GvZj8A7KqdAxcPEGAIoEpYFUINTh7Fp+Q8XSofcbUyrdfr0TdogG3HDs3yLYyuTBns588XeP30iy95vWgjhCgdgutHU/C7j5tUFefP5BNCaOKVfD6YvBKRu3ATFFV4DLsFNk33y6LNu0P75vr6yR//zHecqqpYs7MxmEwoHlr1LkqX8JgYHvm/7/nk/uFkpWrfF64wY6b/6tX5vC3CqOeSxTPtIUKMOinY+Cm/K9rUja7LuuHrGL9yPOtPrSfFnIItvyWpDvh026d+WbSx2T3fh0WPnoqhFVl2dBnPr3ieTGsmAOGmcC5kXyji7rwUnYVasbmLPXV++Zmk+Pr/fejwjFp/LcVQoQK7GjX22BxCCM/LtGay8OBClh9bzqn0U1QKq8Tw+OE0jW3q69R8xlglFEUvH5CE8DuvRMDofVC2LKSdKXr87vmez8lJ1xdsLr92uXCT3/WAkBBa3TyElv0HoVy/9FoIF4z+aibLpn3F5gV/oNrtxMbV4PzxY1jNrrdYKExgaJhH4voLVVWpa1FZ56H48x7v4KHIwl1+V7QBCDIE8W7nnD1+c/fPZdzKcS7H+nLrl4xsPFKr1DTx2FLP99ixYWPiuol5Xne2YKOqYLnUgFmjuuZ7vdbKFexr50In8ZAQSE8vcljW0aOEVayIEmBCtfjnKWBCiIIlZybz3IrnWH1yda7XtydvZ/nR5TzT8hmG1C2dq+SiBtT2dQpClF75rbK51ie1clbcHP636FjmVDi9E8r5x2lw+RVkHLmWnZ7O6l++B6DVzdqseheiy1330+Wu+698/flDI0g7r33RRlEUhk8s2f2ltm48wWY8c4DBK/3qUi3ayd0Ywmv8/hFfn5p9GNnQ9aLL1B1TNcxGG/+c+MfXKThEVUG1BvDvfZ/SolpsvmOM0dF5G+E4woGCDcCxe+9DVaVRpxDF0azds+j8U+c8BZvLsmxZ/G/N/66sBPRnlSdp+/RJHxOIqZJ8cySEX3utIix60bGxG/7Ps7l4iTU7m3WzZ6F66ERXIVrdrP2DmqqNmjJm5mwiy5Xsprnbdyej1zimSQ8HJvbi7na+OfVr85ELPDxzEwM++Ze3F+4iOc0zq7CKO79caXO9x5o/xpfbv3TpXovdv1Zn3DXvLl+n4DBFgb9vX0BMcJkCx6iqStjgQaTO+rnIeLGvvcqZ8Q5+83OZzcaFOXOwZ8lfYCGKi93nd3P/ovu5mH3RofGtZrYCIHFEogezcl9Q47Jkbj3rdhzFqCOiezUNMhJCeJQ1HVIde8hEVppncymEqqqc3LubjJRLVKhVx+14luwsLNlZmIKCNchOiNya9OzD8d072b3q6kNsRadzuVDYsGtPej7wqFbp+bXWjctjSzzqVoxaMUFMGJBAgF5Hi+rRGmXmmjlbjvPsL4lkWW2oKuw8cYkf1x9l3pgOxIb55pQvf1UsijYATco2YcvZLU7f16Gyf+3N23x2s69TcEioMZSve35NTHBMvtcv/PYbp156GZzYsuR0weY/p18Y79G+OUII7Ww9u5W759+NVXX+iOyEqQl+XbiJ7B3nftFGrxDevRrBjfNfvSiE8JJXLhW9RcoZlXzTnyvl7Bl+eOVZ0s4na7YyOSAkFGNgkAaRhMhLURT6jnmGLiNGsnfdasLLxlK9SXPeu62fQ/c36NKT7vc9iDkri6Cwkt3D5nq1G8bSWW9isc1c9OB81IkNYdHYztom5SKrzc6Lv28n85qmymabyqVMC58s28+E/g18mJ3/KTZFm2m9ptFoWiOn7jEqRp5v/TwLDy3kmeXPYM9nD6ARI+GB4UxqN4kbKt+gVbr52nJ6i0fju6tKSBV+6vcTQcYg9LqCF9+d/fJLzr3rxT2jVivIaQZCFAsT1050qWBTHOgjAgluHkvGRgcak15HCdAT2rES4Z2qyDHfQpQ4CoT6phA769UXSD13tZisxSOuFv0GSiNi4XEhkVE06dH7ytdVGjTi6I5tRd53472j0BuNBBmNnkzPb306vhO9Jyxhr4N/2+9oXYVu9cvTua5/PSw6lJyONZ/jyy02lb93nwGkaHOtYlO0ceQfjxBjCGabGZPORJeqXXi25bOcyzzHU8ufKvAeCxaSs5IZuXQkEaYIVg5bqWXauYxc5F8Nka81q98s6pWpV+Q4e1aWdws2/wlp1470FSu8Pq8QwjHJmcmMWzGOnck73Ypz4cIFoqKKOGLXh8oMqUtg/WjOT09yaLzWvXCEEC44shZWvgvJ+6FSC6jTC2xZ8NQ+eEeDPg6KDqp69sFffs6fOM7FM6c0jakoCukXzmsaUwhHDB7/Gu8P61/omGZ9BmA0BXgpI/9kDDJx/8AGPPvr9iLHHnyjt98WYMODjFjs+ReeyoSYvJyN/ys2RRsABQW1gKpi24pt+azbZ3n+YA6e43j3+0vmS9w9/26+7fWtO2nmy2K1kGXP0jyuu8Y1H8fwhsMdHn/qnXcLH3D591/L7UwmE9Y03+0VF0IUTlVVRi4eycGLB92O5c8Fm8uCG8RgeqE1579Lwnw49b+u7XnHRQyo7v3khBC57fwdfh0F1v++B0veB9t+0HaOgDAIr6htTAeYMzM03z6e0x9nl6YxhXCETqdjzLRf+OKRe8lMuZT7oqLQZ8wz1GsjD0IAbm1ZlXG/bi9wrU3lqECWP9XFbws2ALFhgbSMi2LdwfNYrllxE2TSM6pDDR9m5p+KVdFmaJ2h/LAn/39oP+j8Qb5/MM9kOreMfeOZjSRMTWBa92k0rajd/uRvd36rWSx3BeoC+ePmPygfXt7pey99/33hA1Q1ZyuTlt9EmM1kby6iF5BsnxLCZ7af286x1GMldltUfgxhJmIfaAzkfMhRFIXU1FSy5x4joH4EYY28/wFOCHEdux3+fPJqwcZTsi7C6R1QzrvL+ctWi9M8pqLTEVWxsuZxhXCEISCAh7+c6es0/J6iKOyY0JPO7/zNmdScw1oUYGTHOEZ3qU1EUPFYqfLxsGbcP20DO05cwqjXYbbaebBjDXollOxTwFxRrIo2z93wHCtPrORY2rFcrz/d8mmCjPk3TDPpTGTbnT956K7Fd1E2sCx/Df3LpVyvdypd2+WrzooLjWN67+lEBkW6HMOWkQE2W9EDfXBMZFjPnl6fUwiR41TGKXSK+4VTf25CXJjLDwzCwsIIuy3ex9kIIa5IO51TUPGGBc/BiDnemes/eoOReu07s2vl39rFNBpp2W+gZvGEEJ4RHGBg3QvdfJ2GW6JCTPzyUFsOnkvnTEoW9SqEExFUOnsVFaVYFW10io55A+ex4vgKftnzC9GB0dzf6H4qhhb8RDPUGEp2tmvHRZ/NOkvC1ATKBZfjj/5/EBTgeif9nnE9+WnPTy7f76quVbryYdcPXb5fVVUyt2whZe48LGecb77pNZUr+ToDIUqt+tH1Sbc4eCzuNQbUHMCr7V/1QEZCCAEEhnvv9Mnk/d6Z5zq9R49Fp9Ox8x/3HzJGlq9I95GPEFOlmgaZCSGEY6rHhFA9JsTXafi1YlW0gZwnmh0rd6Rj5Y4OjU/OTnZ7ztMZp2n1Qyv06Fk8ZDFlg8s6HaNVhVZEB0Rrko+jXmj1ArfF3+by/ebjxzlwxx2oJ51fJRR5xx1cnDHD5bmdlTp1Gjz5pNfmE0JcFalEunRf5yqdNc1DCCFyMYVAjY6wX5tV04VKOeaTLVKKTkev0WPJSk/jwMZ1LscJDAvn3g8+9+seGEIIUVqV6EYgWm9JsmGj66yuDPtjGMmZyZhtZuxObAWKCYrRNJ/CfNf7O7cKNpfmzmP/jd1cKtgAxIwahb688z1zXGY2e28uIUQuj6943LX7/n6chKkJV34IIYTmbp0BgZHemWtKW7D5prdX6wFDihwz4p1PCrzW9e5RUrARQgg/VaKLNjoP/fK2n99O558603xGcxpPb0zb79qSkp1S4HiLzUKLaS3YfXG3R/K53uTOk0ko69wHIFVVMR86hPnoUS7MmcMJN1atRD/0IMbYslR4c5LLMZylryFdxoXwhQxLBjvO7tAklhRuhBCaCwiBsTshurZ35vv9Ue/Mc52KdeJp2KVHoddjqlSj1yNPXj3p8z+1W7ejXrtOnk5RCCGEi4rd9ihnxIbEemWeVEsqXX/qyvo71ud5SvHMsmeYf2S+V/IwYGDziCJOWbqOaldZ9dUqEtenEHU+ifhdM9DbMh0qd8XvSsJut3Pq9YlkrPqXgNq1KTtmDOkr/2VPu/bYUlNd+4W4oNac3702lxDiqmk7p5FmT/N1GkIIUTBTCDy8Fl6NATx8WMKuOcAUz85RgJ4PPkZsjZos+/oz1Gt6+ZSvVYcBz74EQP0OXajdsg2Jfy0kKz2dum07El1JTosSQgh/VqKLNgDTe03nzvl3enyebHs2Y5eNZd+lfRxOOYxe0WNX7dg9/c3Bf55s+iR3N7rb6fuWfLGZPZuzCM66QIOkb9HbLY7dqMspTul0Oiq+OP7KywdvHUrWtm1O5+EunaHE/1EWwi/N3DkTFe0afSZMTSi2p0gJIfyYXg/PHoK3a4Kj3+u4QoOT9NzRtEcfmvboQ2ZaGslHDxNZrjyhZaJzjTEGBtKs980+ylAIIYSzSvwn3SaxTehRtQeLjizy+FxLji658nOr6p09zdN6TaNpbFOX7s1Ks7BnywVQFMqfWsWeWkM4Xa4lqqKn5v7fqHJ8GQXtbo59+eU8r2UmJfmkYINRjoYTwhfMNjOXzJd8nYYQQjgmKALGHYOJ5Tw3R8JQz8V2QlBoKJXjvdsUWQghhGeU6J42l73R4Q0iAyJ9nYbm3m7/tssFG4Dzp64e0XuyQntOlW+FXW9C1ek5ENcHizEs3+fnutq1iR6a95uSc5986nIu7ghs0sQn8wpR2lntVmlcKYQoXkyBoA/wXPxeb3outhBCiFKpVBRtTAYTK25bwbgW43ydiiYUFFbfupqbat7kVpzw6MArP88KLIOqu7pixW4MYmPjx8kKKJPrnpD+/aj7x5z8A/poxUvlLz73ybxClHbBxmDCTeGaxpStUUIIj2vkwdUwer3nYgshhCiVSkXR5rLhDYbzWtvXfJ2GSwyKgYcbP8z3vb9n24hthAaFuh0zNCqQmMqhoKqoSt5vMjJDy3Oo2tXCUKWPJlP1rbcKjFf2ybFu5+QKY1CQT+YVQkD18OqaxZKCjRDCK/p9CJVaOD6+ehfHxhnd/95MCCGEuF6pKtoA3Fz7Ztbfvt5jx4F7SpgpjIeaPETDsg01jXvL080JCM7/90JnyyY07TiQc1JUePfuhcYKrOzC6QOytUKIYq1fzX4u3Tei/ggSRyTm+nHZtaeeCCGE5nQ6GLkUHtsCNToXPja6Ntz1G4RXLTru8FlaZCeEEELkUuIbEecn0BjI+uHraT6zua9TcdizrZ71SFxTgIF73+nElNF/g6peLaLYbSiqjWjjBeJ3JTkcT1+5MrZjx4oYpCeoZQsyN24Ci5snOMipUUL41KDag5i4dqLTzden75zOo80eZcnhJey/uB8dOuYdmMfx9OPYsRMdGM2TLZ50uSgkhBBFKlMd7vo97+vHNsGpLVC7B0T890DqkbXw1Y1wZmfe8RHVYPhPEFvPo+kKIYQonUrtJ16TwcSm4ZtoNrOZr1MpUkJ0An1q9PFYfJ1ex73vtufnSRtIOZMJgEnNosfoZlRr2tepWHWWLCapXnyB15WIcOqtXYvl7Fn2dejoVt4A1QrqryOE8AqdTsf/2v2P51c+79R9dux0/rEzaZa0fK8nZyXz8qqXUVDoW9O59yEhhHBL5WY5P65lCoaHV0N6MqSegKg4CAjzSXpCCCFKl1JbtAEwGoxsvXMrN/58I+cyz/k6nXyNaTKG+xvf7/F5gkJM3PlqWyBna4JbJ8IYDGDN/6m7mpFTFLKc0+D322QiuLp2/TSEEK7pV7Mf765/l+TsZKfuK6hgc5nFbmHy5slStBFC+I+Q6JwfQohSLfFsInsu7KFtxbYEG4M5l3mOiqEVCTJIr02hvVJdtIGcp8TLbl3GN4nf8N6m93ydzhUDagzg1Q6v+mRut4/wNRoLLNpc3g6VtX+/e3MAUXfd6XYMIYQ2nmjxBOP/Ha953FPppzSPKYQQQgjhrD/3/8n7G9/nTOaZfK8bFAND6w7l2VbPuv95ysc+2PABU3dOxabaiAuL48seX1IutJyv0yq1Sn3R5rJ7Eu6hS9Uu9Jvt2/4JfeP68kanN3yag7sUoxE1M7PQMafHPef2PLGPP+52DCGENnrE9eDFf19ERdsmwmWDymoaTwghhBDCGVvObOHxZY+TnFX4imKramXmrpn8sf8PPur6Ec3K+38bjvx0+6kbpzNPX/n6YOpBuv3S7crXtSJq8evNvxb7wlRxUryOUPKwuIg4EkckEqAL8PrcCgrf9Pim2BdsANT09EKvZ1+8WPBKHCfopAmxEH4jyBBE/5r9NY/7ZIsnNY8phBBCCOGIvef3ctf8u4os2FwrxZLCiIUjGDB7AEdTjnowO+0NnD0wV8EmP/su7aPptKZeykiAFG3yteHODUy4YQKhhlAC9YH0qd6HzXdsxqDxwqRW5VpdOep224httKjQQtP4PmOzFXr5wA1tvJSIEMKbxt8wHgXtnro81fwpetforVk8IYQQQghHHUk5wqA/Brm8inj/pf3cMf8OsqxZGmemvb6/9CVhagJ7L+11aLwNG7/u+dXDWYnLZKlCAQbWHcjAugNzvbZ5xGYWHljIy6teJt1W+GoSR3x848dux/BHSrlyqKcLr9C6y9SggUfjCyGcZ9QZMWLEjNnlGDp0TGg7gf61+qNT5LmCEEIIIbxPVVXuWXCP29u+L2ZdZMmRJfSt4b+HKiRMTXDpvo+3fMzAOgOLHijcJt8RO6lnjZ6suWMN73Z81604f9z8B0HGktldPG7mDI/PUfmdtz0+hxDCOdN2TnOrYNMgugGLhyxmQO0BUrARQgghhM+8s/6dAhsOO8OOnUMXD7mfkAccuXDE5YINQFRAlIbZiMLIShsX9ajeg21x2/hp10/8e+JfUCHpfBJ6RY+iU0hOTyZTzduMV4eOBYMWUCG0gg+y9o7AypUJ6tCBzBUrPBO/eTMC5KhvIfzOimPO/Z2vX6Y+P/b70UPZCCGEEEI4L8WcwrSkaZrFW3xkMY80e0SzeFr4bc9vvLT6JbdiTGw/UaNsRFGkaOMGRVEYGj+UofFD870+a/csXlvzGnbsGBUjX3b/kuYVmns5S9+I+/ILDo5+hKylS7UJqCjoIiKIefghytwpR30L4Y/Kh5R3avzIRiM9lIkQQgghhGteX/O6pvEOXDqAxWbBqDdqGtdV7214j292fON2HGeaMwv3SNHGg4bUHcKQukN8nYZP2DMytCvYAHU3bUQXVDK3kwlRUjzQ6AH+PPCnw/u/u1bp6uGMhBBCCCGccy7znOYxZyTN4J6G92ge11kWi0WTgg1A1fCqmsQRRZOmAUIzttRUUv/9F8vJk6T9o+3WqOy9jnUyF0L4TrWIakxoOwG9or/ymvLff9eb0GYCOp38EySEEEII/3Jb3ds0j/nR5o9Yf2q95nGdcTz1OM2+a6ZJrGZlm1E5rLImsUTRZKWN0MTebt2xHjt29QVFu2N/AWxG/1hOKIQo3C21b6FnXE82nt6ISWeiefnm6NAxZdsUlh5eSpWwKrzU5iWig6J9naoQQgghRB6T1k7SPKbFbuH9je/zXZ/vNI9dlKMpRxm9ZDQHUw9qFvOLnl9oFksUTYo2wm37Bw3OXbABUN07Hu96RwcPIX7Hdk1jCiE8I9gYTIfKHXK9NrrJaEY3Ge2jjIQQQgghHHMmy/1To/Kz/dx2sm3ZBOgD8r2uqioXsi4QGRip2Smau5N3M/jPwZrEumxY3WEF/hqEZ0jRRrjNvGOH5yex2Uhft46QVq08P5cQQgghhBBCaEhFpcWMFhh1RobWGcqzrZ8FYPqO6by14a1cY8sGlWXRoEUY9K59XM8wZ/D0iqf559g/bud9rdblW/P8Dc9rGlMUTYo2wmW2tDTOT9PuOLyiZGzaLEUbIYQQQgghRLFlsVuYsWsG3+/6HoPOQLY9O8+Ys5ln6f5zd5YNXeZ0/NTsVDr82AGbanM710G1BvFI00c4nnac+DLxmAwmt2MK50nRRrjk7Ndfc+6tt706Z/hNPb06nxBCCCGEEKJ0SYhOIDE50ePz2LBhsxdcWDmXdQ6zzYxJ71yh5OHFD7tdsIkNiGX2LbMJCwgDICY4xq14wj1ydIdw2smPPvZ6wQadjoC4OO/OKYQQQgghhChVJnXUvhGxq46nHXdq/Im0E2xJ3uLWnAbFwNLbll4p2Ajfk5U2wmEHBg8he7tvmgHXXr/OJ/MKIYQQQgghSo9wU7ivU7jilz2/MLTuUKqEV3Fo/BN/PeHWfNEB0S5tyRKeJUUbUSRVVdmV0AisVp/MX2v1KgwhIT6ZWwghhBBCCFF6BBoCfZ3CFVN3TmXqzqncFHcTb3cqeqfDzgs7XZ4rcYTnt4QJ18j2KFEoq9XKrgYNfVawqb12DcaoKJ/MLYQQQgghhChdAg2BGBT/Wtuw4NAC5h+cX+Q4I0aX4q8bJrsa/JkUbUSB7HY7exsmgN3uk/mrzfoJQ0SET+YWQgghhBBClE6z+s3SPGazss14qtlT6BW9S/d/ue3LIseMajLKqZitY1uTOCKRIFOQSzkJ75CijSjQoTvu9NncZV9+ieCEBJ/NL4QQQgghhCidakXVyilm6LUrZpQJKsOIhBFsvnMzdSLrOH3/oUuHaDK1CQlTE0iYmkCTqU1YenhprjEjGowgRFd0W4nYwFgSRyTyVa+vnM5DeJ8UbcQV2QcPkvrvKo6MeZykevFkb9rkkzwihg8nZtgwn8wthBBCCCGEEADLhy4nxJC3CPJow0edjrXkyBISpiZw1/y7+LL7l0QHRjt1v0W1YOPqUd42bDz+9+McTT2K2WImYWoCrWa2It2eXmicATUGsHTo0kLHCP/iX5v1hMdZ09I4+sRYstatBYOR6PvuI6J/Pw7fOhTb+fO+To/YyR8S3aOHr9MQQgghhBBClHJBxiBWDVvFb/t/Y+nhpZQLLsfoJqOJCY7hqx1fkalmOh1zy9ktdJrVSbMc7/7zbs6Yzzg8fvaB2aiovNbhNc1yEJ4lRRs/p9rtZO/Zg6LXY6pVC9vFi6gWC4ayZVEUpfB7VZXkr7/m7IeTwWzOOyDbTPLkySRPnuyh7PNX8++/ufDzz1z46ivUrCwAdLFlqfH779J0WAghhBBCCOE3dDodg2oPYlDtQbleb1elHUuOLPFRVlc5U7C57PcDv0vRphiRoo0fy1i/nmNjHseWkgI2G6hqzgVFwRAbS+XJHxLUuHGe+9I3bOCID/vRFCbyvnsxlS9HuUdGU+6R0b5ORwghhBBCCCGc1rN6T5YeWYqK6utURAknPW38lPXcOY6MHJWzZclqvVqwAVBVrKdPc+j24VhO566sXlq7zo8LNvdR4emnfZ2GEEIIIYQQQrilW9Vu3FDhBl+nIUoBKdr4qUt//Hll61CBbDaOPfYYZyZ/xJ72HUhq0JATI0Z4J0EXXJw61dcpCCGEEEIIIYTbDDoDU7pN4b2O7/k6FacF64N9nYJwghRt/FTW7t2Ojdu6leRPP8V27lzOFip/ZrX6OgMhhBBCCCGE0IRep6d79e483bx47Sb497Z/fZ2CcIIUbfyQLSODlDlzfJ2G9nTyx00IIYQQQghRstzV8C7GNhrr6zRyqRhQMc9rYfowEkckYjBIa9viRP5v+aHDw+8Au93XaWiuzP33+ToFIYQQQgghhNDcPU3vYf7R+SRdSPJ1KkxqO4k+tfv4Og2hEVn64GdUVSU7yfd/0TUXGUm5sf5VfRZCCCGEEEIIrfzU/ycWDVjk6zSkYFPCSNHG3/h7XxoXhPTuTfya1b5OQwjhokxrJiuPr2TNyTVYbBZfpyOEKE3M6bB3CRz4G+T9RwhRDFSIqMC33b/12fzjWozz2dzCM2R7lJ9RDAZ0oaHY09J8nYrblJgY6vyzHJ30shGi2FpyeAnPr3wevaJHRUWn6JjcZTItyrfwdWpCiJJu+2/w+8Og04MK6A0w7AeoKkfsCiH8W/OKzVlx6wr6/daPi5aLXp17eIPhXp1PeJ58mvZDFd9+y9cpuE6nI/Khh4jflUS9lSukYCNEMXYy7STjVowj05pJmiWNdEs6qeZURi8dTbol3dfpCSFKsvMHYfaDYMmA7FQwp0LmBZgxKGf1jRBC+LnIoEhW3L6CBQMXeG3OxBGJXptLeI98ovZDYV26EDfrJ4xVq/g6FYzVq1N56rfoK5QHRcl5Ua9HiY0loFEjKkyeTN2dO4jflZTzY+cOKox5zLdJCyE08eeBP7Gr+TdF/+vIX17ORghRqmz9HuwFbBnfPd+7uQghhBsqhVUicUQiHSp08Ej8qiFVWXPbGinYlGCyPcpPBSUkUGtRThOrpJatIDW14MEGA1itms5vrFGdCq+9RkizZgCELVumaXwhhP9LNadiseftIWFTbaSaC3lPEkIId2VdhHzef7DbIDvF6+kIIYS7Pun+Cd1/7s7pjNMux3i62dPclXCXhlmJ4kCKNsVA/Pp1JMXXB1XN93rkXXcRPfRWzs+YifnQIYJbtiRq6K3oIyLIPHiQ5A8+REUlY+s27JcuoQsOJvTGG6nw4nh0RqOXfzVCiOKiXaV2/LD7BzKtmXmutanYxgcZCSFKjdo9YNMMyLMVU4UanX2RkRBCuEVRFJ5r9RzjVowjy5bl9P3RgdFSsCmlpGhTTNTbuYNdDRqC/bqtCiYT5Z9+CkVRKP/C83nuC6pencoffuCdJIUQJUqr8q1oU6ENq0+uvlK4CTIEMaDWAKpHVPdxdkKIEq1GV4hrD4dWXi3cGIOhxT1QpoZvcxNCCBfdWO1GpgRM4YttX3Ak9QgNohvwUOOHWHd8HZ9t/wyTzsQttW7hs8TP8tz7Roc3fJCx8AdStCkmFEWh3o7tnHz9dS79+BMAkbfeSvnnn0O53GtGCCE0pCgK73d5nyWHl/DngT8x6ozcUvsW2lVs5+vUhBAlnU4Hw76Hnb9D4iwwBEDTO6FmV19nJoQQbmlRvkWeUzhrRdXi9oa3X/m6TaU2vLX+LY6nHadmRE2eb/08dcrU8Xaqwk9I0aYYURSFiuPHU3H8eF+nIoQoJXSKjh5xPegR18PXqQghShudHhoOzPkhhBClSLNyzfih7w++TkP4CTk9SgghhBBCCCGEEMIPSdFGCCGEEEIIIYQQwg9J0UYIIYQQQgghhBDCD0nRRgghhBBCCCGEEMIPSdFGCCGEEEIIIYQQwg9J0UYIIYQQQgghhBDCDymqqjo+WFHOAoc9l44Qwguqqapa1tdJuEPei4QoEeS9SAjhL4r1+5G8FwlRYuT7XuRU0UYIIYQQQgghhBBCeIdsjxJCCCGEEEIIIYTwQ1K0EUIIIYQQQgghhPBDUrQRQgghhBBCCCGE8ENStBFCCCGEEEIIIYTwQ1K0EUIIIYQQQgghhPBDUrQRQgghhBBCCCGE8ENStBFCCCGEEEIIIYTwQ1K0EUIIIYQQQgghhPBDUrQRQgghhBBCCCGE8EP/DxvpXfo7p7rqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x324 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [20,4.5])\n",
    "\n",
    "for tsne_idx in range(4):\n",
    "    model.set_weights(weight_list[tsne_idx])\n",
    "    y_h = model.predict(test_x, batch_size=evaluation_batch_size, verbose=0)\n",
    "    predictions = np.argmax(y_h, axis = 1)\n",
    "    ts = TSNE(init= 'pca', learning_rate = 'auto')\n",
    "    results = ts.fit_transform(y_h)\n",
    "    x_min, x_max = np.min(results[:,0]), np.max(results[:,0])\n",
    "    y_min, y_max = np.min(results[:,1]), np.max(results[:,1])\n",
    "    x = (results[:,0] - x_min) / (x_max - x_min)\n",
    "    y = (results[:,1] - y_min) / (y_max - y_min)\n",
    "    plt.subplot(1,4,tsne_idx+1)  \n",
    "    plt.scatter(x, y, c = ['C'+str(p) for p in predictions])\n",
    "    plt.xticks([])        \n",
    "    plt.yticks([])\n",
    "    plt.title(experiment_keywords[tsne_idx])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFA10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "xxx",
   "language": "python",
   "name": "xxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
