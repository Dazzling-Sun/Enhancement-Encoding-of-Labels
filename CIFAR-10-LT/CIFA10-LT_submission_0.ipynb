{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6eHm7W0Kc4Fu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 11:34:57.092046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "# from tensorflow.python.keras.engine import data_adapter\n",
    "# from tensorflow.python.eager import backprop\n",
    "from functools import partial\n",
    "# from tensorflow.python.keras.engine.training import _minimize\n",
    "# import tensorflow_addons as tfa\n",
    "import math\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Uz7ssaYixpuN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 11:34:59.108513: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-11 11:34:59.109476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-05-11 11:34:59.198199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:48:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\n",
      "2022-05-11 11:34:59.198234: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-11 11:34:59.200232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-11 11:34:59.200282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-11 11:34:59.202170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-11 11:34:59.202448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-11 11:34:59.204314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-11 11:34:59.205340: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-11 11:34:59.209364: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-11 11:34:59.214002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lup2j6RUelce"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 11:34:59.227024: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function decode at 0x7f6f7f466af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function decode at 0x7f6f7f466af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 11:34:59.238219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:48:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\n",
      "2022-05-11 11:34:59.238263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-11 11:34:59.238298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-11 11:34:59.238308: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-05-11 11:34:59.238319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-11 11:34:59.238329: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-11 11:34:59.238339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-11 11:34:59.238349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-11 11:34:59.238359: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-05-11 11:34:59.244570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-11 11:34:59.244652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-11 11:34:59.970967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-11 11:34:59.971001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-05-11 11:34:59.971007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-05-11 11:34:59.981228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30130 MB memory) -> physical GPU (device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:48:00.0, compute capability: 7.0)\n",
      "2022-05-11 11:34:59.981505: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-11 11:35:00.032499: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-05-11 11:35:00.033003: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2095170000 Hz\n"
     ]
    }
   ],
   "source": [
    "n_class = 10\n",
    "train_set_size = 12406\n",
    "test_set_size = 10000\n",
    "valid_size_per_class = 5\n",
    "\n",
    "valid_set_size = valid_size_per_class * n_class\n",
    "\n",
    "\n",
    "\n",
    "def decode(instance):\n",
    "    feature_spec = {\n",
    "    'image/encoded': tf.io.FixedLenFeature((), tf.string),\n",
    "    'image/class/label': tf.io.FixedLenFeature((), tf.int64)\n",
    "    }\n",
    "    instance = tf.io.parse_example(instance, feature_spec)\n",
    "    image = tf.io.decode_raw(instance[\"image/encoded\"], tf.uint8)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [32, 32, 3])\n",
    "    label = instance[\"image/class/label\"]\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "traing_dateset = tf.data.TFRecordDataset(\"./data/cifar10-lt_train.tfrecord\").map(decode)\n",
    "\n",
    "train_x, train_y = np.zeros([train_set_size, 32, 32, 3]), np.zeros([train_set_size])\n",
    "for i,instance in enumerate(traing_dateset.as_numpy_iterator()):\n",
    "    train_x[i] = instance[0]\n",
    "    train_y[i] = instance[1]\n",
    "\n",
    "valid_x, valid_y = np.zeros([valid_set_size,32,32,3]), np.zeros([valid_set_size])\n",
    "\n",
    "train_set_size_no_valid = train_set_size - valid_set_size\n",
    "train_x_no_valid = train_x\n",
    "train_y_no_valid = train_y\n",
    "\n",
    "train_size_no_valid_per_class = np.zeros([n_class])\n",
    "\n",
    "for i in range(n_class):\n",
    "    indices_of_instances = np.where(train_y_no_valid == i)[0]\n",
    "    train_size_no_valid_per_class[i] = len(indices_of_instances) - valid_size_per_class\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(indices_of_instances)\n",
    "    \n",
    "    indices_of_valid_instances = indices_of_instances[ : valid_size_per_class]\n",
    "    valid_x[i * valid_size_per_class : (i+1) * valid_size_per_class] = \\\n",
    "        train_x_no_valid[indices_of_valid_instances]\n",
    "    \n",
    "    valid_y[i * valid_size_per_class : (i+1) * valid_size_per_class] = \\\n",
    "        train_y_no_valid[indices_of_valid_instances]\n",
    "    \n",
    "    train_x_no_valid = np.delete(train_x_no_valid, indices_of_valid_instances, 0)\n",
    "    train_y_no_valid = np.delete(train_y_no_valid, indices_of_valid_instances, 0)\n",
    "\n",
    "    \n",
    "test_dateset = tf.data.TFRecordDataset(\"./data/cifar10_test.tfrecord\").map(decode)\n",
    "\n",
    "test_x, test_y = np.zeros([test_set_size, 32, 32, 3]), np.zeros([test_set_size])\n",
    "for i,instance in enumerate(test_dateset.as_numpy_iterator()):\n",
    "    test_x[i] = instance[0]\n",
    "    test_y[i] = instance[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 303661208\n",
      "test set size: 245760152\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "print('training set size:',train_x_no_valid.__sizeof__())\n",
    "print('test set size:',test_x.__sizeof__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q5nA5eZEfHXn"
   },
   "outputs": [],
   "source": [
    "normalize_x = lambda x: x/255 - 0.5\n",
    "\n",
    "train_x_no_valid = tf.constant(normalize_x(train_x_no_valid))\n",
    "test_x = tf.constant(normalize_x(test_x))\n",
    "valid_x = tf.constant(normalize_x(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qqabQ0IpfIlM"
   },
   "outputs": [],
   "source": [
    "train_y_no_valid = tf.constant(keras.utils.to_categorical(train_y_no_valid, n_class))\n",
    "test_y = tf.constant(keras.utils.to_categorical(test_y, n_class))\n",
    "valid_y = tf.constant(keras.utils.to_categorical(valid_y, n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(train_size_no_valid_per_class)), train_size_no_valid_per_class)\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.xlabel('Sorted Index of Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 4\n",
    "\n",
    "# @trackable.no_automatic_dependency_tracking \n",
    "def train_generator(epochs, batch_size, encoding_matrix):\n",
    "    def preprocess_data(x, y):\n",
    "        x = tf.image.resize_with_crop_or_pad(x, 32 + padding, 32 + padding)\n",
    "        x = tf.image.random_crop(x, [32, 32, 3])\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        y = tf.matmul(train_y_no_valid, encoding_matrix)\n",
    "    \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_x_no_valid, y)\n",
    "        ).map(\n",
    "            preprocess_data\n",
    "        ).shuffle(\n",
    "            train_set_size, epoch\n",
    "        ).batch(\n",
    "            batch_size\n",
    "        ).prefetch(\n",
    "            tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "        for batch in train_dataset:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal loss \n",
    "def focal(gamma=2.):\n",
    "    def focal_loss (y_true, y_pred):\n",
    "        ep = 1e-7\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = y_pred + (1. - 2.*y_pred) * ep\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        return tf.reduce_sum(-y_true*((1-y_pred)**gamma)*tf.math.log(y_pred), axis=1)\n",
    "    \n",
    "    return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_key = 'ce'\n",
    "focal_gamma = 1.\n",
    "loss_fn_dic = {'focal':focal(focal_gamma), \n",
    "               'ce':keras.losses.categorical_crossentropy,\n",
    "               'mse':keras.losses.mean_squared_error}\n",
    "loss_fn = loss_fn_dic[loss_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NizAKxPTxpuQ"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9CiBEzRDxpuQ"
   },
   "outputs": [],
   "source": [
    "evaluation_batch_size = 5000\n",
    "\n",
    "class UpdateEncodingMatrixAndGetMetrics_EEC (keras.callbacks.Callback):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def __init__(self, \n",
    "                 beginEncodingEpoch,endEncodingEpoch, \n",
    "                 codingEnhancementRate, mu, \n",
    "                 trainSet_x, trainSet_y, \n",
    "                 validSet_x, validSet_y,\n",
    "                 trainMetrics, validMetrics,\n",
    "                 keyWord):\n",
    "        super().__init__()\n",
    "        self.beginEncodingEpoch = beginEncodingEpoch\n",
    "        self.endEncodingEpoch = endEncodingEpoch\n",
    "        self.codingEnhancementRate = tf.constant(codingEnhancementRate)\n",
    "        self.trainSet_x = trainSet_x\n",
    "        self.trainSet_y = trainSet_y\n",
    "        self.validSet_x = validSet_x\n",
    "        self.validSet_y = validSet_y\n",
    "        self.trainMetrics = trainMetrics\n",
    "        self.validMetrics = validMetrics\n",
    "        self.mu = mu\n",
    "        self.keyWord = keyWord\n",
    "\n",
    "    \n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _confusion_matrix(self, labels, preds):\n",
    "        pred_classes = tf.one_hot(tf.argmax(preds, axis=1), 10)\n",
    "        cm=tf.matmul(labels, pred_classes, transpose_a=True)\n",
    "        cm=cm/tf.reduce_sum(cm, axis=1, keepdims=True)\n",
    "        return cm\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _soft_confusion_matrix(self, labels, preds):\n",
    "        scm=tf.matmul(labels, preds, transpose_a=True)\n",
    "        scm=scm/tf.reduce_sum(scm, axis=1, keepdims=True)\n",
    "        return scm\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def _make_encoding_matrix(self, confusionMatrix):\n",
    "        coSenMatrix =  confusionMatrix - tf.eye(10)\n",
    "        encodingMatrix = - self.codingEnhancementRate \\\n",
    "                            * coSenMatrix \\\n",
    "                            + tf.eye(10)\n",
    "        \n",
    "        return encodingMatrix\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def on_train_begin(self, logs):\n",
    "        if self.endEncodingEpoch <= 0 or self.beginEncodingEpoch > 0:\n",
    "            encodingMatrix = tf.eye(10)\n",
    "        else:\n",
    "            y_h = tf.constant(self.model.predict(self.validSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "            y = self.validSet_y\n",
    "            scm = self._soft_confusion_matrix(y, y_h)\n",
    "            encodingMatrix = ((1-self.mu) * encoding_matrix +\n",
    "                              self.mu * self._make_encoding_matrix(scm))\n",
    "        K.set_value(encoding_matrix, encodingMatrix)\n",
    "\n",
    "   \n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        print('\\n'+self.keyWord)\n",
    "        \n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        y_h = tf.constant(self.model.predict(self.trainSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.trainSet_y\n",
    "        self.trainMetrics[0][epoch] = self._soft_confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[1][epoch] = self._confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[2][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.trainMetrics[3][epoch] = tf.reduce_mean(keras.metrics.categorical_accuracy(y, y_h)).numpy()\n",
    "        self.trainMetrics[4][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "\n",
    "        print('\\nTest on train set:',\n",
    "              'loss=', self.trainMetrics[2][epoch],\n",
    "              'acc=', self.trainMetrics[3][epoch],\n",
    "              'auc=', self.trainMetrics[4][epoch] )\n",
    "\n",
    "        y_h = tf.constant(self.model.predict(self.validSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.validSet_y\n",
    "        scm = self._soft_confusion_matrix(y, y_h)\n",
    "        self.validMetrics[0][epoch] = scm.numpy()\n",
    "        self.validMetrics[1][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.validMetrics[2][epoch] = scm.numpy().diagonal().mean()\n",
    "        self.validMetrics[3][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "\n",
    "        print('Test on valid set:',\n",
    "            'loss=', self.validMetrics[1][epoch],\n",
    "            'acc=', self.validMetrics[2][epoch],\n",
    "            'auc=', self.validMetrics[3][epoch] )\n",
    "\n",
    "        if epoch+1 < self.endEncodingEpoch and epoch+1>=self.beginEncodingEpoch:\n",
    "            new_encoding_matrix = (1-self.mu) * encoding_matrix + \\\n",
    "                              self.mu * self._make_encoding_matrix(scm)\n",
    "            K.set_value(encoding_matrix, new_encoding_matrix)\n",
    "\n",
    "        elif epoch+1 == self.endEncodingEpoch:\n",
    "            K.set_value(encoding_matrix, tf.eye(10))\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_train_end(self, logs):\n",
    "        K.set_value(encoding_matrix, tf.eye(10))\n",
    "        print('*'*100)\n",
    "        print('*'*100)\n",
    "        print('*'*100)\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "class UpdateEncodingMatrixAndGetMetrics_Base (UpdateEncodingMatrixAndGetMetrics_EEC):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def __init__(self, \n",
    "                 trainSet_x, trainSet_y, \n",
    "                 validSet_x, validSet_y,\n",
    "                 trainMetrics, validMetrics,\n",
    "                 keyWord):\n",
    "        super(UpdateEncodingMatrixAndGetMetrics_EEC, self).__init__()\n",
    "        self.trainSet_x = trainSet_x\n",
    "        self.trainSet_y = trainSet_y\n",
    "        self.validSet_x = validSet_x\n",
    "        self.validSet_y = validSet_y\n",
    "        self.trainMetrics = trainMetrics\n",
    "        self.validMetrics = validMetrics\n",
    "        self.keyWord = keyWord\n",
    "\n",
    "   \n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def _make_encoding_matrix(self):\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def on_train_begin(self, logs):\n",
    "        super(UpdateEncodingMatrixAndGetMetrics_EEC, self).on_train_begin(logs)\n",
    "\n",
    "\n",
    "    @trackable.no_automatic_dependency_tracking\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        y_h = tf.constant(self.model.predict(self.trainSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.trainSet_y\n",
    "        self.trainMetrics[0][epoch] = self._soft_confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[1][epoch] = self._confusion_matrix(y, y_h).numpy()\n",
    "        self.trainMetrics[2][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.trainMetrics[3][epoch] = tf.reduce_mean(keras.metrics.categorical_accuracy(y, y_h)).numpy()\n",
    "        self.trainMetrics[4][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "\n",
    "        print('\\nTest on train set:',\n",
    "            'loss=', self.trainMetrics[2][epoch],\n",
    "            'acc=', self.trainMetrics[3][epoch],\n",
    "            'auc=', self.trainMetrics[4][epoch] )\n",
    "\n",
    "        y_h = tf.constant(self.model.predict(self.validSet_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "        y = self.validSet_y\n",
    "        scm = self._soft_confusion_matrix(y, y_h)\n",
    "        self.validMetrics[0][epoch] = scm.numpy()\n",
    "        self.validMetrics[1][epoch] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "        self.validMetrics[2][epoch] = scm.numpy().diagonal().mean()\n",
    "        self.validMetrics[3][epoch] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "        \n",
    "        print('Test on valid set:',\n",
    "            'loss=', self.validMetrics[1][epoch],\n",
    "            'acc=', self.validMetrics[2][epoch],\n",
    "            'auc=', self.validMetrics[3][epoch] )\n",
    "\n",
    "        print()\n",
    "\n",
    "        \n",
    "        \n",
    "#################################################################################################################        \n",
    "class UpdateEncodingMatrixAndGetMetrics_Reweight (UpdateEncodingMatrixAndGetMetrics_EEC):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _make_encoding_matrix(self, confusionMatrix):\n",
    "        CoSenMatrix =  confusionMatrix - tf.eye(10)\n",
    "        encodingMatrix = - self.codingEnhancementRate \\\n",
    "                            * CoSenMatrix \\\n",
    "                            + tf.eye(10)\n",
    "        encodingMatrix = encodingMatrix * tf.eye(10)\n",
    "        return encodingMatrix        \n",
    "\n",
    "#################################################################################################################        \n",
    "class UpdateEncodingMatrixAndGetMetrics_CoSen (UpdateEncodingMatrixAndGetMetrics_EEC):\n",
    "    @trackable.no_automatic_dependency_tracking \n",
    "    def _make_encoding_matrix(self, confusionMatrix):\n",
    "        CoSenMatrix =  (1 - tf.eye(10))*(confusionMatrix - tf.eye(10))\n",
    "        encodingMatrix = - self.codingEnhancementRate \\\n",
    "                            * CoSenMatrix \\\n",
    "                            + tf.eye(10)\n",
    "        return encodingMatrix     \n",
    "    \n",
    "##################################################################################################################\n",
    "class AdjustLR (keras.callbacks.Callback):\n",
    "    def __init__(self, schedule, base_learning_rate = None):\n",
    "        super().__init__()\n",
    "        self._schedule = schedule\n",
    "        self._base_learning_rate = base_learning_rate\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        warmup_lr_multiplier, warmup_end_epoch = self._schedule[0]\n",
    "        learning_rate = (\n",
    "            self._base_learning_rate * warmup_lr_multiplier * (epoch+1) /\n",
    "            (warmup_end_epoch+1))\n",
    "        for mult, start_epoch in self._schedule:\n",
    "            learning_rate = tf.where(epoch >= start_epoch,\n",
    "                                   self._base_learning_rate * mult, learning_rate)\n",
    "\n",
    "        K.set_value(self.model.optimizer.learning_rate, learning_rate)\n",
    "        print(self.model.optimizer.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu-nz9Bm3695"
   },
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d (ZeroPadding2 (None, 38, 38, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 36, 36, 16)        432       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 36, 36, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 36, 36, 16)        0         \n",
      "_________________________________________________________________\n",
      "residual_unit (ResidualUnit) (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_1 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_2 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_3 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_4 (ResidualUni (None, 36, 36, 16)        3008      \n",
      "_________________________________________________________________\n",
      "residual_unit_5 (ResidualUni (None, 18, 18, 32)        11776     \n",
      "_________________________________________________________________\n",
      "residual_unit_6 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_7 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_8 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_9 (ResidualUni (None, 18, 18, 32)        11648     \n",
      "_________________________________________________________________\n",
      "residual_unit_10 (ResidualUn (None, 9, 9, 64)          46080     \n",
      "_________________________________________________________________\n",
      "residual_unit_11 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "residual_unit_12 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "residual_unit_13 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "residual_unit_14 (ResidualUn (None, 9, 9, 64)          45824     \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 304,226\n",
      "Trainable params: 300,494\n",
      "Non-trainable params: 3,732\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "he_normal = keras.initializers.HeUniform()\n",
    "glorot_normal = keras.initializers.GlorotNormal()\n",
    "he_uniform = keras.initializers.HeUniform()\n",
    "glorot_uniform = keras.initializers.GlorotUniform()\n",
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"SAME\", use_bias=False, \n",
    "                        kernel_initializer=he_normal)\n",
    "DefaultBN = partial(keras.layers.BatchNormalization, \n",
    "                      momentum=0.9,\n",
    "                      epsilon=1e-5)\n",
    "\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters,kernel_size=1,strides=strides),\n",
    "            DefaultBN(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            DefaultBN(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters,kernel_size=1),\n",
    "            DefaultBN()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                DefaultBN()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Input(shape=(32, 32, 3)))\n",
    "model.add(keras.layers.ZeroPadding2D(padding=(3, 3)))\n",
    "model.add(DefaultConv2D(16,padding='valid'))\n",
    "model.add(DefaultBN())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "prev_filters = 16\n",
    "for filters in [16] * 5 + [32] * 5 + [64] * 5:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model.add(DefaultBN())\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, kernel_initializer=keras.initializers.RandomNormal(stddev=0.01)))\n",
    "model.add(DefaultBN())\n",
    "model.add(keras.layers.Softmax())\n",
    "model.summary()\n",
    "initialWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = lambda learning_rate=0.01:keras.optimizers.SGD(learning_rate = learning_rate, \n",
    "                                                          momentum=0.9,  \n",
    "                                                          nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 11:35:09.513959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-11 11:35:09.785096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 7s 26ms/step - loss: 2.3094\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = Optimizer()\n",
    "model.compile(optimizer=optimizer, loss = loss_fn)\n",
    "model.set_weights(initialWeights)\n",
    "        \n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.prev_loss = 0\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(batch_loss)\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)        \n",
    "        \n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=100):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), 5.0])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    \n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, train_x_no_valid, train_y_no_valid, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-P5P0ZLDxpuW",
    "outputId": "168357d3-92d5-4f6f-eaba-92ef84ecea7c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lrDic = {'focal':0.5,\n",
    "         'ce':0.1,\n",
    "         'mse':5.0\n",
    "        }\n",
    "lr = lrDic[loss_key]\n",
    "codingEnhancementRateDic = {'focal':0.5,\n",
    "                            'ce':1.0,\n",
    "                            'mse':0.5\n",
    "                           }\n",
    "codingEnhancementRate = codingEnhancementRateDic[loss_key]\n",
    "\n",
    "lr_schedule_dic = {'focal':[(1.0, 5), (0.1, 200), (0.01, 300)],\n",
    "                   'ce':[(1.0, 5), (0.1, 200), (0.01, 250)],\n",
    "                   'mse':[(1.0, 5), (0.1, 200), (0.01, 300)]}\n",
    "\n",
    "lr_schedule = lr_schedule_dic[loss_key]\n",
    "\n",
    "mu = 0.1 if loss_key == \"mse\" else 0.5  #update rate\n",
    "\n",
    "epochs_dic = {'focal':400,\n",
    "          'ce':300,\n",
    "          'mse':400} \n",
    "epochs = epochs_dic[loss_key]\n",
    "endEncodingEpoch = epochs\n",
    "beginEncodingEpoch = 0\n",
    "batch_size = 128\n",
    "steps = math.ceil(train_set_size_no_valid / batch_size) \n",
    "print(steps)\n",
    "\n",
    "if 'softConfusionMatrix_train' not in locals() or len(softConfusionMatrix_train[0]) != epochs:\n",
    "    softConfusionMatrix_train = np.zeros((4, epochs, n_class, n_class))\n",
    "    confusionMatrix_train = np.zeros((4, epochs, n_class, n_class))\n",
    "    loss_train=np.zeros((4,epochs))\n",
    "    acc_train=np.zeros((4,epochs))\n",
    "    auc_train=np.zeros((4,epochs))\n",
    "    \n",
    "    softConfusionMatrix_valid = np.zeros((4, epochs, n_class, n_class))\n",
    "    loss_valid=np.zeros((4,epochs))\n",
    "    acc_valid=np.zeros((4,epochs))\n",
    "    auc_valid=np.zeros((4,epochs))\n",
    "\n",
    "model_weights = [] \n",
    "\n",
    "\n",
    "# Pretrain-------------------------------------------------------------------------\n",
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(initialWeights)\n",
    "optimizer = Optimizer(lr)\n",
    "model.compile(optimizer=optimizer, loss = loss_fn)\n",
    "\n",
    "metric_idx = 0\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_Base(train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Pretrain')\n",
    "                        \n",
    "_ = model.fit(trainGen, epochs = beginEncodingEpoch, verbose = 2,\n",
    "                steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "\n",
    "pretrainedWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 5s - loss: 1.5215\n",
      "\n",
      "Test on train set: loss= 1.2747505903244019 acc= 0.5844933390617371 auc= 0.7405386569012873\n",
      "Test on valid set: loss= 2.4610650539398193 acc= 0.20691156387329102 auc= 0.7142222222222221\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 2s - loss: 1.1429\n",
      "\n",
      "Test on train set: loss= 1.0731741189956665 acc= 0.6341049075126648 auc= 0.8154658638562733\n",
      "Test on valid set: loss= 2.460397958755493 acc= 0.2263018637895584 auc= 0.7791111111111111\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 3s - loss: 1.0247\n",
      "\n",
      "Test on train set: loss= 0.9596424102783203 acc= 0.6734380125999451 auc= 0.8605473223943545\n",
      "Test on valid set: loss= 2.476236343383789 acc= 0.23468229174613953 auc= 0.7946666666666665\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 3s - loss: 0.9348\n",
      "\n",
      "Test on train set: loss= 0.8912901878356934 acc= 0.6952087879180908 auc= 0.8828094009231966\n",
      "Test on valid set: loss= 2.3591842651367188 acc= 0.25522080063819885 auc= 0.8186666666666668\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 3s - loss: 0.8718\n",
      "\n",
      "Test on train set: loss= 0.8955844640731812 acc= 0.6994982361793518 auc= 0.8760796372850548\n",
      "Test on valid set: loss= 2.612971782684326 acc= 0.2668382525444031 auc= 0.8017777777777777\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.8302\n",
      "\n",
      "Test on train set: loss= 0.8940712809562683 acc= 0.7137423157691956 auc= 0.8849720979017734\n",
      "Test on valid set: loss= 2.554152727127075 acc= 0.2929179072380066 auc= 0.823111111111111\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.7747\n",
      "\n",
      "Test on train set: loss= 0.7391931414604187 acc= 0.754046618938446 auc= 0.9196053913093213\n",
      "Test on valid set: loss= 2.000196933746338 acc= 0.3448810279369354 auc= 0.8804444444444444\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.7274\n",
      "\n",
      "Test on train set: loss= 0.7145959734916687 acc= 0.7640013098716736 auc= 0.9171885730485568\n",
      "Test on valid set: loss= 2.343125104904175 acc= 0.30855920910835266 auc= 0.8457777777777776\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6870\n",
      "\n",
      "Test on train set: loss= 0.6443000435829163 acc= 0.7780835032463074 auc= 0.9398571209267199\n",
      "Test on valid set: loss= 2.072490930557251 acc= 0.34731581807136536 auc= 0.8777777777777777\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6464\n",
      "\n",
      "Test on train set: loss= 0.6798281669616699 acc= 0.7748462557792664 auc= 0.9429822562570542\n",
      "Test on valid set: loss= 1.9825468063354492 acc= 0.3605049252510071 auc= 0.8933333333333333\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6251\n",
      "\n",
      "Test on train set: loss= 0.5663784146308899 acc= 0.8091615438461304 auc= 0.9565429527347471\n",
      "Test on valid set: loss= 1.6954036951065063 acc= 0.3966687321662903 auc= 0.9013333333333333\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.6003\n",
      "\n",
      "Test on train set: loss= 0.5653565526008606 acc= 0.8093234300613403 auc= 0.9567022494070152\n",
      "Test on valid set: loss= 1.9451733827590942 acc= 0.39233559370040894 auc= 0.8897777777777778\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5595\n",
      "\n",
      "Test on train set: loss= 0.583581268787384 acc= 0.7954030632972717 auc= 0.9605743927320749\n",
      "Test on valid set: loss= 1.8303537368774414 acc= 0.3839547336101532 auc= 0.9111111111111111\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5458\n",
      "\n",
      "Test on train set: loss= 0.5376617312431335 acc= 0.8122369647026062 auc= 0.964131990853188\n",
      "Test on valid set: loss= 1.4153023958206177 acc= 0.46857839822769165 auc= 0.9306666666666666\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5167\n",
      "\n",
      "Test on train set: loss= 0.5170815587043762 acc= 0.8275331854820251 auc= 0.9642876150807671\n",
      "Test on valid set: loss= 1.7071894407272339 acc= 0.44351738691329956 auc= 0.9177777777777777\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.5062\n",
      "\n",
      "Test on train set: loss= 0.5453535914421082 acc= 0.815555214881897 auc= 0.9664675911154706\n",
      "Test on valid set: loss= 1.480466365814209 acc= 0.4608039855957031 auc= 0.928888888888889\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4805\n",
      "\n",
      "Test on train set: loss= 0.5793215036392212 acc= 0.8076238036155701 auc= 0.9674384319639516\n",
      "Test on valid set: loss= 2.5572614669799805 acc= 0.3923141360282898 auc= 0.8977777777777778\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4702\n",
      "\n",
      "Test on train set: loss= 0.48036453127861023 acc= 0.8377306461334229 auc= 0.9704161296118148\n",
      "Test on valid set: loss= 1.7911889553070068 acc= 0.44760385155677795 auc= 0.9191111111111111\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4515\n",
      "\n",
      "Test on train set: loss= 0.49661752581596375 acc= 0.830527663230896 auc= 0.9728033204875366\n",
      "Test on valid set: loss= 1.8240749835968018 acc= 0.4608365595340729 auc= 0.9302222222222223\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4328\n",
      "\n",
      "Test on train set: loss= 0.5764274597167969 acc= 0.8024441599845886 auc= 0.9731893398016505\n",
      "Test on valid set: loss= 1.73085355758667 acc= 0.4980136454105377 auc= 0.9413333333333332\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4140\n",
      "\n",
      "Test on train set: loss= 0.5863402485847473 acc= 0.815312385559082 auc= 0.9729676385546787\n",
      "Test on valid set: loss= 1.495166301727295 acc= 0.5425053834915161 auc= 0.9302222222222222\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.4107\n",
      "\n",
      "Test on train set: loss= 0.417259156703949 acc= 0.8549692630767822 auc= 0.982211552676375\n",
      "Test on valid set: loss= 1.6957310438156128 acc= 0.49946704506874084 auc= 0.9373333333333334\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3901\n",
      "\n",
      "Test on train set: loss= 0.38882938027381897 acc= 0.8651667237281799 auc= 0.9845572180775057\n",
      "Test on valid set: loss= 1.0964231491088867 acc= 0.6148260235786438 auc= 0.955111111111111\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3852\n",
      "\n",
      "Test on train set: loss= 0.3539508879184723 acc= 0.8824052810668945 auc= 0.9839568308402594\n",
      "Test on valid set: loss= 1.2249897718429565 acc= 0.568961501121521 auc= 0.9337777777777779\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3753\n",
      "\n",
      "Test on train set: loss= 0.41108569502830505 acc= 0.8633052706718445 auc= 0.984954212556322\n",
      "Test on valid set: loss= 1.6193618774414062 acc= 0.5047860145568848 auc= 0.9297777777777776\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3523\n",
      "\n",
      "Test on train set: loss= 0.36338961124420166 acc= 0.8735027313232422 auc= 0.9851079468276989\n",
      "Test on valid set: loss= 1.435279130935669 acc= 0.5172101855278015 auc= 0.9373333333333331\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3466\n",
      "\n",
      "Test on train set: loss= 0.3637729287147522 acc= 0.8718841075897217 auc= 0.9868405502443569\n",
      "Test on valid set: loss= 1.790671706199646 acc= 0.5068158507347107 auc= 0.9359999999999999\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.3405\n",
      "\n",
      "Test on train set: loss= 0.338697224855423 acc= 0.8793298602104187 auc= 0.9897268319822896\n",
      "Test on valid set: loss= 1.184743881225586 acc= 0.5948575139045715 auc= 0.9511111111111111\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.3236\n",
      "\n",
      "Test on train set: loss= 0.3456309735774994 acc= 0.8802201151847839 auc= 0.988455644806104\n",
      "Test on valid set: loss= 1.6685179471969604 acc= 0.5282213091850281 auc= 0.9324444444444445\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.3220\n",
      "\n",
      "Test on train set: loss= 0.35051384568214417 acc= 0.8779540061950684 auc= 0.988634058249682\n",
      "Test on valid set: loss= 1.3062137365341187 acc= 0.6022910475730896 auc= 0.9408888888888889\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.3111\n",
      "\n",
      "Test on train set: loss= 0.31849026679992676 acc= 0.891874372959137 auc= 0.9896276230074061\n",
      "Test on valid set: loss= 1.1558462381362915 acc= 0.6024296283721924 auc= 0.9466666666666667\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2968\n",
      "\n",
      "Test on train set: loss= 0.35866841673851013 acc= 0.8743929862976074 auc= 0.9881263176657612\n",
      "Test on valid set: loss= 1.7804187536239624 acc= 0.5535849332809448 auc= 0.9368888888888888\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2892\n",
      "\n",
      "Test on train set: loss= 0.3249225318431854 acc= 0.8862091302871704 auc= 0.9911384407162924\n",
      "Test on valid set: loss= 1.3762717247009277 acc= 0.5900963544845581 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.2765\n",
      "\n",
      "Test on train set: loss= 0.35434746742248535 acc= 0.8745548725128174 auc= 0.989981385234616\n",
      "Test on valid set: loss= 1.581574559211731 acc= 0.5779345035552979 auc= 0.9408888888888889\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2809\n",
      "\n",
      "Test on train set: loss= 0.2506217658519745 acc= 0.9127549529075623 auc= 0.9936071078447224\n",
      "Test on valid set: loss= 1.7842499017715454 acc= 0.5284827947616577 auc= 0.9324444444444445\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2693\n",
      "\n",
      "Test on train set: loss= 0.26884540915489197 acc= 0.9020718932151794 auc= 0.993752559646123\n",
      "Test on valid set: loss= 1.6432324647903442 acc= 0.575952410697937 auc= 0.9480000000000001\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.2611\n",
      "\n",
      "Test on train set: loss= 0.2682037353515625 acc= 0.9063612818717957 auc= 0.9931753046827086\n",
      "Test on valid set: loss= 1.6783442497253418 acc= 0.5652045011520386 auc= 0.9511111111111112\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2445\n",
      "\n",
      "Test on train set: loss= 0.25551384687423706 acc= 0.9091129899024963 auc= 0.9936864604323666\n",
      "Test on valid set: loss= 1.8497529029846191 acc= 0.5301187634468079 auc= 0.9302222222222222\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2406\n",
      "\n",
      "Test on train set: loss= 0.2885934114456177 acc= 0.8994011282920837 auc= 0.99283044519663\n",
      "Test on valid set: loss= 1.675339698791504 acc= 0.5753450393676758 auc= 0.9506666666666665\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2356\n",
      "\n",
      "Test on train set: loss= 0.23620879650115967 acc= 0.9176918268203735 auc= 0.9946960018648532\n",
      "Test on valid set: loss= 1.6857870817184448 acc= 0.5479037165641785 auc= 0.9324444444444444\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2323\n",
      "\n",
      "Test on train set: loss= 0.21532081067562103 acc= 0.926837146282196 auc= 0.9952316254490908\n",
      "Test on valid set: loss= 1.624922275543213 acc= 0.5885826349258423 auc= 0.9502222222222223\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2263\n",
      "\n",
      "Test on train set: loss= 0.27794599533081055 acc= 0.9011816382408142 auc= 0.9939656744834862\n",
      "Test on valid set: loss= 1.6056135892868042 acc= 0.5584732890129089 auc= 0.9404444444444444\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2138\n",
      "\n",
      "Test on train set: loss= 0.2073928564786911 acc= 0.9290223121643066 auc= 0.9956570495424348\n",
      "Test on valid set: loss= 2.1948463916778564 acc= 0.5373172163963318 auc= 0.9248888888888889\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2155\n",
      "\n",
      "Test on train set: loss= 0.24216872453689575 acc= 0.9129977226257324 auc= 0.9952348301063049\n",
      "Test on valid set: loss= 1.424003005027771 acc= 0.6178674101829529 auc= 0.936888888888889\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2112\n",
      "\n",
      "Test on train set: loss= 0.23894202709197998 acc= 0.9170443415641785 auc= 0.9943590024227273\n",
      "Test on valid set: loss= 2.074218988418579 acc= 0.555336058139801 auc= 0.9328888888888889\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.2144\n",
      "\n",
      "Test on train set: loss= 0.2175549566745758 acc= 0.9216575026512146 auc= 0.9954579998725569\n",
      "Test on valid set: loss= 1.9563260078430176 acc= 0.5633672475814819 auc= 0.9302222222222222\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1972\n",
      "\n",
      "Test on train set: loss= 0.2906380295753479 acc= 0.900210440158844 auc= 0.9952747650199445\n",
      "Test on valid set: loss= 1.9230504035949707 acc= 0.5976366400718689 auc= 0.9319999999999998\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1973\n",
      "\n",
      "Test on train set: loss= 0.17661739885807037 acc= 0.9370346665382385 auc= 0.9967114182051582\n",
      "Test on valid set: loss= 1.6159884929656982 acc= 0.6221962571144104 auc= 0.9431111111111111\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1917\n",
      "\n",
      "Test on train set: loss= 0.2120589017868042 acc= 0.9248948097229004 auc= 0.9962398158921297\n",
      "Test on valid set: loss= 1.485419511795044 acc= 0.6257556080818176 auc= 0.9480000000000001\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1903\n",
      "\n",
      "Test on train set: loss= 0.24803973734378815 acc= 0.9142116904258728 auc= 0.9955849125366306\n",
      "Test on valid set: loss= 2.1650540828704834 acc= 0.5736574530601501 auc= 0.9235555555555555\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1837\n",
      "\n",
      "Test on train set: loss= 0.17428511381149292 acc= 0.9368727803230286 auc= 0.9969893748341241\n",
      "Test on valid set: loss= 1.293949842453003 acc= 0.6583973169326782 auc= 0.9480000000000001\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1622\n",
      "\n",
      "Test on train set: loss= 0.2429434210062027 acc= 0.9160731434822083 auc= 0.9960132684150127\n",
      "Test on valid set: loss= 1.2073657512664795 acc= 0.6469568610191345 auc= 0.9511111111111111\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1732\n",
      "\n",
      "Test on train set: loss= 0.17764532566070557 acc= 0.9365490674972534 auc= 0.997022954375101\n",
      "Test on valid set: loss= 2.4514412879943848 acc= 0.5221766233444214 auc= 0.924\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1635\n",
      "\n",
      "Test on train set: loss= 0.252533882856369 acc= 0.9132405519485474 auc= 0.9950862902161951\n",
      "Test on valid set: loss= 2.771191358566284 acc= 0.5035628080368042 auc= 0.9204444444444443\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1699\n",
      "\n",
      "Test on train set: loss= 0.2200910598039627 acc= 0.9257041215896606 auc= 0.995961695295936\n",
      "Test on valid set: loss= 2.0079028606414795 acc= 0.5858383774757385 auc= 0.9293333333333333\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1679\n",
      "\n",
      "Test on train set: loss= 0.1602012664079666 acc= 0.9441567063331604 auc= 0.9975996460427204\n",
      "Test on valid set: loss= 1.520684003829956 acc= 0.6129144430160522 auc= 0.9480000000000001\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1589\n",
      "\n",
      "Test on train set: loss= 0.18952040374279022 acc= 0.9324215054512024 auc= 0.9967613693848701\n",
      "Test on valid set: loss= 1.5438728332519531 acc= 0.6502312421798706 auc= 0.952888888888889\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1431\n",
      "\n",
      "Test on train set: loss= 0.23773647844791412 acc= 0.9168015718460083 auc= 0.9955796470111407\n",
      "Test on valid set: loss= 1.728971004486084 acc= 0.6139830350875854 auc= 0.9417777777777779\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1528\n",
      "\n",
      "Test on train set: loss= 0.1582416594028473 acc= 0.9444804191589355 auc= 0.9977168719226384\n",
      "Test on valid set: loss= 1.3699917793273926 acc= 0.6477267742156982 auc= 0.9582222222222221\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1568\n",
      "\n",
      "Test on train set: loss= 0.1641969233751297 acc= 0.94205242395401 auc= 0.9971594408222721\n",
      "Test on valid set: loss= 1.903814673423767 acc= 0.5731322765350342 auc= 0.9386666666666666\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1492\n",
      "\n",
      "Test on train set: loss= 0.13098080456256866 acc= 0.9531401991844177 auc= 0.9983417768179491\n",
      "Test on valid set: loss= 1.984168291091919 acc= 0.6090090274810791 auc= 0.9479999999999998\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1418\n",
      "\n",
      "Test on train set: loss= 0.11772771924734116 acc= 0.9564583897590637 auc= 0.9985159412676866\n",
      "Test on valid set: loss= 1.5263503789901733 acc= 0.6431472301483154 auc= 0.9488888888888889\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1399\n",
      "\n",
      "Test on train set: loss= 0.213387131690979 acc= 0.9249756932258606 auc= 0.9973799118102358\n",
      "Test on valid set: loss= 1.5906094312667847 acc= 0.6441232562065125 auc= 0.9475555555555555\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1327\n",
      "\n",
      "Test on train set: loss= 0.15059438347816467 acc= 0.9479604959487915 auc= 0.9981072252630044\n",
      "Test on valid set: loss= 1.9993609189987183 acc= 0.6186231374740601 auc= 0.9488888888888889\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1290\n",
      "\n",
      "Test on train set: loss= 0.16088402271270752 acc= 0.9415668249130249 auc= 0.9978835356311888\n",
      "Test on valid set: loss= 1.649957537651062 acc= 0.6340773701667786 auc= 0.9422222222222221\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1265\n",
      "\n",
      "Test on train set: loss= 0.15439866483211517 acc= 0.9443994760513306 auc= 0.9979324484587135\n",
      "Test on valid set: loss= 1.6299710273742676 acc= 0.6398585438728333 auc= 0.9346666666666668\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1311\n",
      "\n",
      "Test on train set: loss= 0.1377282291650772 acc= 0.9495791792869568 auc= 0.998438134862581\n",
      "Test on valid set: loss= 1.5962741374969482 acc= 0.6480311751365662 auc= 0.9426666666666665\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1255\n",
      "\n",
      "Test on train set: loss= 0.09298890829086304 acc= 0.9676270484924316 auc= 0.9991935720423847\n",
      "Test on valid set: loss= 1.5178207159042358 acc= 0.6825029253959656 auc= 0.9577777777777777\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1150\n",
      "\n",
      "Test on train set: loss= 0.18742108345031738 acc= 0.9341210722923279 auc= 0.9984757912840767\n",
      "Test on valid set: loss= 1.7949762344360352 acc= 0.6537619233131409 auc= 0.9551111111111112\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1215\n",
      "\n",
      "Test on train set: loss= 0.11305706948041916 acc= 0.9607478380203247 auc= 0.9988449217326989\n",
      "Test on valid set: loss= 1.676194667816162 acc= 0.6358395218849182 auc= 0.9453333333333334\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1020\n",
      "\n",
      "Test on train set: loss= 0.12189583480358124 acc= 0.957915186882019 auc= 0.9987508439155338\n",
      "Test on valid set: loss= 1.7407736778259277 acc= 0.6077808141708374 auc= 0.9528888888888888\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1042\n",
      "\n",
      "Test on train set: loss= 0.13498832285404205 acc= 0.9520880579948425 auc= 0.9986768477893101\n",
      "Test on valid set: loss= 1.6770730018615723 acc= 0.6626764535903931 auc= 0.9417777777777779\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1067\n",
      "\n",
      "Test on train set: loss= 0.14847883582115173 acc= 0.9473939538002014 auc= 0.998245959824148\n",
      "Test on valid set: loss= 1.767502784729004 acc= 0.6320902109146118 auc= 0.9408888888888889\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1093\n",
      "\n",
      "Test on train set: loss= 0.1035427376627922 acc= 0.9617189764976501 auc= 0.9991911715124668\n",
      "Test on valid set: loss= 1.818448543548584 acc= 0.634093165397644 auc= 0.9493333333333334\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.1059\n",
      "\n",
      "Test on train set: loss= 0.0996180921792984 acc= 0.965684711933136 auc= 0.9992709166866325\n",
      "Test on valid set: loss= 1.46484375 acc= 0.6898990869522095 auc= 0.9484444444444444\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0999\n",
      "\n",
      "Test on train set: loss= 0.1584039181470871 acc= 0.942861795425415 auc= 0.9984757281006056\n",
      "Test on valid set: loss= 1.7180691957473755 acc= 0.6399028301239014 auc= 0.9466666666666667\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0974\n",
      "\n",
      "Test on train set: loss= 0.09717808663845062 acc= 0.9661703109741211 auc= 0.9993103182659817\n",
      "Test on valid set: loss= 1.6148566007614136 acc= 0.6843038201332092 auc= 0.9577777777777777\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1005\n",
      "\n",
      "Test on train set: loss= 0.1078554168343544 acc= 0.9607478380203247 auc= 0.9990025269354719\n",
      "Test on valid set: loss= 1.5188392400741577 acc= 0.6223481297492981 auc= 0.9608888888888888\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0988\n",
      "\n",
      "Test on train set: loss= 0.1011788621544838 acc= 0.9647944569587708 auc= 0.999158350771931\n",
      "Test on valid set: loss= 1.4578741788864136 acc= 0.6970288157463074 auc= 0.9555555555555557\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.1093\n",
      "\n",
      "Test on train set: loss= 0.10843153297901154 acc= 0.9598575830459595 auc= 0.9992794654974672\n",
      "Test on valid set: loss= 1.2101176977157593 acc= 0.6911145448684692 auc= 0.9537777777777778\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0985\n",
      "\n",
      "Test on train set: loss= 0.11452692747116089 acc= 0.9602622389793396 auc= 0.9989177368745935\n",
      "Test on valid set: loss= 1.5391165018081665 acc= 0.6313071846961975 auc= 0.9466666666666667\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0963\n",
      "\n",
      "Test on train set: loss= 0.08055566996335983 acc= 0.972968578338623 auc= 0.9993814488442517\n",
      "Test on valid set: loss= 1.593389630317688 acc= 0.6652587652206421 auc= 0.9568888888888889\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0895\n",
      "\n",
      "Test on train set: loss= 0.11676783859729767 acc= 0.9562965631484985 auc= 0.9993620337689695\n",
      "Test on valid set: loss= 1.9036294221878052 acc= 0.6472756266593933 auc= 0.944888888888889\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0863\n",
      "\n",
      "Test on train set: loss= 0.07342569530010223 acc= 0.9750728607177734 auc= 0.9994043890606099\n",
      "Test on valid set: loss= 1.5853570699691772 acc= 0.690843403339386 auc= 0.9457777777777776\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0827\n",
      "\n",
      "Test on train set: loss= 0.09916343539953232 acc= 0.9642279148101807 auc= 0.9991598227035668\n",
      "Test on valid set: loss= 1.4759619235992432 acc= 0.6557489037513733 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0862\n",
      "\n",
      "Test on train set: loss= 0.12458639591932297 acc= 0.9558918476104736 auc= 0.9992306280155179\n",
      "Test on valid set: loss= 1.6944873332977295 acc= 0.6740347743034363 auc= 0.9511111111111111\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0891\n",
      "\n",
      "Test on train set: loss= 0.12668834626674652 acc= 0.9548397660255432 auc= 0.999023399539215\n",
      "Test on valid set: loss= 1.8833028078079224 acc= 0.633433997631073 auc= 0.9515555555555556\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0873\n",
      "\n",
      "Test on train set: loss= 0.057746633887290955 acc= 0.9812237024307251 auc= 0.9996316345745898\n",
      "Test on valid set: loss= 1.6196879148483276 acc= 0.664706289768219 auc= 0.9537777777777778\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0722\n",
      "\n",
      "Test on train set: loss= 0.059162724763154984 acc= 0.9781482815742493 auc= 0.9996790216701198\n",
      "Test on valid set: loss= 1.6591110229492188 acc= 0.6447876691818237 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0736\n",
      "\n",
      "Test on train set: loss= 0.06960882991552353 acc= 0.9753156304359436 auc= 0.9995718095537509\n",
      "Test on valid set: loss= 1.408173680305481 acc= 0.6758053302764893 auc= 0.9706666666666667\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0794\n",
      "\n",
      "Test on train set: loss= 0.07053188234567642 acc= 0.9747490882873535 auc= 0.9995680221945099\n",
      "Test on valid set: loss= 1.744828224182129 acc= 0.6042020320892334 auc= 0.9422222222222223\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0727\n",
      "\n",
      "Test on train set: loss= 0.07596836239099503 acc= 0.9743444323539734 auc= 0.9994442689504235\n",
      "Test on valid set: loss= 1.6908890008926392 acc= 0.638845682144165 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0771\n",
      "\n",
      "Test on train set: loss= 0.11106386035680771 acc= 0.9594529271125793 auc= 0.9991378500862096\n",
      "Test on valid set: loss= 1.7538375854492188 acc= 0.6290335059165955 auc= 0.9515555555555555\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0697\n",
      "\n",
      "Test on train set: loss= 0.07246725261211395 acc= 0.9752346873283386 auc= 0.9996394674218909\n",
      "Test on valid set: loss= 1.4401178359985352 acc= 0.7210054397583008 auc= 0.9613333333333334\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0700\n",
      "\n",
      "Test on train set: loss= 0.05743122100830078 acc= 0.98057621717453 auc= 0.9996972635942706\n",
      "Test on valid set: loss= 1.7835781574249268 acc= 0.6424592733383179 auc= 0.9417777777777779\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0770\n",
      "\n",
      "Test on train set: loss= 0.11222309619188309 acc= 0.9644706845283508 auc= 0.9990774031232134\n",
      "Test on valid set: loss= 1.98484206199646 acc= 0.6162324547767639 auc= 0.9426666666666665\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0709\n",
      "\n",
      "Test on train set: loss= 0.06550464034080505 acc= 0.9759631156921387 auc= 0.9996115806130295\n",
      "Test on valid set: loss= 2.0064241886138916 acc= 0.5975674390792847 auc= 0.9475555555555557\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0707\n",
      "\n",
      "Test on train set: loss= 0.058646053075790405 acc= 0.9782292246818542 auc= 0.9996558070826419\n",
      "Test on valid set: loss= 1.7662304639816284 acc= 0.621505856513977 auc= 0.9435555555555556\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0711\n",
      "\n",
      "Test on train set: loss= 0.07602917402982712 acc= 0.9719974398612976 auc= 0.9995568475756113\n",
      "Test on valid set: loss= 1.6581780910491943 acc= 0.6006841063499451 auc= 0.9444444444444444\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0690\n",
      "\n",
      "Test on train set: loss= 0.06309734284877777 acc= 0.977338969707489 auc= 0.999694455522009\n",
      "Test on valid set: loss= 1.40543794631958 acc= 0.6864886283874512 auc= 0.9573333333333333\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0570\n",
      "\n",
      "Test on train set: loss= 0.05768558382987976 acc= 0.9810618162155151 auc= 0.999692176590924\n",
      "Test on valid set: loss= 1.4225589036941528 acc= 0.6812425255775452 auc= 0.9528888888888888\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0720\n",
      "\n",
      "Test on train set: loss= 0.08242078870534897 acc= 0.9704596996307373 auc= 0.9993930609739353\n",
      "Test on valid set: loss= 1.435646414756775 acc= 0.6521049737930298 auc= 0.9497777777777777\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0705\n",
      "\n",
      "Test on train set: loss= 0.06566223502159119 acc= 0.976610541343689 auc= 0.9996500966814705\n",
      "Test on valid set: loss= 1.675339698791504 acc= 0.636212170124054 auc= 0.96\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0658\n",
      "\n",
      "Test on train set: loss= 0.0607699416577816 acc= 0.98089998960495 auc= 0.999711523042311\n",
      "Test on valid set: loss= 1.4142978191375732 acc= 0.6412470936775208 auc= 0.955111111111111\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0641\n",
      "\n",
      "Test on train set: loss= 0.05472582206130028 acc= 0.9800097346305847 auc= 0.9998136289622364\n",
      "Test on valid set: loss= 1.583132028579712 acc= 0.6569629907608032 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0626\n",
      "\n",
      "Test on train set: loss= 0.09575501829385757 acc= 0.9660084247589111 auc= 0.9996276811443622\n",
      "Test on valid set: loss= 1.0999664068222046 acc= 0.7243581414222717 auc= 0.9688888888888888\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0543\n",
      "\n",
      "Test on train set: loss= 0.04829208552837372 acc= 0.9825995564460754 auc= 0.999738077669978\n",
      "Test on valid set: loss= 1.616589069366455 acc= 0.6721858382225037 auc= 0.9568888888888889\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0632\n",
      "\n",
      "Test on train set: loss= 0.0680401399731636 acc= 0.9754775166511536 auc= 0.9997151095487661\n",
      "Test on valid set: loss= 1.3245794773101807 acc= 0.7140008211135864 auc= 0.9595555555555556\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0624\n",
      "\n",
      "Test on train set: loss= 0.05979776754975319 acc= 0.9775817394256592 auc= 0.9997469835054809\n",
      "Test on valid set: loss= 1.9327927827835083 acc= 0.648546576499939 auc= 0.9426666666666668\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0656\n",
      "\n",
      "Test on train set: loss= 0.09257090836763382 acc= 0.9677889347076416 auc= 0.9994010429897084\n",
      "Test on valid set: loss= 1.4757472276687622 acc= 0.6890045404434204 auc= 0.9675555555555556\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0595\n",
      "\n",
      "Test on train set: loss= 0.057734549045562744 acc= 0.9794431924819946 auc= 0.9996827299035702\n",
      "Test on valid set: loss= 1.0490357875823975 acc= 0.7405399680137634 auc= 0.9666666666666666\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0513\n",
      "\n",
      "Test on train set: loss= 0.04705314710736275 acc= 0.9840563535690308 auc= 0.9998168819703823\n",
      "Test on valid set: loss= 1.5436680316925049 acc= 0.6896992325782776 auc= 0.9564444444444444\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0580\n",
      "\n",
      "Test on train set: loss= 0.0425722599029541 acc= 0.9851893782615662 auc= 0.9997689666163347\n",
      "Test on valid set: loss= 1.719161033630371 acc= 0.6524606943130493 auc= 0.952\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0558\n",
      "\n",
      "Test on train set: loss= 0.05942531302571297 acc= 0.9791194796562195 auc= 0.9997345139830106\n",
      "Test on valid set: loss= 1.9045147895812988 acc= 0.6542748808860779 auc= 0.944\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0491\n",
      "\n",
      "Test on train set: loss= 0.05849023908376694 acc= 0.9786338806152344 auc= 0.9997845088035968\n",
      "Test on valid set: loss= 1.6720778942108154 acc= 0.6899608373641968 auc= 0.9515555555555556\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0482\n",
      "\n",
      "Test on train set: loss= 0.07324733585119247 acc= 0.9744253754615784 auc= 0.9995573664663377\n",
      "Test on valid set: loss= 1.4220761060714722 acc= 0.7538127899169922 auc= 0.9564444444444444\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0556\n",
      "\n",
      "Test on train set: loss= 0.05636381357908249 acc= 0.9795241355895996 auc= 0.9997258481294555\n",
      "Test on valid set: loss= 1.7903475761413574 acc= 0.6915169358253479 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0447\n",
      "\n",
      "Test on train set: loss= 0.0381908193230629 acc= 0.9869698882102966 auc= 0.9998793655798032\n",
      "Test on valid set: loss= 1.3144679069519043 acc= 0.7270887494087219 auc= 0.9715555555555555\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0537\n",
      "\n",
      "Test on train set: loss= 0.0358082614839077 acc= 0.9875364303588867 auc= 0.9999023573199949\n",
      "Test on valid set: loss= 1.5427331924438477 acc= 0.682679295539856 auc= 0.9626666666666666\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0450\n",
      "\n",
      "Test on train set: loss= 0.05720733851194382 acc= 0.9801715612411499 auc= 0.9997247763433803\n",
      "Test on valid set: loss= 1.4298397302627563 acc= 0.7001324892044067 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0499\n",
      "\n",
      "Test on train set: loss= 0.028792129829525948 acc= 0.9899643659591675 auc= 0.9999318922874867\n",
      "Test on valid set: loss= 1.4533112049102783 acc= 0.695422351360321 auc= 0.960888888888889\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0482\n",
      "\n",
      "Test on train set: loss= 0.08680514991283417 acc= 0.9691647887229919 auc= 0.9995917042366023\n",
      "Test on valid set: loss= 1.6510310173034668 acc= 0.6633186340332031 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0420\n",
      "\n",
      "Test on train set: loss= 0.027735773473978043 acc= 0.9906118512153625 auc= 0.9999207862883122\n",
      "Test on valid set: loss= 1.3640927076339722 acc= 0.7023271322250366 auc= 0.9639999999999999\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0511\n",
      "\n",
      "Test on train set: loss= 0.04202111065387726 acc= 0.9851084351539612 auc= 0.9998623194297561\n",
      "Test on valid set: loss= 1.765238642692566 acc= 0.6932275295257568 auc= 0.944888888888889\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0491\n",
      "\n",
      "Test on train set: loss= 0.042475827038288116 acc= 0.984865665435791 auc= 0.9998640717099448\n",
      "Test on valid set: loss= 1.7699705362319946 acc= 0.6825765371322632 auc= 0.9479999999999998\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0394\n",
      "\n",
      "Test on train set: loss= 0.04258948192000389 acc= 0.984946608543396 auc= 0.9998669328593477\n",
      "Test on valid set: loss= 1.8069653511047363 acc= 0.6515692472457886 auc= 0.9497777777777777\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0399\n",
      "\n",
      "Test on train set: loss= 0.03292632848024368 acc= 0.9885076284408569 auc= 0.9999057798666531\n",
      "Test on valid set: loss= 1.7268656492233276 acc= 0.6881762742996216 auc= 0.9591111111111111\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0384\n",
      "\n",
      "Test on train set: loss= 0.026627589017152786 acc= 0.9914211630821228 auc= 0.9999391275371583\n",
      "Test on valid set: loss= 1.6103358268737793 acc= 0.6888574361801147 auc= 0.9506666666666668\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0372\n",
      "\n",
      "Test on train set: loss= 0.030562421306967735 acc= 0.9893169403076172 auc= 0.9999382396446688\n",
      "Test on valid set: loss= 1.6699419021606445 acc= 0.6846189498901367 auc= 0.9551111111111112\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0416\n",
      "\n",
      "Test on train set: loss= 0.048731010407209396 acc= 0.9830042123794556 auc= 0.9998132073686179\n",
      "Test on valid set: loss= 1.5264599323272705 acc= 0.7121208906173706 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0453\n",
      "\n",
      "Test on train set: loss= 0.031513866037130356 acc= 0.9900453090667725 auc= 0.9999108235788366\n",
      "Test on valid set: loss= 1.451613426208496 acc= 0.6949537396430969 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0425\n",
      "\n",
      "Test on train set: loss= 0.049631267786026 acc= 0.9823567271232605 auc= 0.999735713920853\n",
      "Test on valid set: loss= 1.419744849205017 acc= 0.6841391921043396 auc= 0.96\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0402\n",
      "\n",
      "Test on train set: loss= 0.02694021537899971 acc= 0.9906927943229675 auc= 0.9999321209197353\n",
      "Test on valid set: loss= 1.389538288116455 acc= 0.6919310092926025 auc= 0.9684444444444444\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0378\n",
      "\n",
      "Test on train set: loss= 0.05190037935972214 acc= 0.9792813062667847 auc= 0.9998626238446718\n",
      "Test on valid set: loss= 1.589913010597229 acc= 0.6710710525512695 auc= 0.9435555555555555\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0384\n",
      "\n",
      "Test on train set: loss= 0.038668204098939896 acc= 0.9859986901283264 auc= 0.9998927009213426\n",
      "Test on valid set: loss= 1.6256729364395142 acc= 0.6924679279327393 auc= 0.9502222222222223\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0323\n",
      "\n",
      "Test on train set: loss= 0.037984758615493774 acc= 0.9868889451026917 auc= 0.9998882477110582\n",
      "Test on valid set: loss= 1.4732906818389893 acc= 0.7002332210540771 auc= 0.952\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0278\n",
      "\n",
      "Test on train set: loss= 0.021118128672242165 acc= 0.9930398464202881 auc= 0.9999673292423268\n",
      "Test on valid set: loss= 1.5603647232055664 acc= 0.6937878727912903 auc= 0.9635555555555557\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0332\n",
      "\n",
      "Test on train set: loss= 0.028623882681131363 acc= 0.9902881383895874 auc= 0.9999212830605204\n",
      "Test on valid set: loss= 1.5285449028015137 acc= 0.7242190837860107 auc= 0.9546666666666667\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0343\n",
      "\n",
      "Test on train set: loss= 0.043791674077510834 acc= 0.9844610095024109 auc= 0.9998461080050864\n",
      "Test on valid set: loss= 1.651051640510559 acc= 0.6555300354957581 auc= 0.9533333333333335\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0385\n",
      "\n",
      "Test on train set: loss= 0.04325729236006737 acc= 0.9860796332359314 auc= 0.9998553424550731\n",
      "Test on valid set: loss= 1.891351580619812 acc= 0.6597107648849487 auc= 0.9524444444444444\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0349\n",
      "\n",
      "Test on train set: loss= 0.03622835502028465 acc= 0.9876173734664917 auc= 0.9999160778612357\n",
      "Test on valid set: loss= 1.4828811883926392 acc= 0.6988568305969238 auc= 0.9502222222222221\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0376\n",
      "\n",
      "Test on train set: loss= 0.034237924963235855 acc= 0.9879410862922668 auc= 0.9999330717991548\n",
      "Test on valid set: loss= 1.4723483324050903 acc= 0.730953574180603 auc= 0.9564444444444444\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0384\n",
      "\n",
      "Test on train set: loss= 0.01918639801442623 acc= 0.9940919280052185 auc= 0.9999549267312385\n",
      "Test on valid set: loss= 1.8394771814346313 acc= 0.6873093843460083 auc= 0.9426666666666665\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0303\n",
      "\n",
      "Test on train set: loss= 0.02337672747671604 acc= 0.992392361164093 auc= 0.9999502302352941\n",
      "Test on valid set: loss= 1.876505732536316 acc= 0.695738673210144 auc= 0.9564444444444444\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0328\n",
      "\n",
      "Test on train set: loss= 0.034910548478364944 acc= 0.9879410862922668 auc= 0.9998919516224773\n",
      "Test on valid set: loss= 1.5176266431808472 acc= 0.6922069191932678 auc= 0.9493333333333333\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0303\n",
      "\n",
      "Test on train set: loss= 0.0208580382168293 acc= 0.9937682151794434 auc= 0.9999602103688533\n",
      "Test on valid set: loss= 1.5583441257476807 acc= 0.717117965221405 auc= 0.9533333333333334\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0287\n",
      "\n",
      "Test on train set: loss= 0.025605469942092896 acc= 0.9909355640411377 auc= 0.9999308470716173\n",
      "Test on valid set: loss= 1.8256546258926392 acc= 0.6983264684677124 auc= 0.9586666666666666\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0362\n",
      "\n",
      "Test on train set: loss= 0.04709921404719353 acc= 0.9839754104614258 auc= 0.9998828041515152\n",
      "Test on valid set: loss= 1.6114856004714966 acc= 0.6733709573745728 auc= 0.9546666666666667\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0343\n",
      "\n",
      "Test on train set: loss= 0.028336353600025177 acc= 0.9900453090667725 auc= 0.9999222040464874\n",
      "Test on valid set: loss= 1.4581036567687988 acc= 0.7109667062759399 auc= 0.9568888888888887\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0355\n",
      "\n",
      "Test on train set: loss= 0.02026188187301159 acc= 0.9929589033126831 auc= 0.9999678558066973\n",
      "Test on valid set: loss= 1.6501559019088745 acc= 0.6761097311973572 auc= 0.9586666666666666\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0326\n",
      "\n",
      "Test on train set: loss= 0.02704697474837303 acc= 0.9900453090667725 auc= 0.9999518119131633\n",
      "Test on valid set: loss= 1.5234146118164062 acc= 0.7120047807693481 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0279\n",
      "\n",
      "Test on train set: loss= 0.024666016921401024 acc= 0.991744875907898 auc= 0.9999551398146895\n",
      "Test on valid set: loss= 1.676037311553955 acc= 0.6810630559921265 auc= 0.952888888888889\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0289\n",
      "\n",
      "Test on train set: loss= 0.020518694072961807 acc= 0.9932016730308533 auc= 0.9999629739941192\n",
      "Test on valid set: loss= 1.3545652627944946 acc= 0.7219968438148499 auc= 0.9568888888888889\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0276\n",
      "\n",
      "Test on train set: loss= 0.02446041628718376 acc= 0.9916639924049377 auc= 0.99993668042676\n",
      "Test on valid set: loss= 1.4429409503936768 acc= 0.7185925245285034 auc= 0.9542222222222222\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0354\n",
      "\n",
      "Test on train set: loss= 0.053327932953834534 acc= 0.9802525043487549 auc= 0.9999165062462101\n",
      "Test on valid set: loss= 1.2112033367156982 acc= 0.7076008319854736 auc= 0.9586666666666666\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0333\n",
      "\n",
      "Test on train set: loss= 0.019328869879245758 acc= 0.9927970170974731 auc= 0.9999684156026157\n",
      "Test on valid set: loss= 1.2569199800491333 acc= 0.7277895212173462 auc= 0.9591111111111111\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0306\n",
      "\n",
      "Test on train set: loss= 0.022587545216083527 acc= 0.9922304749488831 auc= 0.999950860588361\n",
      "Test on valid set: loss= 1.4223397970199585 acc= 0.7072116136550903 auc= 0.9577777777777777\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0290\n",
      "\n",
      "Test on train set: loss= 0.025618992745876312 acc= 0.9906927943229675 auc= 0.9999474005146309\n",
      "Test on valid set: loss= 1.3590612411499023 acc= 0.7238378524780273 auc= 0.9688888888888888\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0293\n",
      "\n",
      "Test on train set: loss= 0.023644395172595978 acc= 0.992311418056488 auc= 0.9999701954568696\n",
      "Test on valid set: loss= 1.5618808269500732 acc= 0.735171914100647 auc= 0.9617777777777776\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0204\n",
      "\n",
      "Test on train set: loss= 0.019058121368288994 acc= 0.9936872720718384 auc= 0.9999717082693051\n",
      "Test on valid set: loss= 1.8206896781921387 acc= 0.705225944519043 auc= 0.9493333333333334\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0268\n",
      "\n",
      "Test on train set: loss= 0.021057642996311188 acc= 0.9927160739898682 auc= 0.999966379644737\n",
      "Test on valid set: loss= 1.449588656425476 acc= 0.7339695692062378 auc= 0.9577777777777777\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0249\n",
      "\n",
      "Test on train set: loss= 0.030915936455130577 acc= 0.9886694550514221 auc= 0.9999243501433728\n",
      "Test on valid set: loss= 1.2986044883728027 acc= 0.7454308271408081 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0248\n",
      "\n",
      "Test on train set: loss= 0.020361589267849922 acc= 0.9930398464202881 auc= 0.9999697305701583\n",
      "Test on valid set: loss= 1.689609408378601 acc= 0.7089574337005615 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0276\n",
      "\n",
      "Test on train set: loss= 0.014209123328328133 acc= 0.9951440691947937 auc= 0.9999828196585222\n",
      "Test on valid set: loss= 1.449070692062378 acc= 0.7372776865959167 auc= 0.96\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0262\n",
      "\n",
      "Test on train set: loss= 0.020729320123791695 acc= 0.9932016730308533 auc= 0.9999631877505901\n",
      "Test on valid set: loss= 1.424412488937378 acc= 0.7213701009750366 auc= 0.9613333333333334\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0222\n",
      "\n",
      "Test on train set: loss= 0.02107868157327175 acc= 0.9932826161384583 auc= 0.9999772443994617\n",
      "Test on valid set: loss= 1.586230754852295 acc= 0.7072986960411072 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0225\n",
      "\n",
      "Test on train set: loss= 0.012616807594895363 acc= 0.9961152672767639 auc= 0.999986274265203\n",
      "Test on valid set: loss= 1.2954891920089722 acc= 0.7079941034317017 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0240\n",
      "\n",
      "Test on train set: loss= 0.017267001792788506 acc= 0.9939301013946533 auc= 0.9999779566248114\n",
      "Test on valid set: loss= 1.5060334205627441 acc= 0.7112648487091064 auc= 0.9604444444444444\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0240\n",
      "\n",
      "Test on train set: loss= 0.022216424345970154 acc= 0.992473304271698 auc= 0.9999560696010501\n",
      "Test on valid set: loss= 1.400578260421753 acc= 0.6983539462089539 auc= 0.9608888888888888\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0316\n",
      "\n",
      "Test on train set: loss= 0.05159085988998413 acc= 0.9813046455383301 auc= 0.9998667877423975\n",
      "Test on valid set: loss= 1.624260425567627 acc= 0.7019473910331726 auc= 0.9684444444444444\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0299\n",
      "\n",
      "Test on train set: loss= 0.03380966931581497 acc= 0.9879410862922668 auc= 0.9999313723004286\n",
      "Test on valid set: loss= 1.6810760498046875 acc= 0.6819757223129272 auc= 0.9524444444444443\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0342\n",
      "\n",
      "Test on train set: loss= 0.02495577186346054 acc= 0.9918258190155029 auc= 0.9999485135891957\n",
      "Test on valid set: loss= 1.570890188217163 acc= 0.6706504225730896 auc= 0.9555555555555555\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0282\n",
      "\n",
      "Test on train set: loss= 0.01766011491417885 acc= 0.9940919280052185 auc= 0.9999639907647261\n",
      "Test on valid set: loss= 1.5273629426956177 acc= 0.6766855716705322 auc= 0.9577777777777777\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0289\n",
      "\n",
      "Test on train set: loss= 0.032737087458372116 acc= 0.988345742225647 auc= 0.9999199127049618\n",
      "Test on valid set: loss= 1.1235785484313965 acc= 0.7517982721328735 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0320\n",
      "\n",
      "Test on train set: loss= 0.026867611333727837 acc= 0.9906118512153625 auc= 0.9999490341512388\n",
      "Test on valid set: loss= 1.4993417263031006 acc= 0.741974413394928 auc= 0.9573333333333333\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0253\n",
      "\n",
      "Test on train set: loss= 0.017053300514817238 acc= 0.9949821829795837 auc= 0.999982265373635\n",
      "Test on valid set: loss= 1.4103814363479614 acc= 0.7739604711532593 auc= 0.968\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0227\n",
      "\n",
      "Test on train set: loss= 0.024864766746759415 acc= 0.9904499650001526 auc= 0.9999689376275536\n",
      "Test on valid set: loss= 1.398058295249939 acc= 0.7630537152290344 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0228\n",
      "\n",
      "Test on train set: loss= 0.024173201993107796 acc= 0.992311418056488 auc= 0.9999543816969174\n",
      "Test on valid set: loss= 1.3123756647109985 acc= 0.7280603647232056 auc= 0.9666666666666668\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0233\n",
      "\n",
      "Test on train set: loss= 0.012217125855386257 acc= 0.9954677820205688 auc= 0.99998993979746\n",
      "Test on valid set: loss= 1.748889446258545 acc= 0.6801695823669434 auc= 0.9560000000000001\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0198\n",
      "\n",
      "Test on train set: loss= 0.009784195572137833 acc= 0.9971673488616943 auc= 0.9999923795628609\n",
      "Test on valid set: loss= 1.6220386028289795 acc= 0.6959698796272278 auc= 0.9653333333333334\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0245\n",
      "\n",
      "Test on train set: loss= 0.011078736744821072 acc= 0.995872437953949 auc= 0.9999927811258248\n",
      "Test on valid set: loss= 1.6944770812988281 acc= 0.7316457033157349 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0214\n",
      "\n",
      "Test on train set: loss= 0.014738891273736954 acc= 0.9944965839385986 auc= 0.9999911148144163\n",
      "Test on valid set: loss= 1.3692153692245483 acc= 0.7391190528869629 auc= 0.9684444444444444\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0261\n",
      "\n",
      "Test on train set: loss= 0.026962924748659134 acc= 0.9902071952819824 auc= 0.9999579886086801\n",
      "Test on valid set: loss= 1.4725285768508911 acc= 0.686914324760437 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0200\n",
      "\n",
      "Test on train set: loss= 0.019420132040977478 acc= 0.9937682151794434 auc= 0.9999554242216675\n",
      "Test on valid set: loss= 1.5447639226913452 acc= 0.6978236436843872 auc= 0.9617777777777778\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0174\n",
      "\n",
      "Test on train set: loss= 0.01748739928007126 acc= 0.9944156408309937 auc= 0.9999813406313407\n",
      "Test on valid set: loss= 1.9408237934112549 acc= 0.6573536992073059 auc= 0.9524444444444443\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0216\n",
      "\n",
      "Test on train set: loss= 0.03093094378709793 acc= 0.9885885119438171 auc= 0.9999245891674242\n",
      "Test on valid set: loss= 1.6920450925827026 acc= 0.6866371035575867 auc= 0.9524444444444444\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0279\n",
      "\n",
      "Test on train set: loss= 0.025361135601997375 acc= 0.9918258190155029 auc= 0.9999652319416816\n",
      "Test on valid set: loss= 1.7002720832824707 acc= 0.7239089608192444 auc= 0.9564444444444445\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0180\n",
      "\n",
      "Test on train set: loss= 0.02171473205089569 acc= 0.992473304271698 auc= 0.9999729150484589\n",
      "Test on valid set: loss= 1.4780884981155396 acc= 0.7056718468666077 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0188\n",
      "\n",
      "Test on train set: loss= 0.018970796838402748 acc= 0.9933635592460632 auc= 0.9999722601985797\n",
      "Test on valid set: loss= 1.473170518875122 acc= 0.7045897245407104 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0164\n",
      "\n",
      "Test on train set: loss= 0.011634962633252144 acc= 0.996600866317749 auc= 0.9999928851207818\n",
      "Test on valid set: loss= 1.512829303741455 acc= 0.7392436861991882 auc= 0.96\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0180\n",
      "\n",
      "Test on train set: loss= 0.01161187794059515 acc= 0.9964389801025391 auc= 0.999984236732411\n",
      "Test on valid set: loss= 1.3716598749160767 acc= 0.6838143467903137 auc= 0.9671111111111113\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0169\n",
      "\n",
      "Test on train set: loss= 0.008494011126458645 acc= 0.9977338910102844 auc= 0.9999921144375528\n",
      "Test on valid set: loss= 1.6204959154129028 acc= 0.6662129163742065 auc= 0.9546666666666667\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0142\n",
      "\n",
      "Test on train set: loss= 0.007114568259567022 acc= 0.9976529479026794 auc= 0.9999967701750127\n",
      "Test on valid set: loss= 1.5403081178665161 acc= 0.722582995891571 auc= 0.9560000000000001\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0138\n",
      "\n",
      "Test on train set: loss= 0.007546304725110531 acc= 0.9981385469436646 auc= 0.9999958620887407\n",
      "Test on valid set: loss= 1.4724308252334595 acc= 0.7199407815933228 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0185\n",
      "\n",
      "Test on train set: loss= 0.012707333080470562 acc= 0.9963580369949341 auc= 0.9999778459879858\n",
      "Test on valid set: loss= 1.7120684385299683 acc= 0.7076419591903687 auc= 0.9635555555555555\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0166\n",
      "\n",
      "Test on train set: loss= 0.011954355984926224 acc= 0.9961961507797241 auc= 0.9999897639375526\n",
      "Test on valid set: loss= 1.9911320209503174 acc= 0.6784635782241821 auc= 0.9497777777777779\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0146\n",
      "\n",
      "Test on train set: loss= 0.010322791524231434 acc= 0.9969245791435242 auc= 0.9999918451341884\n",
      "Test on valid set: loss= 1.1484328508377075 acc= 0.7494679689407349 auc= 0.968\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0198\n",
      "\n",
      "Test on train set: loss= 0.01381155289709568 acc= 0.9947394132614136 auc= 0.9999895424494138\n",
      "Test on valid set: loss= 1.3889267444610596 acc= 0.7056434154510498 auc= 0.9595555555555555\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 0.0171\n",
      "\n",
      "Test on train set: loss= 0.009538961574435234 acc= 0.996600866317749 auc= 0.9999934979390718\n",
      "Test on valid set: loss= 1.4579133987426758 acc= 0.6966387033462524 auc= 0.9631111111111113\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 0.0130\n",
      "\n",
      "Test on train set: loss= 0.006463855504989624 acc= 0.9983813762664795 auc= 0.9999944888524194\n",
      "Test on valid set: loss= 1.4322383403778076 acc= 0.6851842999458313 auc= 0.9622222222222222\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0118\n",
      "\n",
      "Test on train set: loss= 0.00384528492577374 acc= 0.9991097450256348 auc= 0.9999993918280305\n",
      "Test on valid set: loss= 1.274369239807129 acc= 0.7039705514907837 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0097\n",
      "\n",
      "Test on train set: loss= 0.0030939739663153887 acc= 0.9993525147438049 auc= 0.9999995914260532\n",
      "Test on valid set: loss= 1.2144078016281128 acc= 0.7261174321174622 auc= 0.9639999999999999\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0050\n",
      "\n",
      "Test on train set: loss= 0.002673253882676363 acc= 0.9994334578514099 auc= 0.9999996405299605\n",
      "Test on valid set: loss= 1.220744013786316 acc= 0.7236398458480835 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0047\n",
      "\n",
      "Test on train set: loss= 0.0022874674759805202 acc= 0.9996762871742249 auc= 0.999999854682264\n",
      "Test on valid set: loss= 1.1939598321914673 acc= 0.7243542671203613 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0067\n",
      "\n",
      "Test on train set: loss= 0.002031247364357114 acc= 0.9996762871742249 auc= 0.9999999622829907\n",
      "Test on valid set: loss= 1.1460597515106201 acc= 0.7307409048080444 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0054\n",
      "\n",
      "Test on train set: loss= 0.0019746108446270227 acc= 0.9995144009590149 auc= 0.9999999533369539\n",
      "Test on valid set: loss= 1.2065004110336304 acc= 0.7222938537597656 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0043\n",
      "\n",
      "Test on train set: loss= 0.0015640759374946356 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2693800926208496 acc= 0.7154991030693054 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0051\n",
      "\n",
      "Test on train set: loss= 0.001496125478297472 acc= 0.99983811378479 auc= 0.9999999917331104\n",
      "Test on valid set: loss= 1.2626723051071167 acc= 0.7220105528831482 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0040\n",
      "\n",
      "Test on train set: loss= 0.001397032174281776 acc= 0.99983811378479 auc= 0.9999999917331104\n",
      "Test on valid set: loss= 1.29256272315979 acc= 0.7118505239486694 auc= 0.968\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0044\n",
      "\n",
      "Test on train set: loss= 0.0013235615333542228 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2191945314407349 acc= 0.725319504737854 auc= 0.968\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0034\n",
      "\n",
      "Test on train set: loss= 0.0014451346360147 acc= 0.9997572302818298 auc= 0.9999999834662209\n",
      "Test on valid set: loss= 1.246385931968689 acc= 0.7222359776496887 auc= 0.9666666666666668\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0037\n",
      "\n",
      "Test on train set: loss= 0.0013497152831405401 acc= 0.9997572302818298 auc= 0.9999999917331104\n",
      "Test on valid set: loss= 1.306524634361267 acc= 0.7234703302383423 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0042\n",
      "\n",
      "Test on train set: loss= 0.001424271846190095 acc= 0.9997572302818298 auc= 0.9999999917331104\n",
      "Test on valid set: loss= 1.2197482585906982 acc= 0.7225323915481567 auc= 0.9684444444444443\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0033\n",
      "\n",
      "Test on train set: loss= 0.0012267036363482475 acc= 0.99983811378479 auc= 0.9999999917331104\n",
      "Test on valid set: loss= 1.2543967962265015 acc= 0.717690646648407 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0035\n",
      "\n",
      "Test on train set: loss= 0.0011474834755063057 acc= 0.999919056892395 auc= 0.9999999917331104\n",
      "Test on valid set: loss= 1.1821489334106445 acc= 0.732474684715271 auc= 0.9671111111111111\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0035\n",
      "\n",
      "Test on train set: loss= 0.0011568977497518063 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2760133743286133 acc= 0.722322404384613 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0039\n",
      "\n",
      "Test on train set: loss= 0.001117066596634686 acc= 0.999919056892395 auc= 0.9999999917331104\n",
      "Test on valid set: loss= 1.214445948600769 acc= 0.7316635847091675 auc= 0.9688888888888888\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0032\n",
      "\n",
      "Test on train set: loss= 0.001068399171344936 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2528916597366333 acc= 0.7250930666923523 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0036\n",
      "\n",
      "Test on train set: loss= 0.001140058971941471 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2532135248184204 acc= 0.7283998727798462 auc= 0.9702222222222222\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0029\n",
      "\n",
      "Test on train set: loss= 0.0010543169919401407 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2876986265182495 acc= 0.7155518531799316 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0031\n",
      "\n",
      "Test on train set: loss= 0.0010628513991832733 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.239288568496704 acc= 0.7222948670387268 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0034\n",
      "\n",
      "Test on train set: loss= 0.001000777119770646 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2816216945648193 acc= 0.7222385406494141 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0029\n",
      "\n",
      "Test on train set: loss= 0.0009895593393594027 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2378909587860107 acc= 0.7284557223320007 auc= 0.9666666666666668\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0028\n",
      "\n",
      "Test on train set: loss= 0.0009301717509515584 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2383520603179932 acc= 0.734570324420929 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0030\n",
      "\n",
      "Test on train set: loss= 0.0010534183820709586 acc= 0.9997572302818298 auc= 1.0\n",
      "Test on valid set: loss= 1.223006010055542 acc= 0.7337127327919006 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0032\n",
      "\n",
      "Test on train set: loss= 0.0008461734978482127 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2909091711044312 acc= 0.7337071895599365 auc= 0.9684444444444444\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0030\n",
      "\n",
      "Test on train set: loss= 0.0008775359601713717 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3001114130020142 acc= 0.7305467128753662 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0009247823036275804 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3453903198242188 acc= 0.7220049500465393 auc= 0.9671111111111109\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.0007039129268378019 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3071168661117554 acc= 0.7294062972068787 auc= 0.9666666666666666\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0037\n",
      "\n",
      "Test on train set: loss= 0.0009282523533329368 acc= 0.99983811378479 auc= 1.0\n",
      "Test on valid set: loss= 1.2587348222732544 acc= 0.7384927272796631 auc= 0.9675555555555556\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0029\n",
      "\n",
      "Test on train set: loss= 0.0008799343486316502 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.3116422891616821 acc= 0.7300319075584412 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0030\n",
      "\n",
      "Test on train set: loss= 0.0009645405225455761 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3131252527236938 acc= 0.7292959094047546 auc= 0.9653333333333334\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.0007486221147701144 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.284407377243042 acc= 0.7290722727775574 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.0007733185775578022 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.2581202983856201 acc= 0.7291157841682434 auc= 0.9666666666666666\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0008268640376627445 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3081806898117065 acc= 0.7286950945854187 auc= 0.968\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0007037481991574168 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2956151962280273 acc= 0.7296841144561768 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0007358327275142074 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.272847294807434 acc= 0.7323475480079651 auc= 0.9675555555555555\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.000782953342422843 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2397204637527466 acc= 0.7440219521522522 auc= 0.9653333333333334\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0034\n",
      "\n",
      "Test on train set: loss= 0.0007562152459286153 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2968573570251465 acc= 0.726986289024353 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0027\n",
      "\n",
      "Test on train set: loss= 0.0007714732782915235 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3278809785842896 acc= 0.7261959910392761 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0008389606373384595 acc= 0.999919056892395 auc= 1.0\n",
      "Test on valid set: loss= 1.288899540901184 acc= 0.7368165254592896 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0006253632018342614 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.316707968711853 acc= 0.7291373014450073 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0006469681393355131 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2935705184936523 acc= 0.732774555683136 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005909150349907577 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2853987216949463 acc= 0.7350797653198242 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0006635671597905457 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.273757815361023 acc= 0.7403138875961304 auc= 0.9653333333333334\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0006359427352435887 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2955645322799683 acc= 0.7233529686927795 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0006698424112983048 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3373839855194092 acc= 0.7179784178733826 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0006484618061222136 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2952395677566528 acc= 0.7316650152206421 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0025\n",
      "\n",
      "Test on train set: loss= 0.0006009756471030414 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2700597047805786 acc= 0.7424618005752563 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0006569724646396935 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3057855367660522 acc= 0.727128267288208 auc= 0.9644444444444444\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005865902057848871 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2623567581176758 acc= 0.7371335625648499 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0006840860587544739 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3367950916290283 acc= 0.7233258485794067 auc= 0.9640000000000001\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0006044768379069865 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.319059133529663 acc= 0.7190762758255005 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0016\n",
      "\n",
      "Test on train set: loss= 0.000592959753703326 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.314690113067627 acc= 0.7256483435630798 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.000561414984986186 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2846719026565552 acc= 0.7373671531677246 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0006335314246825874 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2988290786743164 acc= 0.7280621528625488 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005567391635850072 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3271489143371582 acc= 0.7275595664978027 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0026\n",
      "\n",
      "Test on train set: loss= 0.0005156454280950129 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3022313117980957 acc= 0.7258431315422058 auc= 0.9666666666666668\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005530850030481815 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.287021279335022 acc= 0.7348522543907166 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005497101810760796 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.302162528038025 acc= 0.7372077703475952 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.000528930511791259 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3112099170684814 acc= 0.7295947670936584 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0005296376766636968 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.291999101638794 acc= 0.7365076541900635 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0017\n",
      "\n",
      "Test on train set: loss= 0.0006089537055231631 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3175766468048096 acc= 0.7308250665664673 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0006057564751245081 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2475086450576782 acc= 0.740912675857544 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0006803811993449926 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2607851028442383 acc= 0.7475279569625854 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0005777322221547365 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3067283630371094 acc= 0.7325560450553894 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.00055274972692132 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.240217685699463 acc= 0.744468092918396 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0005682242335751653 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2996214628219604 acc= 0.7318633198738098 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0006692251772619784 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2885950803756714 acc= 0.7425425052642822 auc= 0.9671111111111111\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.000577468192204833 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3226815462112427 acc= 0.7240526676177979 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0006266490672715008 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2801228761672974 acc= 0.7317565679550171 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0005108827608637512 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3549377918243408 acc= 0.717590868473053 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0006240731454454362 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2361845970153809 acc= 0.7419332265853882 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005660091410391033 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3240296840667725 acc= 0.7321053147315979 auc= 0.9671111111111111\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.000553285120986402 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.311009407043457 acc= 0.7271002531051636 auc= 0.9653333333333334\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0005800959770567715 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2933294773101807 acc= 0.7322865724563599 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005628661601804197 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2597965002059937 acc= 0.7356059551239014 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0017\n",
      "\n",
      "Test on train set: loss= 0.0005678543238900602 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2756147384643555 acc= 0.7351920008659363 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0006794690270908177 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2796560525894165 acc= 0.7332577705383301 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0004891204880550504 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.305932879447937 acc= 0.7259680032730103 auc= 0.9666666666666668\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.000570791307836771 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.254302978515625 acc= 0.7401822209358215 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0026\n",
      "\n",
      "Test on train set: loss= 0.0005100026610307395 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2811074256896973 acc= 0.7393729090690613 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0004896094324067235 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3233140707015991 acc= 0.7254368662834167 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0015\n",
      "\n",
      "Test on train set: loss= 0.0005390102742239833 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.305816650390625 acc= 0.7282280325889587 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005608030478470027 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2926509380340576 acc= 0.7331745028495789 auc= 0.9671111111111111\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0005538463010452688 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3013010025024414 acc= 0.7293573617935181 auc= 0.9648888888888889\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005867978325113654 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2761328220367432 acc= 0.7281813621520996 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0005535241216421127 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2706133127212524 acc= 0.7401431798934937 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005564005696214736 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3355262279510498 acc= 0.7224366068840027 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0005877778748981655 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2828232049942017 acc= 0.7275592088699341 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0023\n",
      "\n",
      "Test on train set: loss= 0.0005864356644451618 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2934203147888184 acc= 0.731315016746521 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0018\n",
      "\n",
      "Test on train set: loss= 0.0006349861505441368 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2373929023742676 acc= 0.7396245002746582 auc= 0.9657777777777777\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0021\n",
      "\n",
      "Test on train set: loss= 0.0005321272183209658 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2471745014190674 acc= 0.7437244653701782 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0017\n",
      "\n",
      "Test on train set: loss= 0.0005095897358842194 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2823562622070312 acc= 0.7308783531188965 auc= 0.9666666666666668\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005109774647280574 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2732727527618408 acc= 0.7277956008911133 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0020\n",
      "\n",
      "Test on train set: loss= 0.0005035648355260491 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3261910676956177 acc= 0.7213865518569946 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005158715648576617 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2609882354736328 acc= 0.7351787090301514 auc= 0.9662222222222221\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 0.0024\n",
      "\n",
      "Test on train set: loss= 0.0005164702888578176 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.2938367128372192 acc= 0.7361137866973877 auc= 0.9653333333333333\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0019\n",
      "\n",
      "Test on train set: loss= 0.0005602895398624241 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3468528985977173 acc= 0.7202554941177368 auc= 0.9662222222222223\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "Baseline\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 0.0022\n",
      "\n",
      "Test on train set: loss= 0.0005850696470588446 acc= 1.0 auc= 1.0\n",
      "Test on valid set: loss= 1.3136906623840332 acc= 0.7288689017295837 auc= 0.968\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "metric_idx = 0\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_Base(train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Baseline')\n",
    "                        \n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "baselineWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancement Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 5s - loss: 1.5090\n",
      "\n",
      "Test on train set: loss= 1.3514435291290283 acc= 0.5666882395744324 auc= 0.7598766139618284\n",
      "Test on valid set: loss= 2.7832343578338623 acc= 0.18894359469413757 auc= 0.6786666666666668\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 3s - loss: -1.5091e+00\n",
      "\n",
      "Test on train set: loss= 4.384975910186768 acc= 0.4679507911205292 auc= 0.7018917956450196\n",
      "Test on valid set: loss= 8.08192253112793 acc= 0.17855356633663177 auc= 0.6564444444444443\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 3s - loss: -3.6232e+00\n",
      "\n",
      "Test on train set: loss= 3.3237667083740234 acc= 0.5702492594718933 auc= 0.771125702463562\n",
      "Test on valid set: loss= 8.093157768249512 acc= 0.19782651960849762 auc= 0.67\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 3s - loss: -9.5885e+00\n",
      "\n",
      "Test on train set: loss= 2.9014039039611816 acc= 0.6566849946975708 auc= 0.8075757576683728\n",
      "Test on valid set: loss= 6.9335856437683105 acc= 0.27805060148239136 auc= 0.7131111111111111\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 3s - loss: -1.1019e+01\n",
      "\n",
      "Test on train set: loss= 3.330199956893921 acc= 0.6796697974205017 auc= 0.8142151299748134\n",
      "Test on valid set: loss= 8.682873725891113 acc= 0.25440019369125366 auc= 0.7055555555555556\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2075e+01\n",
      "\n",
      "Test on train set: loss= 3.8385541439056396 acc= 0.6489964127540588 auc= 0.7963350699387248\n",
      "Test on valid set: loss= 9.728069305419922 acc= 0.2604714334011078 auc= 0.7042222222222222\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3469e+01\n",
      "\n",
      "Test on train set: loss= 3.3203132152557373 acc= 0.7092910408973694 auc= 0.8369381430587062\n",
      "Test on valid set: loss= 8.976017951965332 acc= 0.3359294831752777 auc= 0.7382222222222221\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2891e+01\n",
      "\n",
      "Test on train set: loss= 3.345444440841675 acc= 0.7406118512153625 auc= 0.837913939862778\n",
      "Test on valid set: loss= 9.487329483032227 acc= 0.3250751197338104 auc= 0.7633333333333333\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2449e+01\n",
      "\n",
      "Test on train set: loss= 3.246079206466675 acc= 0.7415021061897278 auc= 0.8568026620363375\n",
      "Test on valid set: loss= 8.469438552856445 acc= 0.3055567145347595 auc= 0.7964444444444444\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.6957e+01\n",
      "\n",
      "Test on train set: loss= 3.711991548538208 acc= 0.7120427489280701 auc= 0.8397743476782491\n",
      "Test on valid set: loss= 8.407703399658203 acc= 0.3958047926425934 auc= 0.7893333333333333\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4332e+01\n",
      "\n",
      "Test on train set: loss= 4.03141450881958 acc= 0.6911621689796448 auc= 0.8485070866661999\n",
      "Test on valid set: loss= 8.783269882202148 acc= 0.3770199418067932 auc= 0.7624444444444445\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.5336e+01\n",
      "\n",
      "Test on train set: loss= 5.098843097686768 acc= 0.6379086971282959 auc= 0.8335461999268963\n",
      "Test on valid set: loss= 10.10122299194336 acc= 0.3392027020454407 auc= 0.756\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0111e+01\n",
      "\n",
      "Test on train set: loss= 5.614452838897705 acc= 0.596390426158905 auc= 0.7391939309242355\n",
      "Test on valid set: loss= 11.24908447265625 acc= 0.2523091435432434 auc= 0.6744444444444444\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0688e+01\n",
      "\n",
      "Test on train set: loss= 4.9013872146606445 acc= 0.661945641040802 auc= 0.7908699770356014\n",
      "Test on valid set: loss= 9.314035415649414 acc= 0.36851373314857483 auc= 0.776\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3711e+01\n",
      "\n",
      "Test on train set: loss= 8.267224311828613 acc= 0.4603431522846222 auc= 0.705087658986654\n",
      "Test on valid set: loss= 11.62549114227295 acc= 0.21160566806793213 auc= 0.6337777777777778\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.4793e+01\n",
      "\n",
      "Test on train set: loss= 3.721021890640259 acc= 0.7421495914459229 auc= 0.8077370682247091\n",
      "Test on valid set: loss= 8.806377410888672 acc= 0.4200533926486969 auc= 0.7599999999999999\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0660e+02\n",
      "\n",
      "Test on train set: loss= 2.846506118774414 acc= 0.799530565738678 auc= 0.8532607153182452\n",
      "Test on valid set: loss= 9.39301586151123 acc= 0.3650410771369934 auc= 0.8071111111111111\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.7656e+01\n",
      "\n",
      "Test on train set: loss= 2.880730152130127 acc= 0.7969407439231873 auc= 0.8944862129259029\n",
      "Test on valid set: loss= 6.649763107299805 acc= 0.510881781578064 auc= 0.8839999999999998\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7236e+01\n",
      "\n",
      "Test on train set: loss= 4.368808269500732 acc= 0.7111524939537048 auc= 0.798204269117225\n",
      "Test on valid set: loss= 10.396345138549805 acc= 0.34000319242477417 auc= 0.7395555555555555\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7795e+01\n",
      "\n",
      "Test on train set: loss= 3.0058562755584717 acc= 0.7918420433998108 auc= 0.8810808632085511\n",
      "Test on valid set: loss= 7.613405227661133 acc= 0.4992228150367737 auc= 0.8666666666666666\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.1593e+01\n",
      "\n",
      "Test on train set: loss= 3.8584816455841064 acc= 0.7351084351539612 auc= 0.8207715427644982\n",
      "Test on valid set: loss= 7.067872524261475 acc= 0.5129410028457642 auc= 0.8202222222222224\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.7290e+01\n",
      "\n",
      "Test on train set: loss= 2.914907455444336 acc= 0.7985594272613525 auc= 0.9013877650921618\n",
      "Test on valid set: loss= 6.964554309844971 acc= 0.5032904744148254 auc= 0.8631111111111112\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.3510e+01\n",
      "\n",
      "Test on train set: loss= 2.8288767337799072 acc= 0.8053576946258545 auc= 0.8857855962973714\n",
      "Test on valid set: loss= 8.63992691040039 acc= 0.4177447259426117 auc= 0.8162222222222223\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.3275e+01\n",
      "\n",
      "Test on train set: loss= 2.8049941062927246 acc= 0.811427652835846 auc= 0.8690474049596976\n",
      "Test on valid set: loss= 7.208391189575195 acc= 0.5039463043212891 auc= 0.7946666666666667\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7968e+01\n",
      "\n",
      "Test on train set: loss= 3.3260340690612793 acc= 0.7723373174667358 auc= 0.8733066119947959\n",
      "Test on valid set: loss= 6.6101813316345215 acc= 0.5415047407150269 auc= 0.8486666666666667\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3596e+01\n",
      "\n",
      "Test on train set: loss= 3.8615670204162598 acc= 0.7421495914459229 auc= 0.8440948732864824\n",
      "Test on valid set: loss= 8.179401397705078 acc= 0.47041574120521545 auc= 0.7973333333333333\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3713e+01\n",
      "\n",
      "Test on train set: loss= 3.0308966636657715 acc= 0.7976691722869873 auc= 0.8216511569366654\n",
      "Test on valid set: loss= 8.906510353088379 acc= 0.41810235381126404 auc= 0.7644444444444445\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.2441e+01\n",
      "\n",
      "Test on train set: loss= 4.256722450256348 acc= 0.714875340461731 auc= 0.8553255978693661\n",
      "Test on valid set: loss= 7.146539211273193 acc= 0.5400115251541138 auc= 0.7917777777777777\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.5408e+01\n",
      "\n",
      "Test on train set: loss= 3.519578695297241 acc= 0.7664292454719543 auc= 0.8074057077643225\n",
      "Test on valid set: loss= 10.481368064880371 acc= 0.2891221344470978 auc= 0.742\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.1668e+01\n",
      "\n",
      "Test on train set: loss= 4.423745632171631 acc= 0.7067821025848389 auc= 0.7735948890164266\n",
      "Test on valid set: loss= 9.839417457580566 acc= 0.37348300218582153 auc= 0.7237777777777779\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0831e+02\n",
      "\n",
      "Test on train set: loss= 2.9922168254852295 acc= 0.8051958680152893 auc= 0.8390416794429362\n",
      "Test on valid set: loss= 7.564062595367432 acc= 0.5199320912361145 auc= 0.8053333333333335\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1296e+02\n",
      "\n",
      "Test on train set: loss= 3.1734869480133057 acc= 0.7937843799591064 auc= 0.8508630883234917\n",
      "Test on valid set: loss= 6.843021392822266 acc= 0.5602204203605652 auc= 0.8528888888888888\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.3028e+01\n",
      "\n",
      "Test on train set: loss= 2.6068851947784424 acc= 0.8296374082565308 auc= 0.8582380462392397\n",
      "Test on valid set: loss= 6.124876022338867 acc= 0.6200000047683716 auc= 0.8795555555555555\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.8158e+01\n",
      "\n",
      "Test on train set: loss= 3.219919204711914 acc= 0.7902233600616455 auc= 0.8600555310530147\n",
      "Test on valid set: loss= 6.758464336395264 acc= 0.5598703622817993 auc= 0.8524444444444444\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1247e+01\n",
      "\n",
      "Test on train set: loss= 2.486232280731201 acc= 0.8276950716972351 auc= 0.902841564341424\n",
      "Test on valid set: loss= 4.423046112060547 acc= 0.658784031867981 auc= 0.8806666666666667\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.7080e+01\n",
      "\n",
      "Test on train set: loss= 2.449568510055542 acc= 0.8395111560821533 auc= 0.8809872734368742\n",
      "Test on valid set: loss= 6.115899562835693 acc= 0.611526370048523 auc= 0.8568888888888889\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.5811e+01\n",
      "\n",
      "Test on train set: loss= 2.327775001525879 acc= 0.8468760251998901 auc= 0.87780481125047\n",
      "Test on valid set: loss= 7.321208477020264 acc= 0.5187710523605347 auc= 0.841777777777778\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.8147e+01\n",
      "\n",
      "Test on train set: loss= 2.1737544536590576 acc= 0.8570734858512878 auc= 0.899167437360847\n",
      "Test on valid set: loss= 6.076850414276123 acc= 0.5988441705703735 auc= 0.8684444444444445\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.7665e+01\n",
      "\n",
      "Test on train set: loss= 2.289999485015869 acc= 0.8507607579231262 auc= 0.8676878837381787\n",
      "Test on valid set: loss= 8.023115158081055 acc= 0.47564077377319336 auc= 0.8197777777777778\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.3780e+01\n",
      "\n",
      "Test on train set: loss= 2.7838797569274902 acc= 0.8162835836410522 auc= 0.8551843223707278\n",
      "Test on valid set: loss= 7.651155948638916 acc= 0.4984775483608246 auc= 0.7813333333333333\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.9673e+01\n",
      "\n",
      "Test on train set: loss= 2.28505802154541 acc= 0.850517988204956 auc= 0.8658233416389173\n",
      "Test on valid set: loss= 6.684366226196289 acc= 0.544247031211853 auc= 0.8668888888888888\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.0891e+01\n",
      "\n",
      "Test on train set: loss= 2.4030816555023193 acc= 0.8406442403793335 auc= 0.9051085500567906\n",
      "Test on valid set: loss= 6.688631057739258 acc= 0.5799658894538879 auc= 0.8542222222222222\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.2590e+01\n",
      "\n",
      "Test on train set: loss= 2.172215461730957 acc= 0.857559084892273 auc= 0.8801889082362578\n",
      "Test on valid set: loss= 7.582951545715332 acc= 0.5160620808601379 auc= 0.7944444444444445\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.9068e+01\n",
      "\n",
      "Test on train set: loss= 2.1513984203338623 acc= 0.8584493398666382 auc= 0.8968858345631064\n",
      "Test on valid set: loss= 4.831902503967285 acc= 0.6688758134841919 auc= 0.8744444444444444\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0641e+02\n",
      "\n",
      "Test on train set: loss= 2.5267491340637207 acc= 0.8362738490104675 auc= 0.8281482957783404\n",
      "Test on valid set: loss= 6.447440147399902 acc= 0.599798858165741 auc= 0.834\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.3488e+01\n",
      "\n",
      "Test on train set: loss= 2.5771031379699707 acc= 0.8320654034614563 auc= 0.8651480105120595\n",
      "Test on valid set: loss= 6.355291843414307 acc= 0.5983383655548096 auc= 0.8255555555555555\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.1106e+01\n",
      "\n",
      "Test on train set: loss= 2.1610264778137207 acc= 0.8565878868103027 auc= 0.9029993020473783\n",
      "Test on valid set: loss= 6.125375747680664 acc= 0.6195058822631836 auc= 0.8364444444444444\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.7201e+01\n",
      "\n",
      "Test on train set: loss= 2.1953155994415283 acc= 0.845661997795105 auc= 0.8622762103646373\n",
      "Test on valid set: loss= 6.874985218048096 acc= 0.5473192930221558 auc= 0.8297777777777778\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1344e+02\n",
      "\n",
      "Test on train set: loss= 2.605307102203369 acc= 0.831256091594696 auc= 0.8513922407825408\n",
      "Test on valid set: loss= 8.703771591186523 acc= 0.46000003814697266 auc= 0.7513333333333333\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.6985e+01\n",
      "\n",
      "Test on train set: loss= 2.1279830932617188 acc= 0.8611201047897339 auc= 0.902343355251792\n",
      "Test on valid set: loss= 6.791516304016113 acc= 0.5642440319061279 auc= 0.852888888888889\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4398e+02\n",
      "\n",
      "Test on train set: loss= 2.6027042865753174 acc= 0.8315798044204712 auc= 0.8786250548342887\n",
      "Test on valid set: loss= 5.803909778594971 acc= 0.638651967048645 auc= 0.8486666666666667\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4997e+02\n",
      "\n",
      "Test on train set: loss= 2.470045328140259 acc= 0.8404014110565186 auc= 0.8893795525852559\n",
      "Test on valid set: loss= 6.757736682891846 acc= 0.5614515542984009 auc= 0.8184444444444445\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3520e+02\n",
      "\n",
      "Test on train set: loss= 1.9928511381149292 acc= 0.8715603947639465 auc= 0.8625660382733438\n",
      "Test on valid set: loss= 7.426855564117432 acc= 0.5306884050369263 auc= 0.805111111111111\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3507e+02\n",
      "\n",
      "Test on train set: loss= 2.3020684719085693 acc= 0.8485755920410156 auc= 0.8782113410706665\n",
      "Test on valid set: loss= 5.480969429016113 acc= 0.6591994762420654 auc= 0.874\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.2031e+02\n",
      "\n",
      "Test on train set: loss= 1.9393277168273926 acc= 0.8740692734718323 auc= 0.9040547728199468\n",
      "Test on valid set: loss= 4.535416603088379 acc= 0.6801384687423706 auc= 0.8871111111111112\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.5586e+01\n",
      "\n",
      "Test on train set: loss= 1.7842252254486084 acc= 0.8853188753128052 auc= 0.880314962555435\n",
      "Test on valid set: loss= 5.517256259918213 acc= 0.6431283354759216 auc= 0.8506666666666666\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2650e+02\n",
      "\n",
      "Test on train set: loss= 1.62717866897583 acc= 0.8947879672050476 auc= 0.8917150970259341\n",
      "Test on valid set: loss= 6.316633701324463 acc= 0.5998597741127014 auc= 0.8344444444444443\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4032e+02\n",
      "\n",
      "Test on train set: loss= 1.9532592296600342 acc= 0.8749595284461975 auc= 0.8616890161377334\n",
      "Test on valid set: loss= 5.078197956085205 acc= 0.6434366106987 auc= 0.8704444444444445\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2222e+02\n",
      "\n",
      "Test on train set: loss= 2.2173140048980713 acc= 0.8569925427436829 auc= 0.8665886110583987\n",
      "Test on valid set: loss= 5.273393630981445 acc= 0.6600617170333862 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -8.5594e+01\n",
      "\n",
      "Test on train set: loss= 2.040543556213379 acc= 0.8637908697128296 auc= 0.9179457462541084\n",
      "Test on valid set: loss= 5.448542594909668 acc= 0.6419417262077332 auc= 0.8735555555555555\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1426e+02\n",
      "\n",
      "Test on train set: loss= 1.902056097984314 acc= 0.8779540061950684 auc= 0.8708425255932685\n",
      "Test on valid set: loss= 6.124886512756348 acc= 0.6199899911880493 auc= 0.85\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1852e+02\n",
      "\n",
      "Test on train set: loss= 2.5777828693389893 acc= 0.8213823437690735 auc= 0.8820079578311779\n",
      "Test on valid set: loss= 6.172239780426025 acc= 0.6018718481063843 auc= 0.8473333333333335\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3620e+02\n",
      "\n",
      "Test on train set: loss= 1.9663934707641602 acc= 0.8716412782669067 auc= 0.8647590660160702\n",
      "Test on valid set: loss= 6.816565036773682 acc= 0.5523585081100464 auc= 0.8119999999999999\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3643e+02\n",
      "\n",
      "Test on train set: loss= 1.731083631515503 acc= 0.8896892070770264 auc= 0.8794797064970631\n",
      "Test on valid set: loss= 5.9786696434021 acc= 0.6200030446052551 auc= 0.8373333333333333\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.6834e+02\n",
      "\n",
      "Test on train set: loss= 2.242126941680908 acc= 0.857316255569458 auc= 0.8657031923751367\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5799999237060547 auc= 0.8093333333333333\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7527e+02\n",
      "\n",
      "Test on train set: loss= 2.2324490547180176 acc= 0.857316255569458 auc= 0.8584745831497937\n",
      "Test on valid set: loss= 6.1529221534729 acc= 0.6049206852912903 auc= 0.8204444444444444\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0489e+02\n",
      "\n",
      "Test on train set: loss= 2.444066286087036 acc= 0.8404823541641235 auc= 0.889180622088056\n",
      "Test on valid set: loss= 5.1663498878479 acc= 0.6341952681541443 auc= 0.8464444444444444\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.0039e+02\n",
      "\n",
      "Test on train set: loss= 2.19275164604187 acc= 0.8603107929229736 auc= 0.8664321006173511\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8404444444444443\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.4506e+02\n",
      "\n",
      "Test on train set: loss= 1.6931475400924683 acc= 0.8892036080360413 auc= 0.8849924604962366\n",
      "Test on valid set: loss= 6.064611434936523 acc= 0.60434889793396 auc= 0.8568888888888889\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.9843e+02\n",
      "\n",
      "Test on train set: loss= 2.4783122539520264 acc= 0.8413726091384888 auc= 0.8533823410591603\n",
      "Test on valid set: loss= 7.801626205444336 acc= 0.5000027418136597 auc= 0.7813333333333334\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.6135e+02\n",
      "\n",
      "Test on train set: loss= 1.9170029163360596 acc= 0.8772256374359131 auc= 0.8680040037111926\n",
      "Test on valid set: loss= 5.880831718444824 acc= 0.6076231598854065 auc= 0.8140000000000001\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8476e+02\n",
      "\n",
      "Test on train set: loss= 1.7426456212997437 acc= 0.8877468705177307 auc= 0.8942966581517664\n",
      "Test on valid set: loss= 5.480159282684326 acc= 0.659993052482605 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7520e+02\n",
      "\n",
      "Test on train set: loss= 1.8947842121124268 acc= 0.8793298602104187 auc= 0.8745939586262494\n",
      "Test on valid set: loss= 5.025148391723633 acc= 0.6465623378753662 auc= 0.8602222222222222\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4408e+02\n",
      "\n",
      "Test on train set: loss= 1.7930227518081665 acc= 0.8856425881385803 auc= 0.8613393236726802\n",
      "Test on valid set: loss= 6.7696003913879395 acc= 0.5799999833106995 auc= 0.8411111111111111\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.5809e+02\n",
      "\n",
      "Test on train set: loss= 1.5841947793960571 acc= 0.8989155292510986 auc= 0.9112873059070499\n",
      "Test on valid set: loss= 5.802690505981445 acc= 0.6398246884346008 auc= 0.834\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8312e+02\n",
      "\n",
      "Test on train set: loss= 2.166653871536255 acc= 0.8529459238052368 auc= 0.9026533493112842\n",
      "Test on valid set: loss= 5.658454418182373 acc= 0.6399999856948853 auc= 0.8326666666666667\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.0389e+01\n",
      "\n",
      "Test on train set: loss= 4.011336326599121 acc= 0.7370508313179016 auc= 0.7930662128370256\n",
      "Test on valid set: loss= 8.256739616394043 acc= 0.4688265919685364 auc= 0.79\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3487e+02\n",
      "\n",
      "Test on train set: loss= 2.3673641681671143 acc= 0.8474425673484802 auc= 0.8408423879449863\n",
      "Test on valid set: loss= 6.769603252410889 acc= 0.5799970030784607 auc= 0.7884444444444444\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.4098e+02\n",
      "\n",
      "Test on train set: loss= 2.680295705795288 acc= 0.8294755816459656 auc= 0.8340004737883417\n",
      "Test on valid set: loss= 8.772945404052734 acc= 0.4402042031288147 auc= 0.742\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0661e+02\n",
      "\n",
      "Test on train set: loss= 2.687880039215088 acc= 0.8300420641899109 auc= 0.8194704884124471\n",
      "Test on valid set: loss= 8.059060096740723 acc= 0.49998703598976135 auc= 0.7942222222222222\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.5093e+02\n",
      "\n",
      "Test on train set: loss= 2.414313793182373 acc= 0.8454192280769348 auc= 0.8528464273940439\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5800000429153442 auc= 0.8122222222222222\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9884e+02\n",
      "\n",
      "Test on train set: loss= 2.029855489730835 acc= 0.8695370554924011 auc= 0.8696345491826207\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8351111111111111\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0603e+02\n",
      "\n",
      "Test on train set: loss= 1.606873869895935 acc= 0.8974587321281433 auc= 0.8795883937448954\n",
      "Test on valid set: loss= 5.740306854248047 acc= 0.6353048086166382 auc= 0.8264444444444445\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7707e+02\n",
      "\n",
      "Test on train set: loss= 1.7257508039474487 acc= 0.8897701501846313 auc= 0.9037178712349148\n",
      "Test on valid set: loss= 3.9199390411376953 acc= 0.7375987768173218 auc= 0.8808888888888889\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.8046e+02\n",
      "\n",
      "Test on train set: loss= 1.7731064558029175 acc= 0.8866947293281555 auc= 0.909745737134984\n",
      "Test on valid set: loss= 4.845612049102783 acc= 0.6918394565582275 auc= 0.8788888888888889\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4427e+02\n",
      "\n",
      "Test on train set: loss= 1.6532615423202515 acc= 0.8945451378822327 auc= 0.8737318564762475\n",
      "Test on valid set: loss= 6.2875657081604 acc= 0.6000059247016907 auc= 0.8135555555555556\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9054e+02\n",
      "\n",
      "Test on train set: loss= 1.6617028713226318 acc= 0.8940595388412476 auc= 0.9031048348049552\n",
      "Test on valid set: loss= 5.480262279510498 acc= 0.6598902940750122 auc= 0.8195555555555556\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.3217e+02\n",
      "\n",
      "Test on train set: loss= 2.071620225906372 acc= 0.8638718128204346 auc= 0.9168968949201597\n",
      "Test on valid set: loss= 4.835482597351074 acc= 0.6999460458755493 auc= 0.8706666666666667\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.2689e+02\n",
      "\n",
      "Test on train set: loss= 1.41130793094635 acc= 0.9092748165130615 auc= 0.9078647464626826\n",
      "Test on valid set: loss= 5.80251407623291 acc= 0.6399999856948853 auc= 0.8271111111111111\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.4943e+02\n",
      "\n",
      "Test on train set: loss= 2.356276273727417 acc= 0.8451764583587646 auc= 0.8832059308033129\n",
      "Test on valid set: loss= 6.470439434051514 acc= 0.5862694382667542 auc= 0.8219999999999998\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9820e+02\n",
      "\n",
      "Test on train set: loss= 1.5192843675613403 acc= 0.9011816382408142 auc= 0.8990305231237764\n",
      "Test on valid set: loss= 6.555142879486084 acc= 0.5800896883010864 auc= 0.7924444444444444\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.1473e+02\n",
      "\n",
      "Test on train set: loss= 1.6559135913848877 acc= 0.8938167691230774 auc= 0.8462916932018439\n",
      "Test on valid set: loss= 6.769732475280762 acc= 0.579867422580719 auc= 0.7964444444444445\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.7529e+02\n",
      "\n",
      "Test on train set: loss= 1.5906381607055664 acc= 0.8988345861434937 auc= 0.8783557413430326\n",
      "Test on valid set: loss= 5.333278656005859 acc= 0.6600030660629272 auc= 0.8513333333333334\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.9591e+02\n",
      "\n",
      "Test on train set: loss= 1.8168869018554688 acc= 0.884914219379425 auc= 0.88758631041346\n",
      "Test on valid set: loss= 6.769600868225098 acc= 0.579999566078186 auc= 0.7835555555555556\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.0412e+02\n",
      "\n",
      "Test on train set: loss= 1.5850168466567993 acc= 0.8956782221794128 auc= 0.9231484191871469\n",
      "Test on valid set: loss= 6.125041484832764 acc= 0.6198353171348572 auc= 0.8124444444444444\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.2858e+02\n",
      "\n",
      "Test on train set: loss= 1.3725712299346924 acc= 0.9132405519485474 auc= 0.8890372976029386\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8491111111111111\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3546e+02\n",
      "\n",
      "Test on train set: loss= 1.4307979345321655 acc= 0.9089511036872864 auc= 0.9076015999694566\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8079999999999998\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.6303e+02\n",
      "\n",
      "Test on train set: loss= 1.318541169166565 acc= 0.9156684875488281 auc= 0.9178762225818213\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8153333333333332\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5973e+02\n",
      "\n",
      "Test on train set: loss= 1.270290493965149 acc= 0.919472336769104 auc= 0.9082471189606063\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8284444444444444\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.9611e+02\n",
      "\n",
      "Test on train set: loss= 2.300079345703125 acc= 0.8459857702255249 auc= 0.8754991284392484\n",
      "Test on valid set: loss= 5.931734085083008 acc= 0.6200074553489685 auc= 0.8160000000000001\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.0807e+02\n",
      "\n",
      "Test on train set: loss= 1.3486647605895996 acc= 0.9120265245437622 auc= 0.9119847520462498\n",
      "Test on valid set: loss= 6.117905139923096 acc= 0.6167823076248169 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.2568e+02\n",
      "\n",
      "Test on train set: loss= 1.0850437879562378 acc= 0.930479109287262 auc= 0.9353865920498095\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.7000000476837158 auc= 0.8306666666666667\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.7681e+02\n",
      "\n",
      "Test on train set: loss= 1.3478240966796875 acc= 0.9139689207077026 auc= 0.9138583996173113\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8266666666666665\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.0867e+02\n",
      "\n",
      "Test on train set: loss= 1.7845622301101685 acc= 0.8871803283691406 auc= 0.8871866949143765\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.8062222222222223\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.6530e+02\n",
      "\n",
      "Test on train set: loss= 1.80409836769104 acc= 0.884914219379425 auc= 0.901858453981845\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.7975555555555556\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.4443e+02\n",
      "\n",
      "Test on train set: loss= 1.4423613548278809 acc= 0.9083036780357361 auc= 0.9193107016807431\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.8155555555555555\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8670e+02\n",
      "\n",
      "Test on train set: loss= 1.8800272941589355 acc= 0.880786657333374 auc= 0.8555937641266149\n",
      "Test on valid set: loss= 8.05904769897461 acc= 0.5 auc= 0.7597777777777777\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6552e+02\n",
      "\n",
      "Test on train set: loss= 1.3428562879562378 acc= 0.9146972894668579 auc= 0.9164649169642661\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8393333333333335\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3176e+02\n",
      "\n",
      "Test on train set: loss= 1.5391004085540771 acc= 0.9028812050819397 auc= 0.8768322287398492\n",
      "Test on valid set: loss= 5.481054306030273 acc= 0.6591182947158813 auc= 0.8299999999999998\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.2765e+02\n",
      "\n",
      "Test on train set: loss= 1.5136938095092773 acc= 0.9036905169487 auc= 0.9315228051246855\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8493333333333333\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.8127e+02\n",
      "\n",
      "Test on train set: loss= 1.3283573389053345 acc= 0.9156684875488281 auc= 0.910152253745401\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8544444444444445\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1894e+02\n",
      "\n",
      "Test on train set: loss= 1.5287209749221802 acc= 0.9030430316925049 auc= 0.9127635756240073\n",
      "Test on valid set: loss= 4.209274768829346 acc= 0.7279028296470642 auc= 0.8846666666666667\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3316e+02\n",
      "\n",
      "Test on train set: loss= 1.602501392364502 acc= 0.892683744430542 auc= 0.9289366151575196\n",
      "Test on valid set: loss= 5.653851509094238 acc= 0.6049298048019409 auc= 0.836\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3450e+02\n",
      "\n",
      "Test on train set: loss= 1.2198535203933716 acc= 0.9217384457588196 auc= 0.9217689838377098\n",
      "Test on valid set: loss= 4.513067245483398 acc= 0.7200000286102295 auc= 0.8424444444444443\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6852e+02\n",
      "\n",
      "Test on train set: loss= 1.3449410200119019 acc= 0.915101945400238 auc= 0.8796731299737408\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.64000004529953 auc= 0.815111111111111\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3878e+02\n",
      "\n",
      "Test on train set: loss= 1.1189589500427246 acc= 0.9289414286613464 auc= 0.9169333805213629\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8155555555555555\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1871e+02\n",
      "\n",
      "Test on train set: loss= 1.059597373008728 acc= 0.9329880475997925 auc= 0.9094167462430626\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7962222222222223\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7204e+02\n",
      "\n",
      "Test on train set: loss= 1.3092069625854492 acc= 0.9173680543899536 auc= 0.8824603977493523\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8186666666666665\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7192e+02\n",
      "\n",
      "Test on train set: loss= 1.1253381967544556 acc= 0.9278083443641663 auc= 0.9289572862015678\n",
      "Test on valid set: loss= 5.481165885925293 acc= 0.6590117812156677 auc= 0.8153333333333332\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.8500e+02\n",
      "\n",
      "Test on train set: loss= 1.5115329027175903 acc= 0.9025574326515198 auc= 0.9353511501107908\n",
      "Test on valid set: loss= 5.802518367767334 acc= 0.6399962902069092 auc= 0.806\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.0715e+02\n",
      "\n",
      "Test on train set: loss= 1.089652419090271 acc= 0.9314503073692322 auc= 0.93410277647822\n",
      "Test on valid set: loss= 5.089310646057129 acc= 0.6613142490386963 auc= 0.8402222222222221\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3268e+02\n",
      "\n",
      "Test on train set: loss= 1.3104082345962524 acc= 0.9139689207077026 auc= 0.9275223088065226\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8231111111111111\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7907e+02\n",
      "\n",
      "Test on train set: loss= 1.2122241258621216 acc= 0.9238426685333252 auc= 0.8846933293488984\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8211111111111112\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.1769e+02\n",
      "\n",
      "Test on train set: loss= 1.0681121349334717 acc= 0.9319359064102173 auc= 0.9159737341071548\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7399997711181641 auc= 0.8560000000000001\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.6180e+02\n",
      "\n",
      "Test on train set: loss= 1.1082566976547241 acc= 0.9296697974205017 auc= 0.9201149559107543\n",
      "Test on valid set: loss= 4.800112724304199 acc= 0.699999988079071 auc= 0.8702222222222222\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3001e+02\n",
      "\n",
      "Test on train set: loss= 1.304635763168335 acc= 0.9167206287384033 auc= 0.8909689661823492\n",
      "Test on valid set: loss= 6.448155403137207 acc= 0.5991033315658569 auc= 0.7757777777777777\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.3660e+02\n",
      "\n",
      "Test on train set: loss= 0.9841476678848267 acc= 0.9375202059745789 auc= 0.91023316836889\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.7871111111111111\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.9718e+02\n",
      "\n",
      "Test on train set: loss= 1.0174578428268433 acc= 0.9358206391334534 auc= 0.9246780954885285\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8162222222222221\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.7882e+02\n",
      "\n",
      "Test on train set: loss= 1.2746275663375854 acc= 0.9198769927024841 auc= 0.8981363267217297\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8102222222222222\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9871e+02\n",
      "\n",
      "Test on train set: loss= 1.1870683431625366 acc= 0.9233570694923401 auc= 0.9355326797732756\n",
      "Test on valid set: loss= 4.526837348937988 acc= 0.7094812989234924 auc= 0.8520000000000001\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3109e+02\n",
      "\n",
      "Test on train set: loss= 1.2186063528060913 acc= 0.9223858714103699 auc= 0.9427194527861855\n",
      "Test on valid set: loss= 5.295652866363525 acc= 0.6600186228752136 auc= 0.8402222222222221\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.6134e+02\n",
      "\n",
      "Test on train set: loss= 1.8563337326049805 acc= 0.8831337094306946 auc= 0.8853991066101774\n",
      "Test on valid set: loss= 7.091961860656738 acc= 0.559999942779541 auc= 0.7737777777777778\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.2548e+02\n",
      "\n",
      "Test on train set: loss= 1.1049163341522217 acc= 0.930398166179657 auc= 0.9323255470229876\n",
      "Test on valid set: loss= 5.88469123840332 acc= 0.6203285455703735 auc= 0.8175555555555555\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.0574e+02\n",
      "\n",
      "Test on train set: loss= 1.1084634065628052 acc= 0.9301553964614868 auc= 0.9082857589025354\n",
      "Test on valid set: loss= 5.502211093902588 acc= 0.6466377973556519 auc= 0.8204444444444444\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.5582e+02\n",
      "\n",
      "Test on train set: loss= 1.4819748401641846 acc= 0.9065231680870056 auc= 0.9250888589737262\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.64000004529953 auc= 0.8086666666666666\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.0659e+02\n",
      "\n",
      "Test on train set: loss= 1.3575512170791626 acc= 0.9129977226257324 auc= 0.9199211166813619\n",
      "Test on valid set: loss= 5.80264949798584 acc= 0.639865517616272 auc= 0.798\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5706e+02\n",
      "\n",
      "Test on train set: loss= 1.3505154848098755 acc= 0.9145354628562927 auc= 0.9235148887006386\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8517777777777779\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.4856e+02\n",
      "\n",
      "Test on train set: loss= 1.156213641166687 acc= 0.927160918712616 auc= 0.9014313283866446\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8091111111111111\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.2805e+02\n",
      "\n",
      "Test on train set: loss= 1.0761131048202515 acc= 0.9308028221130371 auc= 0.9251980555620547\n",
      "Test on valid set: loss= 4.513083457946777 acc= 0.7199830412864685 auc= 0.8733333333333333\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9036e+02\n",
      "\n",
      "Test on train set: loss= 1.640244722366333 acc= 0.8970540761947632 auc= 0.9150314801824647\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.841777777777778\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.2903e+02\n",
      "\n",
      "Test on train set: loss= 1.0014867782592773 acc= 0.9356588125228882 auc= 0.9438531920119267\n",
      "Test on valid set: loss= 4.989634990692139 acc= 0.6800002455711365 auc= 0.852\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.7117e+02\n",
      "\n",
      "Test on train set: loss= 1.3376322984695435 acc= 0.9142116904258728 auc= 0.948289979297164\n",
      "Test on valid set: loss= 3.5468034744262695 acc= 0.779194176197052 auc= 0.8962222222222221\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.2146e+02\n",
      "\n",
      "Test on train set: loss= 0.9815180897712708 acc= 0.9376820921897888 auc= 0.939496505948236\n",
      "Test on valid set: loss= 4.835484027862549 acc= 0.6999446749687195 auc= 0.8524444444444444\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.5172e+02\n",
      "\n",
      "Test on train set: loss= 1.1078884601593018 acc= 0.9292651414871216 auc= 0.9266109997287753\n",
      "Test on valid set: loss= 5.224752426147461 acc= 0.6607030034065247 auc= 0.8304444444444444\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.4381e+02\n",
      "\n",
      "Test on train set: loss= 1.319027304649353 acc= 0.9173680543899536 auc= 0.9162202873339265\n",
      "Test on valid set: loss= 6.747506618499756 acc= 0.5800000429153442 auc= 0.776\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7901e+02\n",
      "\n",
      "Test on train set: loss= 0.7858364582061768 acc= 0.9499028921127319 auc= 0.9489180319725337\n",
      "Test on valid set: loss= 6.447238922119141 acc= 0.6000000238418579 auc= 0.7928888888888889\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.8859e+02\n",
      "\n",
      "Test on train set: loss= 0.8191236257553101 acc= 0.9481223821640015 auc= 0.94204190792303\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.815111111111111\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.8763e+02\n",
      "\n",
      "Test on train set: loss= 0.8760107159614563 acc= 0.9450469613075256 auc= 0.9305227419773938\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7973333333333333\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.4771e+02\n",
      "\n",
      "Test on train set: loss= 1.1110409498214722 acc= 0.9297507405281067 auc= 0.924092661472522\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.842\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.4866e+02\n",
      "\n",
      "Test on train set: loss= 1.05393648147583 acc= 0.9337164163589478 auc= 0.9179248512089654\n",
      "Test on valid set: loss= 6.66630744934082 acc= 0.5800003409385681 auc= 0.7751111111111111\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.9771e+02\n",
      "\n",
      "Test on train set: loss= 0.8438870310783386 acc= 0.9468274712562561 auc= 0.9372974474578084\n",
      "Test on valid set: loss= 5.157793045043945 acc= 0.6799977421760559 auc= 0.8293333333333333\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.2942e+02\n",
      "\n",
      "Test on train set: loss= 1.0829473733901978 acc= 0.9314503073692322 auc= 0.932500297744762\n",
      "Test on valid set: loss= 6.500675201416016 acc= 0.5813825130462646 auc= 0.8073333333333335\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.0082e+02\n",
      "\n",
      "Test on train set: loss= 0.8955520987510681 acc= 0.9432664513587952 auc= 0.9326289765930567\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.8157777777777777\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.6138e+02\n",
      "\n",
      "Test on train set: loss= 0.9436059594154358 acc= 0.9404338002204895 auc= 0.9353587988397235\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8311111111111111\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.1609e+02\n",
      "\n",
      "Test on train set: loss= 1.0333526134490967 acc= 0.934687614440918 auc= 0.8981521198742908\n",
      "Test on valid set: loss= 6.124876022338867 acc= 0.6200000047683716 auc= 0.7984444444444445\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.4715e+02\n",
      "\n",
      "Test on train set: loss= 1.0689257383346558 acc= 0.9325024485588074 auc= 0.9363212955771438\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8217777777777778\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.2340e+02\n",
      "\n",
      "Test on train set: loss= 1.1682409048080444 acc= 0.9264324903488159 auc= 0.9243679893343246\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8733333333333334\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.2076e+02\n",
      "\n",
      "Test on train set: loss= 1.2805116176605225 acc= 0.919553279876709 auc= 0.9135748209675544\n",
      "Test on valid set: loss= 7.414323806762695 acc= 0.5399999618530273 auc= 0.7433333333333334\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.9801e+02\n",
      "\n",
      "Test on train set: loss= 0.9154347777366638 acc= 0.942295253276825 auc= 0.9586630001028255\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7991111111111111\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.8602e+02\n",
      "\n",
      "Test on train set: loss= 1.6088703870773315 acc= 0.8984299302101135 auc= 0.910912907208945\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8084444444444445\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4178e+02\n",
      "\n",
      "Test on train set: loss= 0.9436211585998535 acc= 0.9399482011795044 auc= 0.9199610223818192\n",
      "Test on valid set: loss= 4.835451126098633 acc= 0.6999775171279907 auc= 0.8315555555555555\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.5433e+02\n",
      "\n",
      "Test on train set: loss= 1.0251559019088745 acc= 0.9354969263076782 auc= 0.9187900952937064\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.7000000476837158 auc= 0.8326666666666668\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.3368e+02\n",
      "\n",
      "Test on train set: loss= 0.7745534777641296 acc= 0.9511168599128723 auc= 0.9241648638529496\n",
      "Test on valid set: loss= 5.69448184967041 acc= 0.6399999260902405 auc= 0.82\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.0281e+02\n",
      "\n",
      "Test on train set: loss= 0.8354343771934509 acc= 0.9473939538002014 auc= 0.9169418275350758\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8091111111111111\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.6175e+02\n",
      "\n",
      "Test on train set: loss= 0.9504619240760803 acc= 0.938815176486969 auc= 0.9396207268395862\n",
      "Test on valid set: loss= 4.913390636444092 acc= 0.6804055571556091 auc= 0.85\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.9850e+02\n",
      "\n",
      "Test on train set: loss= 1.3864045143127441 acc= 0.9129167795181274 auc= 0.9183132421360638\n",
      "Test on valid set: loss= 5.514904975891113 acc= 0.6435188055038452 auc= 0.8313333333333335\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.6471e+02\n",
      "\n",
      "Test on train set: loss= 1.2216601371765137 acc= 0.922871470451355 auc= 0.9094334819831718\n",
      "Test on valid set: loss= 4.672846794128418 acc= 0.7000068426132202 auc= 0.8433333333333334\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.9191e+02\n",
      "\n",
      "Test on train set: loss= 1.572149395942688 acc= 0.9019909501075745 auc= 0.8699725887203617\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5799999833106995 auc= 0.7755555555555556\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.0560e+02\n",
      "\n",
      "Test on train set: loss= 0.9963751435279846 acc= 0.9372774362564087 auc= 0.9453482788201475\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8646666666666667\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.5303e+02\n",
      "\n",
      "Test on train set: loss= 0.8414214849472046 acc= 0.9459372162818909 auc= 0.9348707976278519\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8068888888888889\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.9881e+02\n",
      "\n",
      "Test on train set: loss= 1.0085318088531494 acc= 0.9352541565895081 auc= 0.95727300017295\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.8322222222222223\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.5056e+02\n",
      "\n",
      "Test on train set: loss= 0.8762669563293457 acc= 0.9445613622665405 auc= 0.9287500179633646\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8204444444444444\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.1957e+02\n",
      "\n",
      "Test on train set: loss= 0.907349705696106 acc= 0.9424570798873901 auc= 0.9400458047242581\n",
      "Test on valid set: loss= 5.481407642364502 acc= 0.6587838530540466 auc= 0.8104444444444445\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.1036e+02\n",
      "\n",
      "Test on train set: loss= 1.0924640893936157 acc= 0.931207537651062 auc= 0.9262097403565669\n",
      "Test on valid set: loss= 6.124876022338867 acc= 0.6200000047683716 auc= 0.7984444444444445\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.3303e+02\n",
      "\n",
      "Test on train set: loss= 1.054985761642456 acc= 0.9337973594665527 auc= 0.9239068028931472\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.7991111111111111\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.5504e+02\n",
      "\n",
      "Test on train set: loss= 0.765663743019104 acc= 0.9517643451690674 auc= 0.9527907576036887\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8437777777777777\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3065e+02\n",
      "\n",
      "Test on train set: loss= 0.7289940118789673 acc= 0.953787624835968 auc= 0.9581426467098476\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8282222222222222\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.1952e+02\n",
      "\n",
      "Test on train set: loss= 0.8654382228851318 acc= 0.9452897310256958 auc= 0.9458237118613356\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.8311111111111111\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3357e+02\n",
      "\n",
      "Test on train set: loss= 0.9282774925231934 acc= 0.9416477680206299 auc= 0.9386665496519191\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.6999999284744263 auc= 0.8328888888888889\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.3686e+02\n",
      "\n",
      "Test on train set: loss= 0.8278211355209351 acc= 0.9474748969078064 auc= 0.9444312231782289\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8848888888888891\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.5281e+02\n",
      "\n",
      "Test on train set: loss= 1.0194706916809082 acc= 0.9356588125228882 auc= 0.9246875007689775\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5800000429153442 auc= 0.768\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.7753e+02\n",
      "\n",
      "Test on train set: loss= 0.83116215467453 acc= 0.9475558400154114 auc= 0.9411871649464609\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8200000000000001\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.0348e+02\n",
      "\n",
      "Test on train set: loss= 0.6490824222564697 acc= 0.9578342437744141 auc= 0.9572132724276889\n",
      "Test on valid set: loss= 5.158185958862305 acc= 0.6796083450317383 auc= 0.8415555555555555\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.5049e+02\n",
      "\n",
      "Test on train set: loss= 0.8658347129821777 acc= 0.9443185329437256 auc= 0.9556896950512026\n",
      "Test on valid set: loss= 5.157795429229736 acc= 0.6799947619438171 auc= 0.8304444444444444\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.8900e+02\n",
      "\n",
      "Test on train set: loss= 0.8956590890884399 acc= 0.9437520503997803 auc= 0.9377104650521559\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.6645e+02\n",
      "\n",
      "Test on train set: loss= 0.7450201511383057 acc= 0.9528973698616028 auc= 0.9364333642799595\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.7000000476837158 auc= 0.8328888888888889\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.6773e+02\n",
      "\n",
      "Test on train set: loss= 0.8021900057792664 acc= 0.949660062789917 auc= 0.8996331078662841\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8220000000000001\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.9489e+02\n",
      "\n",
      "Test on train set: loss= 0.8162447214126587 acc= 0.9486889243125916 auc= 0.9422381141219036\n",
      "Test on valid set: loss= 5.802621841430664 acc= 0.6398928165435791 auc= 0.7991111111111111\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.0332e+02\n",
      "\n",
      "Test on train set: loss= 0.6185603737831116 acc= 0.9609906077384949 auc= 0.9476140737393102\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8193333333333334\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.2841e+02\n",
      "\n",
      "Test on train set: loss= 0.6266676783561707 acc= 0.9601812958717346 auc= 0.948314999361528\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8313333333333333\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.9760e+02\n",
      "\n",
      "Test on train set: loss= 0.7366098165512085 acc= 0.9533829689025879 auc= 0.9472704378679249\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8302222222222222\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.3903e+02\n",
      "\n",
      "Test on train set: loss= 0.7648602724075317 acc= 0.9520880579948425 auc= 0.9474449868127526\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7979999999999999\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.7414e+02\n",
      "\n",
      "Test on train set: loss= 0.6822913885116577 acc= 0.9565393328666687 auc= 0.9644152596429212\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000691413879 auc= 0.8655555555555556\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.6738e+02\n",
      "\n",
      "Test on train set: loss= 0.67143714427948 acc= 0.957915186882019 auc= 0.9264200933736708\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8295555555555556\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.2140e+02\n",
      "\n",
      "Test on train set: loss= 0.6524458527565002 acc= 0.9591291546821594 auc= 0.9533476458217855\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8102222222222222\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.0053e+02\n",
      "\n",
      "Test on train set: loss= 0.8127906918525696 acc= 0.9486889243125916 auc= 0.9496994490839998\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8542222222222222\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.4533e+02\n",
      "\n",
      "Test on train set: loss= 0.829918622970581 acc= 0.9478795528411865 auc= 0.948434775275142\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.2171e+02\n",
      "\n",
      "Test on train set: loss= 0.7425824999809265 acc= 0.9533020257949829 auc= 0.9353924891885119\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8202222222222222\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.4359e+02\n",
      "\n",
      "Test on train set: loss= 0.6228715777397156 acc= 0.9605859518051147 auc= 0.9547110758594485\n",
      "Test on valid set: loss= 5.119760036468506 acc= 0.6800000071525574 auc= 0.8313333333333333\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.2809e+02\n",
      "\n",
      "Test on train set: loss= 0.9534855484962463 acc= 0.9390579462051392 auc= 0.9562877128842583\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8220000000000001\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -5.8342e+02\n",
      "\n",
      "Test on train set: loss= 0.4148596525192261 acc= 0.9736970067024231 auc= 0.9739122063161799\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8222222222222222\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.0290e+02\n",
      "\n",
      "Test on train set: loss= 0.3322485089302063 acc= 0.9789575934410095 auc= 0.974588366299181\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.8333333333333333\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.5270e+02\n",
      "\n",
      "Test on train set: loss= 0.2892315983772278 acc= 0.9814664721488953 auc= 0.973748194941742\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8511111111111112\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.7598e+02\n",
      "\n",
      "Test on train set: loss= 0.2532620429992676 acc= 0.9837325811386108 auc= 0.9773558540250878\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1169e+02\n",
      "\n",
      "Test on train set: loss= 0.22361354529857635 acc= 0.9859986901283264 auc= 0.9775527611022072\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000691413879 auc= 0.8560000000000001\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1581e+02\n",
      "\n",
      "Test on train set: loss= 0.21397404372692108 acc= 0.9864842891693115 auc= 0.9784941035277779\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.2138e+02\n",
      "\n",
      "Test on train set: loss= 0.23739352822303772 acc= 0.9846228361129761 auc= 0.9762277410316946\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.8605e+02\n",
      "\n",
      "Test on train set: loss= 0.21453262865543365 acc= 0.9860796332359314 auc= 0.979956218668179\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -7.7346e+02\n",
      "\n",
      "Test on train set: loss= 0.17540112137794495 acc= 0.9889122843742371 auc= 0.9815290809058069\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -7.6725e+02\n",
      "\n",
      "Test on train set: loss= 0.19646723568439484 acc= 0.9871317744255066 auc= 0.9795980438176004\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.7057e+02\n",
      "\n",
      "Test on train set: loss= 0.17731386423110962 acc= 0.9885076284408569 auc= 0.9804798367525137\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -7.7183e+02\n",
      "\n",
      "Test on train set: loss= 0.1691662222146988 acc= 0.9893978834152222 auc= 0.9822321323654715\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.7315e+02\n",
      "\n",
      "Test on train set: loss= 0.1739645004272461 acc= 0.9889122843742371 auc= 0.9826680177678859\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.7977e+02\n",
      "\n",
      "Test on train set: loss= 0.1632145643234253 acc= 0.9897215962409973 auc= 0.9819596930077645\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.842\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.8161e+02\n",
      "\n",
      "Test on train set: loss= 0.1726233810186386 acc= 0.988993227481842 auc= 0.9836582832862382\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8333333333333334\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.2912e+02\n",
      "\n",
      "Test on train set: loss= 0.163330540060997 acc= 0.9897215962409973 auc= 0.9822789311032352\n",
      "Test on valid set: loss= 3.932629346847534 acc= 0.7408036589622498 auc= 0.8646666666666667\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.5908e+02\n",
      "\n",
      "Test on train set: loss= 0.16344355046749115 acc= 0.9896406531333923 auc= 0.9821315480789714\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.2119e+02\n",
      "\n",
      "Test on train set: loss= 0.151194766163826 acc= 0.9904499650001526 auc= 0.982407036603191\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000691413879 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1347e+02\n",
      "\n",
      "Test on train set: loss= 0.13566407561302185 acc= 0.9913402199745178 auc= 0.986072016314346\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.8799e+02\n",
      "\n",
      "Test on train set: loss= 0.12932974100112915 acc= 0.991744875907898 auc= 0.9871463727500164\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -7.8482e+02\n",
      "\n",
      "Test on train set: loss= 0.13315029442310333 acc= 0.991744875907898 auc= 0.9863419943281352\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.2102e+02\n",
      "\n",
      "Test on train set: loss= 0.1401030719280243 acc= 0.9910974502563477 auc= 0.9868252887956185\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999094009399 auc= 0.8437777777777777\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.0505e+02\n",
      "\n",
      "Test on train set: loss= 0.13123929500579834 acc= 0.9915021061897278 auc= 0.9870927749917602\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.844888888888889\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.0411e+02\n",
      "\n",
      "Test on train set: loss= 0.14113880693912506 acc= 0.9909355640411377 auc= 0.9854872907795942\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8542222222222222\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.0674e+02\n",
      "\n",
      "Test on train set: loss= 0.1593623161315918 acc= 0.9898025393486023 auc= 0.986648105539154\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.1100e+02\n",
      "\n",
      "Test on train set: loss= 0.12284804880619049 acc= 0.9920686483383179 auc= 0.9878079709819072\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8431111111111111\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1374e+02\n",
      "\n",
      "Test on train set: loss= 0.16692019999027252 acc= 0.9892359972000122 auc= 0.9849773085527446\n",
      "Test on valid set: loss= 4.193439483642578 acc= 0.7374439239501953 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.3788e+02\n",
      "\n",
      "Test on train set: loss= 0.12732724845409393 acc= 0.9920686483383179 auc= 0.9865033344392373\n",
      "Test on valid set: loss= 4.035701274871826 acc= 0.7400046586990356 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.2140e+02\n",
      "\n",
      "Test on train set: loss= 0.11613843590021133 acc= 0.9926351308822632 auc= 0.9899328773740426\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.0738e+02\n",
      "\n",
      "Test on train set: loss= 0.12131121754646301 acc= 0.9922304749488831 auc= 0.9858954646710115\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.0423e+02\n",
      "\n",
      "Test on train set: loss= 0.13022670149803162 acc= 0.991744875907898 auc= 0.9899652384584288\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.0322e+02\n",
      "\n",
      "Test on train set: loss= 0.13027356564998627 acc= 0.9916639924049377 auc= 0.9895081066495989\n",
      "Test on valid set: loss= 4.513698577880859 acc= 0.719377875328064 auc= 0.8435555555555556\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.0434e+02\n",
      "\n",
      "Test on train set: loss= 0.12652873992919922 acc= 0.991744875907898 auc= 0.988377812397862\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8333333333333334\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1137e+02\n",
      "\n",
      "Test on train set: loss= 0.12900686264038086 acc= 0.9916639924049377 auc= 0.9901858885718955\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8548888888888888\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.5401e+02\n",
      "\n",
      "Test on train set: loss= 0.10610820353031158 acc= 0.9932016730308533 auc= 0.989568724600114\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.3002e+02\n",
      "\n",
      "Test on train set: loss= 0.10568779706954956 acc= 0.9933635592460632 auc= 0.9888975538697438\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1727e+02\n",
      "\n",
      "Test on train set: loss= 0.11219474673271179 acc= 0.9927970170974731 auc= 0.9891930307096508\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1965e+02\n",
      "\n",
      "Test on train set: loss= 0.12094589322805405 acc= 0.992311418056488 auc= 0.9903056945847265\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1137e+02\n",
      "\n",
      "Test on train set: loss= 0.11346962302923203 acc= 0.9928779602050781 auc= 0.9913578791346929\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.4086e+02\n",
      "\n",
      "Test on train set: loss= 0.11579768359661102 acc= 0.9927160739898682 auc= 0.9882210318752115\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.2708e+02\n",
      "\n",
      "Test on train set: loss= 0.11790499091148376 acc= 0.992554247379303 auc= 0.9896576717637441\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.1914e+02\n",
      "\n",
      "Test on train set: loss= 0.1141318678855896 acc= 0.9927970170974731 auc= 0.9906268066250508\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1656e+02\n",
      "\n",
      "Test on train set: loss= 0.10869069397449493 acc= 0.9931207299232483 auc= 0.9901187808120214\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.844\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.2963e+02\n",
      "\n",
      "Test on train set: loss= 0.10390231758356094 acc= 0.9932016730308533 auc= 0.9895989890759258\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.845111111111111\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.6426e+02\n",
      "\n",
      "Test on train set: loss= 0.12354404479265213 acc= 0.9915021061897278 auc= 0.9885248668597502\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.4518e+02\n",
      "\n",
      "Test on train set: loss= 0.0951080471277237 acc= 0.9938491582870483 auc= 0.9878574780776104\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.1689e+02\n",
      "\n",
      "Test on train set: loss= 0.10062787681818008 acc= 0.9936872720718384 auc= 0.9904285919971425\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1751e+02\n",
      "\n",
      "Test on train set: loss= 0.09099452942609787 acc= 0.9941728711128235 auc= 0.9910719147425727\n",
      "Test on valid set: loss= 4.370904922485352 acc= 0.7200024724006653 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -8.1582e+02\n",
      "\n",
      "Test on train set: loss= 0.09694795310497284 acc= 0.9938491582870483 auc= 0.9908769532701103\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8431111111111111\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -8.6158e+02\n",
      "\n",
      "Test on train set: loss= 0.09686458855867386 acc= 0.9939301013946533 auc= 0.9883858836120398\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.8333333333333333\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.8298e+02\n",
      "\n",
      "Test on train set: loss= 0.10713382810354233 acc= 0.9929589033126831 auc= 0.9885336441281598\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.5338e+02\n",
      "\n",
      "Test on train set: loss= 0.09341992437839508 acc= 0.9939301013946533 auc= 0.988032536127764\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.844\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.3272e+02\n",
      "\n",
      "Test on train set: loss= 0.09685303270816803 acc= 0.9938491582870483 auc= 0.9886005135006467\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -8.4409e+02\n",
      "\n",
      "Test on train set: loss= 0.09804800897836685 acc= 0.9935253858566284 auc= 0.9904310613800925\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.7797e+02\n",
      "\n",
      "Test on train set: loss= 0.09126581251621246 acc= 0.9942538142204285 auc= 0.9906246361147742\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.4636e+02\n",
      "\n",
      "Test on train set: loss= 0.10366305708885193 acc= 0.9932826161384583 auc= 0.9903676494603543\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.3618e+02\n",
      "\n",
      "Test on train set: loss= 0.08840860426425934 acc= 0.9944156408309937 auc= 0.9913401419661099\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8544444444444445\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2457e+02\n",
      "\n",
      "Test on train set: loss= 0.09549234062433243 acc= 0.9938491582870483 auc= 0.9903635441482688\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2996e+02\n",
      "\n",
      "Test on train set: loss= 0.08990751951932907 acc= 0.9940919280052185 auc= 0.9905461204696137\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.3096e+02\n",
      "\n",
      "Test on train set: loss= 0.08798792958259583 acc= 0.9944965839385986 auc= 0.9903984397903347\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2588e+02\n",
      "\n",
      "Test on train set: loss= 0.0876985713839531 acc= 0.9944965839385986 auc= 0.9903447115337694\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2060e+02\n",
      "\n",
      "Test on train set: loss= 0.10057657212018967 acc= 0.9936063289642334 auc= 0.9901974906476945\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.1963e+02\n",
      "\n",
      "Test on train set: loss= 0.09832414984703064 acc= 0.9938491582870483 auc= 0.9905935810215537\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.1883e+02\n",
      "\n",
      "Test on train set: loss= 0.09732167422771454 acc= 0.9934445023536682 auc= 0.9904891587634216\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.1760e+02\n",
      "\n",
      "Test on train set: loss= 0.08623819053173065 acc= 0.9945775270462036 auc= 0.990980183812564\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.1999e+02\n",
      "\n",
      "Test on train set: loss= 0.09019284695386887 acc= 0.9942538142204285 auc= 0.9914093012303447\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.6100e+02\n",
      "\n",
      "Test on train set: loss= 0.10108882188796997 acc= 0.9933635592460632 auc= 0.9908026214807947\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -8.3905e+02\n",
      "\n",
      "Test on train set: loss= 0.08391664177179337 acc= 0.9946584701538086 auc= 0.9912541763890257\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2966e+02\n",
      "\n",
      "Test on train set: loss= 0.09336963295936584 acc= 0.9940919280052185 auc= 0.9896605895640247\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2568e+02\n",
      "\n",
      "Test on train set: loss= 0.08033449947834015 acc= 0.9947394132614136 auc= 0.9916390555335493\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2309e+02\n",
      "\n",
      "Test on train set: loss= 0.08296310156583786 acc= 0.9947394132614136 auc= 0.9907710183319247\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2297e+02\n",
      "\n",
      "Test on train set: loss= 0.08848503232002258 acc= 0.9944156408309937 auc= 0.9904500342723029\n",
      "Test on valid set: loss= 4.191530704498291 acc= 0.7391908764839172 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.1942e+02\n",
      "\n",
      "Test on train set: loss= 0.10020257532596588 acc= 0.9933635592460632 auc= 0.9908213542509486\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2136e+02\n",
      "\n",
      "Test on train set: loss= 0.07740985602140427 acc= 0.9950631260871887 auc= 0.9917127163591163\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2010e+02\n",
      "\n",
      "Test on train set: loss= 0.07752968370914459 acc= 0.9949821829795837 auc= 0.990841949990493\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.1938e+02\n",
      "\n",
      "Test on train set: loss= 0.09743289649486542 acc= 0.9937682151794434 auc= 0.9917554467026521\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2234e+02\n",
      "\n",
      "Test on train set: loss= 0.0774921402335167 acc= 0.9950631260871887 auc= 0.9907842088152223\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -8.6222e+02\n",
      "\n",
      "Test on train set: loss= 0.08923670649528503 acc= 0.9942538142204285 auc= 0.9917356117054007\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.4364e+02\n",
      "\n",
      "Test on train set: loss= 0.08412974327802658 acc= 0.9946584701538086 auc= 0.9906734529037162\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.4042e+02\n",
      "\n",
      "Test on train set: loss= 0.07797271013259888 acc= 0.9949012398719788 auc= 0.9905062803996838\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.7254e+02\n",
      "\n",
      "Test on train set: loss= 0.08743124455213547 acc= 0.9944965839385986 auc= 0.9903451839105356\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.5470e+02\n",
      "\n",
      "Test on train set: loss= 0.08628713339567184 acc= 0.9944965839385986 auc= 0.9909230111547622\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.3650e+02\n",
      "\n",
      "Test on train set: loss= 0.0878974199295044 acc= 0.9943347573280334 auc= 0.9897655358219299\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2947e+02\n",
      "\n",
      "Test on train set: loss= 0.07144691795110703 acc= 0.9953058958053589 auc= 0.9903660876715442\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2420e+02\n",
      "\n",
      "Test on train set: loss= 0.07909079641103745 acc= 0.9950631260871887 auc= 0.9906747664806856\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8564444444444443\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -8.2353e+02\n",
      "\n",
      "Test on train set: loss= 0.08528414368629456 acc= 0.9946584701538086 auc= 0.9904151937256905\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2153e+02\n",
      "\n",
      "Test on train set: loss= 0.0954282134771347 acc= 0.9939301013946533 auc= 0.9907401707749074\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2173e+02\n",
      "\n",
      "Test on train set: loss= 0.08818631619215012 acc= 0.9943347573280334 auc= 0.9899404116376738\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2260e+02\n",
      "\n",
      "Test on train set: loss= 0.07594894617795944 acc= 0.9952250123023987 auc= 0.9901105398382611\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2017e+02\n",
      "\n",
      "Test on train set: loss= 0.08199701458215714 acc= 0.9948203563690186 auc= 0.9921524157421974\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -8.2259e+02\n",
      "\n",
      "Test on train set: loss= 0.08308451622724533 acc= 0.9947394132614136 auc= 0.9916419058131568\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -8.2338e+02\n",
      "\n",
      "Test on train set: loss= 0.08473339676856995 acc= 0.9945775270462036 auc= 0.991046426212345\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2070e+02\n",
      "\n",
      "Test on train set: loss= 0.10812727361917496 acc= 0.9929589033126831 auc= 0.9927316003706717\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2363e+02\n",
      "\n",
      "Test on train set: loss= 0.08639135211706161 acc= 0.9944156408309937 auc= 0.9892421481742708\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8542222222222222\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2280e+02\n",
      "\n",
      "Test on train set: loss= 0.07939528673887253 acc= 0.9947394132614136 auc= 0.990671362672737\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.3059e+02\n",
      "\n",
      "Test on train set: loss= 0.0847451239824295 acc= 0.9945775270462036 auc= 0.9908953814930935\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8560000000000001\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2849e+02\n",
      "\n",
      "Test on train set: loss= 0.09760602563619614 acc= 0.9935253858566284 auc= 0.9904206855111101\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8546666666666665\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.2603e+02\n",
      "\n",
      "Test on train set: loss= 0.09844634681940079 acc= 0.9936872720718384 auc= 0.9902552890888344\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.6327e+02\n",
      "\n",
      "Test on train set: loss= 0.07210809737443924 acc= 0.9953868389129639 auc= 0.9914256418628569\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "Enhancement\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -8.4421e+02\n",
      "\n",
      "Test on train set: loss= 0.07701083272695541 acc= 0.9952250123023987 auc= 0.991265118352708\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8548888888888889\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "metric_idx = 1\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_EEC(beginEncodingEpoch, endEncodingEpoch,  \n",
    "                                               codingEnhancementRate, mu,\n",
    "                                               train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Enhancement')\n",
    "                        \n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "enhancementWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 6s - loss: 1.5189\n",
      "\n",
      "Test on train set: loss= 1.1770888566970825 acc= 0.6104726195335388 auc= 0.7724332118747087\n",
      "Test on valid set: loss= 2.495112657546997 acc= 0.18380489945411682 auc= 0.6853333333333333\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 3s - loss: 1.7737\n",
      "\n",
      "Test on train set: loss= 4.278204441070557 acc= 0.41777274012565613 auc= 0.7205668161369653\n",
      "Test on valid set: loss= 6.752689361572266 acc= 0.19360317289829254 auc= 0.624\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 3s - loss: 2.4742\n",
      "\n",
      "Test on train set: loss= 2.099769115447998 acc= 0.5994658470153809 auc= 0.7957465541425364\n",
      "Test on valid set: loss= 6.467487812042236 acc= 0.2906305193901062 auc= 0.7408888888888889\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 2s - loss: 3.5971\n",
      "\n",
      "Test on train set: loss= 3.6529974937438965 acc= 0.5707348585128784 auc= 0.7954026552846891\n",
      "Test on valid set: loss= 7.055454254150391 acc= 0.3076986074447632 auc= 0.7022222222222223\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 3s - loss: 4.4587\n",
      "\n",
      "Test on train set: loss= 4.075717926025391 acc= 0.5490450263023376 auc= 0.7920051080533936\n",
      "Test on valid set: loss= 9.108469009399414 acc= 0.2026304006576538 auc= 0.7288888888888889\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 5.5632\n",
      "\n",
      "Test on train set: loss= 4.348857402801514 acc= 0.6172709465026855 auc= 0.8239301039880221\n",
      "Test on valid set: loss= 9.487211227416992 acc= 0.3231031000614166 auc= 0.735111111111111\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 6.7422\n",
      "\n",
      "Test on train set: loss= 5.031980514526367 acc= 0.5562479496002197 auc= 0.7834402522811923\n",
      "Test on valid set: loss= 9.597884178161621 acc= 0.2504793107509613 auc= 0.7033333333333334\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 7.9464\n",
      "\n",
      "Test on train set: loss= 4.950103759765625 acc= 0.6447879672050476 auc= 0.7488020336314228\n",
      "Test on valid set: loss= 11.108281135559082 acc= 0.2592679560184479 auc= 0.6880000000000001\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 9.5262\n",
      "\n",
      "Test on train set: loss= 5.988434314727783 acc= 0.5963094830513 auc= 0.7363273795789599\n",
      "Test on valid set: loss= 10.199446678161621 acc= 0.28685471415519714 auc= 0.7088888888888889\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 10.2417\n",
      "\n",
      "Test on train set: loss= 3.691991090774536 acc= 0.7179508209228516 auc= 0.8509743826987537\n",
      "Test on valid set: loss= 9.156488418579102 acc= 0.3355996608734131 auc= 0.7613333333333333\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 10.0209\n",
      "\n",
      "Test on train set: loss= 7.174707412719727 acc= 0.5120589137077332 auc= 0.7126033603441846\n",
      "Test on valid set: loss= 9.262948989868164 acc= 0.36198195815086365 auc= 0.7353333333333334\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 10.8603\n",
      "\n",
      "Test on train set: loss= 3.927061080932617 acc= 0.6862252950668335 auc= 0.8245825963331784\n",
      "Test on valid set: loss= 9.327877044677734 acc= 0.3978768289089203 auc= 0.820888888888889\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 13.1061\n",
      "\n",
      "Test on train set: loss= 4.1538825035095215 acc= 0.6952897310256958 auc= 0.8344679983256151\n",
      "Test on valid set: loss= 7.921393394470215 acc= 0.48568791151046753 auc= 0.804\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 13.5474\n",
      "\n",
      "Test on train set: loss= 4.494863510131836 acc= 0.6791841983795166 auc= 0.842731694153497\n",
      "Test on valid set: loss= 9.47854232788086 acc= 0.3840908110141754 auc= 0.744\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 13.1304\n",
      "\n",
      "Test on train set: loss= 3.5253703594207764 acc= 0.7376173734664917 auc= 0.8723718838997196\n",
      "Test on valid set: loss= 9.072260856628418 acc= 0.40705403685569763 auc= 0.7962222222222223\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.3119\n",
      "\n",
      "Test on train set: loss= 3.0966458320617676 acc= 0.7775979042053223 auc= 0.8961552009391456\n",
      "Test on valid set: loss= 9.03607177734375 acc= 0.3798335790634155 auc= 0.8508888888888888\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 15.2026\n",
      "\n",
      "Test on train set: loss= 3.2052576541900635 acc= 0.7757365107536316 auc= 0.8417137716067649\n",
      "Test on valid set: loss= 8.32652759552002 acc= 0.4564444422721863 auc= 0.8197777777777777\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.4168\n",
      "\n",
      "Test on train set: loss= 3.09881591796875 acc= 0.7833441495895386 auc= 0.8841962422848197\n",
      "Test on valid set: loss= 9.068492889404297 acc= 0.4028140902519226 auc= 0.8031111111111111\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 14.8391\n",
      "\n",
      "Test on train set: loss= 2.8783013820648193 acc= 0.795969545841217 auc= 0.8799843562413946\n",
      "Test on valid set: loss= 8.79345989227295 acc= 0.43380969762802124 auc= 0.8211111111111112\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 15.7399\n",
      "\n",
      "Test on train set: loss= 4.18093204498291 acc= 0.7067821025848389 auc= 0.8150043351438736\n",
      "Test on valid set: loss= 11.710968971252441 acc= 0.24045972526073456 auc= 0.7362222222222222\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.5998\n",
      "\n",
      "Test on train set: loss= 3.505002021789551 acc= 0.7481385469436646 auc= 0.9131478335463845\n",
      "Test on valid set: loss= 9.43145751953125 acc= 0.38004106283187866 auc= 0.828888888888889\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 20.0514\n",
      "\n",
      "Test on train set: loss= 3.109941005706787 acc= 0.7761411666870117 auc= 0.8534197501956216\n",
      "Test on valid set: loss= 8.546680450439453 acc= 0.4400114119052887 auc= 0.8324444444444445\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.2755\n",
      "\n",
      "Test on train set: loss= 3.798039674758911 acc= 0.7375364303588867 auc= 0.8432267028763423\n",
      "Test on valid set: loss= 8.084301948547363 acc= 0.4846433699131012 auc= 0.7813333333333333\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 18.6302\n",
      "\n",
      "Test on train set: loss= 3.0830061435699463 acc= 0.7819682955741882 auc= 0.8959470085069181\n",
      "Test on valid set: loss= 6.664740085601807 acc= 0.5513828992843628 auc= 0.86\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.1518\n",
      "\n",
      "Test on train set: loss= 2.7539844512939453 acc= 0.8056814670562744 auc= 0.8926989380868242\n",
      "Test on valid set: loss= 6.941264629364014 acc= 0.5417887568473816 auc= 0.8459999999999999\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 18.1013\n",
      "\n",
      "Test on train set: loss= 2.616716146469116 acc= 0.8142603039741516 auc= 0.9055742281112213\n",
      "Test on valid set: loss= 6.128635406494141 acc= 0.6165534853935242 auc= 0.8617777777777779\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 16.4433\n",
      "\n",
      "Test on train set: loss= 3.1394131183624268 acc= 0.7876335382461548 auc= 0.8732266331516468\n",
      "Test on valid set: loss= 5.136539459228516 acc= 0.6210598945617676 auc= 0.8653333333333333\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.4752\n",
      "\n",
      "Test on train set: loss= 4.323156356811523 acc= 0.695613443851471 auc= 0.8264887150621721\n",
      "Test on valid set: loss= 7.118304252624512 acc= 0.5437976121902466 auc= 0.810888888888889\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 13.8090\n",
      "\n",
      "Test on train set: loss= 3.1530797481536865 acc= 0.7790547013282776 auc= 0.8672291403635086\n",
      "Test on valid set: loss= 7.408201694488525 acc= 0.5397915840148926 auc= 0.8482222222222223\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.1982\n",
      "\n",
      "Test on train set: loss= 2.6333281993865967 acc= 0.8178213238716125 auc= 0.9111808814874068\n",
      "Test on valid set: loss= 6.162813663482666 acc= 0.599885106086731 auc= 0.8842222222222222\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.5399\n",
      "\n",
      "Test on train set: loss= 2.4304018020629883 acc= 0.8344933390617371 auc= 0.8707712043157173\n",
      "Test on valid set: loss= 7.736687183380127 acc= 0.5199993252754211 auc= 0.8066666666666666\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.2325\n",
      "\n",
      "Test on train set: loss= 2.538243055343628 acc= 0.8288280963897705 auc= 0.8952408319346287\n",
      "Test on valid set: loss= 6.07838249206543 acc= 0.6001331210136414 auc= 0.8695555555555554\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.3804\n",
      "\n",
      "Test on train set: loss= 2.630464553833008 acc= 0.8201683163642883 auc= 0.8854084675220143\n",
      "Test on valid set: loss= 6.113525390625 acc= 0.5999239683151245 auc= 0.8677777777777779\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 16.4331\n",
      "\n",
      "Test on train set: loss= 3.416896104812622 acc= 0.7664292454719543 auc= 0.8895834786483846\n",
      "Test on valid set: loss= 6.419277191162109 acc= 0.5999605059623718 auc= 0.848888888888889\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 14.4720\n",
      "\n",
      "Test on train set: loss= 3.7920589447021484 acc= 0.7380220293998718 auc= 0.8870164629471488\n",
      "Test on valid set: loss= 5.8329243659973145 acc= 0.6242464780807495 auc= 0.8560000000000001\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.1841\n",
      "\n",
      "Test on train set: loss= 2.4607934951782227 acc= 0.8355454802513123 auc= 0.857108336150229\n",
      "Test on valid set: loss= 6.514598369598389 acc= 0.5790201425552368 auc= 0.8326666666666667\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.7951\n",
      "\n",
      "Test on train set: loss= 2.6641170978546143 acc= 0.8159598708152771 auc= 0.9036485454047242\n",
      "Test on valid set: loss= 6.12809944152832 acc= 0.5998867750167847 auc= 0.8573333333333333\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 18.2532\n",
      "\n",
      "Test on train set: loss= 2.101618528366089 acc= 0.8559404611587524 auc= 0.9180222584804663\n",
      "Test on valid set: loss= 7.083415508270264 acc= 0.5400416254997253 auc= 0.8457777777777779\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.0972\n",
      "\n",
      "Test on train set: loss= 3.7753560543060303 acc= 0.7544512748718262 auc= 0.808408749356674\n",
      "Test on valid set: loss= 8.307933807373047 acc= 0.4800001084804535 auc= 0.7568888888888888\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.0510\n",
      "\n",
      "Test on train set: loss= 2.543071746826172 acc= 0.8281806707382202 auc= 0.8821986167885278\n",
      "Test on valid set: loss= 7.18690299987793 acc= 0.5397251844406128 auc= 0.7984444444444445\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.2935\n",
      "\n",
      "Test on train set: loss= 3.0851006507873535 acc= 0.7979119420051575 auc= 0.8265092257566934\n",
      "Test on valid set: loss= 7.109760284423828 acc= 0.5482138991355896 auc= 0.8160000000000001\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.0998\n",
      "\n",
      "Test on train set: loss= 2.5977070331573486 acc= 0.8255907893180847 auc= 0.8924120876295285\n",
      "Test on valid set: loss= 6.310448169708252 acc= 0.5831352472305298 auc= 0.8748888888888888\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.4194\n",
      "\n",
      "Test on train set: loss= 2.0625216960906982 acc= 0.8603917360305786 auc= 0.9267296339671647\n",
      "Test on valid set: loss= 4.912909030914307 acc= 0.676030158996582 auc= 0.8635555555555555\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 22.1213\n",
      "\n",
      "Test on train set: loss= 2.30441951751709 acc= 0.8488183617591858 auc= 0.8716985648902271\n",
      "Test on valid set: loss= 6.741921424865723 acc= 0.5755713582038879 auc= 0.8186666666666665\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.9208\n",
      "\n",
      "Test on train set: loss= 2.7013397216796875 acc= 0.8187115788459778 auc= 0.8433550795619998\n",
      "Test on valid set: loss= 7.558431625366211 acc= 0.5200148820877075 auc= 0.7655555555555555\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.9078\n",
      "\n",
      "Test on train set: loss= 3.244347333908081 acc= 0.7834250330924988 auc= 0.8994801660687461\n",
      "Test on valid set: loss= 4.469358444213867 acc= 0.7200000286102295 auc= 0.8695555555555554\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 21.9550\n",
      "\n",
      "Test on train set: loss= 2.670499086380005 acc= 0.8237293362617493 auc= 0.9046232453593639\n",
      "Test on valid set: loss= 6.447757720947266 acc= 0.5994873642921448 auc= 0.8853333333333333\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 20.7257\n",
      "\n",
      "Test on train set: loss= 1.761480689048767 acc= 0.8820815682411194 auc= 0.9176717822226236\n",
      "Test on valid set: loss= 4.254140853881836 acc= 0.7115315794944763 auc= 0.8946666666666667\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.0719\n",
      "\n",
      "Test on train set: loss= 2.3590753078460693 acc= 0.8416963219642639 auc= 0.9031826721747219\n",
      "Test on valid set: loss= 5.173494815826416 acc= 0.6691187620162964 auc= 0.8800000000000001\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 16.5211\n",
      "\n",
      "Test on train set: loss= 2.1723952293395996 acc= 0.8557785749435425 auc= 0.9330692023959817\n",
      "Test on valid set: loss= 4.981677055358887 acc= 0.6628943681716919 auc= 0.8797777777777778\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 15.9473\n",
      "\n",
      "Test on train set: loss= 2.5595998764038086 acc= 0.8268048167228699 auc= 0.9232932611364706\n",
      "Test on valid set: loss= 5.496490955352783 acc= 0.6488356590270996 auc= 0.8415555555555555\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 17.4852\n",
      "\n",
      "Test on train set: loss= 1.881360411643982 acc= 0.8713175654411316 auc= 0.9086810744413978\n",
      "Test on valid set: loss= 4.38405704498291 acc= 0.701863169670105 auc= 0.8811111111111112\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 19.8338\n",
      "\n",
      "Test on train set: loss= 2.2856414318084717 acc= 0.846390426158905 auc= 0.9001879265585464\n",
      "Test on valid set: loss= 4.50760555267334 acc= 0.7152261734008789 auc= 0.8644444444444446\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 21.2452\n",
      "\n",
      "Test on train set: loss= 2.848308801651001 acc= 0.8087568879127502 auc= 0.889805345181629\n",
      "Test on valid set: loss= 7.641495227813721 acc= 0.5093604326248169 auc= 0.8406666666666667\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 17.8680\n",
      "\n",
      "Test on train set: loss= 2.201706886291504 acc= 0.8481709361076355 auc= 0.9090409876861443\n",
      "Test on valid set: loss= 5.480163097381592 acc= 0.6599895358085632 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 22.7705\n",
      "\n",
      "Test on train set: loss= 3.3705925941467285 acc= 0.7831013202667236 auc= 0.8369679183734628\n",
      "Test on valid set: loss= 6.463881969451904 acc= 0.5865012407302856 auc= 0.8104444444444443\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 21.5004\n",
      "\n",
      "Test on train set: loss= 1.6544263362884521 acc= 0.8877468705177307 auc= 0.9243404690562148\n",
      "Test on valid set: loss= 5.346304893493652 acc= 0.6472777128219604 auc= 0.852\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.1162\n",
      "\n",
      "Test on train set: loss= 1.701776146888733 acc= 0.8869374990463257 auc= 0.9282221679469416\n",
      "Test on valid set: loss= 5.042698383331299 acc= 0.6610049605369568 auc= 0.8935555555555557\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.8343\n",
      "\n",
      "Test on train set: loss= 1.7801226377487183 acc= 0.8825671672821045 auc= 0.9206037084824565\n",
      "Test on valid set: loss= 4.00447940826416 acc= 0.7280194163322449 auc= 0.894888888888889\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.1079\n",
      "\n",
      "Test on train set: loss= 2.008634567260742 acc= 0.8684040307998657 auc= 0.9211482340704242\n",
      "Test on valid set: loss= 4.679248809814453 acc= 0.6967399716377258 auc= 0.8895555555555555\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.5961\n",
      "\n",
      "Test on train set: loss= 1.914913535118103 acc= 0.8713985085487366 auc= 0.9085434267406395\n",
      "Test on valid set: loss= 5.731927394866943 acc= 0.64000004529953 auc= 0.8602222222222222\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.0604\n",
      "\n",
      "Test on train set: loss= 1.7496631145477295 acc= 0.8834574222564697 auc= 0.9004159734413969\n",
      "Test on valid set: loss= 6.00606632232666 acc= 0.6200007200241089 auc= 0.8346666666666666\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.2363\n",
      "\n",
      "Test on train set: loss= 2.274765729904175 acc= 0.8511654138565063 auc= 0.8850425072588027\n",
      "Test on valid set: loss= 5.480154991149902 acc= 0.6599976420402527 auc= 0.845111111111111\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.0866\n",
      "\n",
      "Test on train set: loss= 1.8326767683029175 acc= 0.8751214146614075 auc= 0.9276175290631403\n",
      "Test on valid set: loss= 5.443849563598633 acc= 0.6403457522392273 auc= 0.8619999999999999\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 18.3272\n",
      "\n",
      "Test on train set: loss= 1.6009249687194824 acc= 0.8930074572563171 auc= 0.9227873529235922\n",
      "Test on valid set: loss= 5.1578049659729 acc= 0.6799856424331665 auc= 0.8800000000000001\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.8347\n",
      "\n",
      "Test on train set: loss= 2.076627016067505 acc= 0.8630625009536743 auc= 0.8887795422101481\n",
      "Test on valid set: loss= 6.4472527503967285 acc= 0.5999854803085327 auc= 0.8293333333333333\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.5198\n",
      "\n",
      "Test on train set: loss= 1.9017993211746216 acc= 0.8764972686767578 auc= 0.9227175533172065\n",
      "Test on valid set: loss= 6.124884128570557 acc= 0.6199919581413269 auc= 0.8151111111111111\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.1905\n",
      "\n",
      "Test on train set: loss= 1.9086133241653442 acc= 0.8750404715538025 auc= 0.9264908561715408\n",
      "Test on valid set: loss= 4.504266262054443 acc= 0.7149580717086792 auc= 0.8828888888888888\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.2126\n",
      "\n",
      "Test on train set: loss= 2.11517596244812 acc= 0.8619294166564941 auc= 0.9174881829872135\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8713333333333333\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 25.7516\n",
      "\n",
      "Test on train set: loss= 1.9720433950424194 acc= 0.8713175654411316 auc= 0.8916978974941989\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8602222222222222\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 21.8577\n",
      "\n",
      "Test on train set: loss= 2.2482051849365234 acc= 0.8547264337539673 auc= 0.8862322900363712\n",
      "Test on valid set: loss= 5.480174541473389 acc= 0.6599779725074768 auc= 0.8493333333333333\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.3332\n",
      "\n",
      "Test on train set: loss= 1.8976986408233643 acc= 0.8748785853385925 auc= 0.9031549742170732\n",
      "Test on valid set: loss= 4.838274955749512 acc= 0.6973470449447632 auc= 0.8584444444444446\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.7780\n",
      "\n",
      "Test on train set: loss= 1.8809700012207031 acc= 0.8773065805435181 auc= 0.9057253136960449\n",
      "Test on valid set: loss= 5.543072700500488 acc= 0.636234700679779 auc= 0.8477777777777777\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 23.4381\n",
      "\n",
      "Test on train set: loss= 1.545631766319275 acc= 0.8979443311691284 auc= 0.9318431782450638\n",
      "Test on valid set: loss= 4.147982597351074 acc= 0.7276926040649414 auc= 0.9086666666666667\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.0707\n",
      "\n",
      "Test on train set: loss= 2.012509346008301 acc= 0.8696179986000061 auc= 0.8958307271755711\n",
      "Test on valid set: loss= 6.041522026062012 acc= 0.6200001239776611 auc= 0.8364444444444444\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.0060\n",
      "\n",
      "Test on train set: loss= 1.6386569738388062 acc= 0.8913887739181519 auc= 0.89706340805782\n",
      "Test on valid set: loss= 6.124107837677002 acc= 0.6200000047683716 auc= 0.8277777777777777\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 21.2344\n",
      "\n",
      "Test on train set: loss= 1.818178415298462 acc= 0.8789252042770386 auc= 0.9036652342032132\n",
      "Test on valid set: loss= 4.835752964019775 acc= 0.6996783018112183 auc= 0.8404444444444445\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.5872\n",
      "\n",
      "Test on train set: loss= 1.6977366209030151 acc= 0.8879086971282959 auc= 0.910944510969145\n",
      "Test on valid set: loss= 5.939836502075195 acc= 0.6200200915336609 auc= 0.8173333333333334\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.1706\n",
      "\n",
      "Test on train set: loss= 1.3299216032028198 acc= 0.9126740097999573 auc= 0.9166499612297698\n",
      "Test on valid set: loss= 6.401827812194824 acc= 0.6000000238418579 auc= 0.8193333333333334\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.0181\n",
      "\n",
      "Test on train set: loss= 1.748429775238037 acc= 0.8852379322052002 auc= 0.9271091936061346\n",
      "Test on valid set: loss= 4.568116664886475 acc= 0.7012752890586853 auc= 0.8606666666666667\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.8377\n",
      "\n",
      "Test on train set: loss= 2.114957332611084 acc= 0.8571544289588928 auc= 0.9235736817300715\n",
      "Test on valid set: loss= 5.157793045043945 acc= 0.6799973845481873 auc= 0.866\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.3333\n",
      "\n",
      "Test on train set: loss= 2.2315871715545654 acc= 0.8555357456207275 auc= 0.888695128366465\n",
      "Test on valid set: loss= 4.941111087799072 acc= 0.6723676323890686 auc= 0.8597777777777778\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.2768\n",
      "\n",
      "Test on train set: loss= 2.0477242469787598 acc= 0.8662188649177551 auc= 0.8844362468670116\n",
      "Test on valid set: loss= 5.744763374328613 acc= 0.6393901705741882 auc= 0.8460000000000001\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.1471\n",
      "\n",
      "Test on train set: loss= 2.48161244392395 acc= 0.8382971882820129 auc= 0.8900604522603877\n",
      "Test on valid set: loss= 6.017951488494873 acc= 0.6199988126754761 auc= 0.7984444444444444\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.9600\n",
      "\n",
      "Test on train set: loss= 1.3069530725479126 acc= 0.9142116904258728 auc= 0.93744998535134\n",
      "Test on valid set: loss= 4.273675441741943 acc= 0.7203157544136047 auc= 0.9051111111111112\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 23.8032\n",
      "\n",
      "Test on train set: loss= 1.6914914846420288 acc= 0.8892845511436462 auc= 0.902375367435669\n",
      "Test on valid set: loss= 5.058160305023193 acc= 0.6607376337051392 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.9942\n",
      "\n",
      "Test on train set: loss= 1.6602473258972168 acc= 0.8914697170257568 auc= 0.9043089394313185\n",
      "Test on valid set: loss= 6.7148542404174805 acc= 0.5799999833106995 auc= 0.8153333333333332\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.6353\n",
      "\n",
      "Test on train set: loss= 1.4619044065475464 acc= 0.9034477472305298 auc= 0.9400292516845562\n",
      "Test on valid set: loss= 4.610157489776611 acc= 0.7001558542251587 auc= 0.8831111111111112\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.6045\n",
      "\n",
      "Test on train set: loss= 2.6992642879486084 acc= 0.8236484527587891 auc= 0.880708042563176\n",
      "Test on valid set: loss= 6.769635200500488 acc= 0.5799657106399536 auc= 0.7902222222222222\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.5628\n",
      "\n",
      "Test on train set: loss= 2.446498394012451 acc= 0.8361929655075073 auc= 0.8833998600007975\n",
      "Test on valid set: loss= 6.420467376708984 acc= 0.5885763168334961 auc= 0.8331111111111111\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.0898\n",
      "\n",
      "Test on train set: loss= 1.8046983480453491 acc= 0.8773065805435181 auc= 0.925811936763125\n",
      "Test on valid set: loss= 5.330997943878174 acc= 0.659907877445221 auc= 0.8502222222222222\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 31.0282\n",
      "\n",
      "Test on train set: loss= 1.8817936182022095 acc= 0.8779540061950684 auc= 0.9105580892306705\n",
      "Test on valid set: loss= 4.516284465789795 acc= 0.7170277833938599 auc= 0.861111111111111\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.6457\n",
      "\n",
      "Test on train set: loss= 1.702818512916565 acc= 0.8877468705177307 auc= 0.91082342089835\n",
      "Test on valid set: loss= 4.875126838684082 acc= 0.6827478408813477 auc= 0.842\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.4030\n",
      "\n",
      "Test on train set: loss= 1.584694743156433 acc= 0.895920991897583 auc= 0.9373153298793861\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.6999997496604919 auc= 0.8493333333333334\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.2302\n",
      "\n",
      "Test on train set: loss= 1.5015575885772705 acc= 0.9004532098770142 auc= 0.9430673068551183\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.8633333333333333\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.4176\n",
      "\n",
      "Test on train set: loss= 1.433345079421997 acc= 0.9067659378051758 auc= 0.9074458211214044\n",
      "Test on valid set: loss= 5.754262924194336 acc= 0.6399999856948853 auc= 0.8675555555555556\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.4125\n",
      "\n",
      "Test on train set: loss= 1.4232968091964722 acc= 0.9059566259384155 auc= 0.9370170685637598\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8699999999999999\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 23.8536\n",
      "\n",
      "Test on train set: loss= 1.546867847442627 acc= 0.8985917568206787 auc= 0.9318041408014078\n",
      "Test on valid set: loss= 5.656041145324707 acc= 0.64000004529953 auc= 0.853111111111111\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 24.4239\n",
      "\n",
      "Test on train set: loss= 1.3771511316299438 acc= 0.9083845615386963 auc= 0.9285465614554231\n",
      "Test on valid set: loss= 6.903258800506592 acc= 0.5600250363349915 auc= 0.8320000000000001\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.2140\n",
      "\n",
      "Test on train set: loss= 1.3438570499420166 acc= 0.911136269569397 auc= 0.919444101396609\n",
      "Test on valid set: loss= 5.808360576629639 acc= 0.6349309682846069 auc= 0.7964444444444445\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 30.7083\n",
      "\n",
      "Test on train set: loss= 2.631518602371216 acc= 0.8297992944717407 auc= 0.8619024597485362\n",
      "Test on valid set: loss= 6.769600868225098 acc= 0.5799989104270935 auc= 0.7668888888888888\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.2779\n",
      "\n",
      "Test on train set: loss= 1.9138662815093994 acc= 0.8764163255691528 auc= 0.9332837653225836\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8186666666666665\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.0001\n",
      "\n",
      "Test on train set: loss= 1.8662313222885132 acc= 0.8793298602104187 auc= 0.9251006153240304\n",
      "Test on valid set: loss= 5.080790042877197 acc= 0.679999828338623 auc= 0.8493333333333333\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.9243\n",
      "\n",
      "Test on train set: loss= 2.0337729454040527 acc= 0.8679993748664856 auc= 0.9106417280171037\n",
      "Test on valid set: loss= 6.447895050048828 acc= 0.5993538498878479 auc= 0.8055555555555556\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.7309\n",
      "\n",
      "Test on train set: loss= 1.4686335325241089 acc= 0.9020718932151794 auc= 0.9363266763190061\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.863111111111111\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 36.2538\n",
      "\n",
      "Test on train set: loss= 1.6531791687011719 acc= 0.892683744430542 auc= 0.9027031296680444\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5799999833106995 auc= 0.8033333333333333\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 32.8374\n",
      "\n",
      "Test on train set: loss= 2.078317403793335 acc= 0.8675137758255005 auc= 0.9025137009216033\n",
      "Test on valid set: loss= 4.513067245483398 acc= 0.7199997305870056 auc= 0.863111111111111\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.9688\n",
      "\n",
      "Test on train set: loss= 1.4535037279129028 acc= 0.9060375690460205 auc= 0.9243725139536778\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8293333333333333\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.6037\n",
      "\n",
      "Test on train set: loss= 1.6761009693145752 acc= 0.8929265141487122 auc= 0.9339139932171283\n",
      "Test on valid set: loss= 5.517011642456055 acc= 0.6431669592857361 auc= 0.8186666666666668\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.0909\n",
      "\n",
      "Test on train set: loss= 1.3904982805252075 acc= 0.9100032448768616 auc= 0.9297658326856928\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8057777777777778\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.6481\n",
      "\n",
      "Test on train set: loss= 1.7345131635665894 acc= 0.888637125492096 auc= 0.9301274665236221\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8193333333333334\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.5167\n",
      "\n",
      "Test on train set: loss= 1.5017001628875732 acc= 0.9023146629333496 auc= 0.9341673735558533\n",
      "Test on valid set: loss= 4.562342166900635 acc= 0.7017022967338562 auc= 0.8731111111111112\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.9794\n",
      "\n",
      "Test on train set: loss= 2.2566030025482178 acc= 0.8561022877693176 auc= 0.8715974341030277\n",
      "Test on valid set: loss= 6.124896049499512 acc= 0.619979739189148 auc= 0.8166666666666667\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.2458\n",
      "\n",
      "Test on train set: loss= 1.613760232925415 acc= 0.8955972790718079 auc= 0.9306766286075463\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6600000262260437 auc= 0.8195555555555556\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.4173\n",
      "\n",
      "Test on train set: loss= 1.1589916944503784 acc= 0.9243282675743103 auc= 0.9518280189142269\n",
      "Test on valid set: loss= 5.158043384552002 acc= 0.6797492504119873 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.2208\n",
      "\n",
      "Test on train set: loss= 1.128521203994751 acc= 0.9273227453231812 auc= 0.9481123112446138\n",
      "Test on valid set: loss= 3.868396520614624 acc= 0.7599464058876038 auc= 0.8871111111111112\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.8535\n",
      "\n",
      "Test on train set: loss= 1.3525893688201904 acc= 0.9132405519485474 auc= 0.9301114745814901\n",
      "Test on valid set: loss= 5.159142971038818 acc= 0.6786924004554749 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.0964\n",
      "\n",
      "Test on train set: loss= 1.316980242729187 acc= 0.9137261509895325 auc= 0.9176472263651385\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.831111111111111\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.7701\n",
      "\n",
      "Test on train set: loss= 1.0766501426696777 acc= 0.9293460845947266 auc= 0.9491798058851059\n",
      "Test on valid set: loss= 4.193028450012207 acc= 0.7378060817718506 auc= 0.8544444444444445\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 24.5976\n",
      "\n",
      "Test on train set: loss= 1.2862610816955566 acc= 0.9163969159126282 auc= 0.9307399747166197\n",
      "Test on valid set: loss= 5.110488414764404 acc= 0.6799999475479126 auc= 0.8428888888888888\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 26.9988\n",
      "\n",
      "Test on train set: loss= 1.4045689105987549 acc= 0.9093557596206665 auc= 0.9346313077836352\n",
      "Test on valid set: loss= 5.681232929229736 acc= 0.6399990320205688 auc= 0.8186666666666665\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 24.8120\n",
      "\n",
      "Test on train set: loss= 1.7670868635177612 acc= 0.8866947293281555 auc= 0.901618577743359\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.64000004529953 auc= 0.8195555555555556\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 26.6340\n",
      "\n",
      "Test on train set: loss= 1.3009101152420044 acc= 0.9160731434822083 auc= 0.9416233843323077\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 25.7409\n",
      "\n",
      "Test on train set: loss= 1.4241009950637817 acc= 0.9081417918205261 auc= 0.9293951050781996\n",
      "Test on valid set: loss= 6.447238922119141 acc= 0.5999999642372131 auc= 0.7853333333333332\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.9614\n",
      "\n",
      "Test on train set: loss= 1.299635887145996 acc= 0.9155875444412231 auc= 0.9504967736293833\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8431111111111111\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.9194\n",
      "\n",
      "Test on train set: loss= 1.2697503566741943 acc= 0.9165587425231934 auc= 0.9490633732226769\n",
      "Test on valid set: loss= 4.023782730102539 acc= 0.7400084733963013 auc= 0.8751111111111112\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.0477\n",
      "\n",
      "Test on train set: loss= 1.093016266822815 acc= 0.9284558296203613 auc= 0.9420257754846139\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8064444444444444\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 30.3691\n",
      "\n",
      "Test on train set: loss= 1.540244221687317 acc= 0.9011816382408142 auc= 0.9188110479735716\n",
      "Test on valid set: loss= 5.321561336517334 acc= 0.6461811661720276 auc= 0.839111111111111\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 27.9395\n",
      "\n",
      "Test on train set: loss= 1.0987118482589722 acc= 0.9298316836357117 auc= 0.947146054327398\n",
      "Test on valid set: loss= 5.388427734375 acc= 0.6599949598312378 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.0423\n",
      "\n",
      "Test on train set: loss= 1.195481300354004 acc= 0.9232761263847351 auc= 0.9461129956632119\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.829111111111111\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.3237\n",
      "\n",
      "Test on train set: loss= 1.130712866783142 acc= 0.927079975605011 auc= 0.9306451107625747\n",
      "Test on valid set: loss= 4.7232794761657715 acc= 0.6800943613052368 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 34.2605\n",
      "\n",
      "Test on train set: loss= 1.228445291519165 acc= 0.9207672476768494 auc= 0.9305974373461294\n",
      "Test on valid set: loss= 6.447237968444824 acc= 0.6000000238418579 auc= 0.8088888888888889\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 26.2613\n",
      "\n",
      "Test on train set: loss= 1.088927149772644 acc= 0.9299125671386719 auc= 0.9506906603313044\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8059999999999998\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.8019\n",
      "\n",
      "Test on train set: loss= 1.4117850065231323 acc= 0.9090320467948914 auc= 0.9199702584971534\n",
      "Test on valid set: loss= 6.250100135803223 acc= 0.6000064015388489 auc= 0.8084444444444443\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 31.9674\n",
      "\n",
      "Test on train set: loss= 1.2343416213989258 acc= 0.919391393661499 auc= 0.9353164427257475\n",
      "Test on valid set: loss= 5.812168598175049 acc= 0.6323422193527222 auc= 0.8088888888888889\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.9116\n",
      "\n",
      "Test on train set: loss= 1.6155818700790405 acc= 0.8969731330871582 auc= 0.9128527386045476\n",
      "Test on valid set: loss= 5.806153774261475 acc= 0.6366727948188782 auc= 0.8093333333333332\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.0063\n",
      "\n",
      "Test on train set: loss= 1.3354856967926025 acc= 0.9142116904258728 auc= 0.9256319931723571\n",
      "Test on valid set: loss= 5.802558422088623 acc= 0.6399563550949097 auc= 0.8160000000000001\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.8270\n",
      "\n",
      "Test on train set: loss= 0.9699592590332031 acc= 0.938005805015564 auc= 0.907184917001428\n",
      "Test on valid set: loss= 5.477512359619141 acc= 0.6599999666213989 auc= 0.8217777777777776\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.6345\n",
      "\n",
      "Test on train set: loss= 1.0094815492630005 acc= 0.9350113272666931 auc= 0.9446821223472488\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8502222222222221\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.7434\n",
      "\n",
      "Test on train set: loss= 1.098630428314209 acc= 0.9299935102462769 auc= 0.9298545640416661\n",
      "Test on valid set: loss= 5.157991886138916 acc= 0.6797999143600464 auc= 0.8402222222222221\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.6257\n",
      "\n",
      "Test on train set: loss= 1.435056209564209 acc= 0.9083845615386963 auc= 0.9360670846586986\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8282222222222224\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.1538\n",
      "\n",
      "Test on train set: loss= 1.0479028224945068 acc= 0.9321786761283875 auc= 0.9501316705489999\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8413333333333334\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.7549\n",
      "\n",
      "Test on train set: loss= 1.1315712928771973 acc= 0.9275655746459961 auc= 0.9434135118458729\n",
      "Test on valid set: loss= 5.416460990905762 acc= 0.6600000262260437 auc= 0.8266666666666668\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 32.8691\n",
      "\n",
      "Test on train set: loss= 1.1882424354553223 acc= 0.9240045547485352 auc= 0.9210153603671767\n",
      "Test on valid set: loss= 6.1129150390625 acc= 0.6200000047683716 auc= 0.7971111111111111\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.1344\n",
      "\n",
      "Test on train set: loss= 1.0186285972595215 acc= 0.9348494410514832 auc= 0.9571835097203776\n",
      "Test on valid set: loss= 4.735823154449463 acc= 0.6845113635063171 auc= 0.8506666666666666\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.2638\n",
      "\n",
      "Test on train set: loss= 1.2434126138687134 acc= 0.9201197624206543 auc= 0.9302608513879301\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8304444444444444\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.8438\n",
      "\n",
      "Test on train set: loss= 0.9064832329750061 acc= 0.9423761963844299 auc= 0.9451798599171738\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8542222222222223\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 34.1700\n",
      "\n",
      "Test on train set: loss= 1.123783826828003 acc= 0.9285367727279663 auc= 0.9244933264034116\n",
      "Test on valid set: loss= 4.8354597091674805 acc= 0.6999689340591431 auc= 0.8320000000000001\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.5919\n",
      "\n",
      "Test on train set: loss= 0.9224563241004944 acc= 0.9408384561538696 auc= 0.9388490020228101\n",
      "Test on valid set: loss= 5.190866470336914 acc= 0.6638263463973999 auc= 0.8304444444444444\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.6608\n",
      "\n",
      "Test on train set: loss= 1.2129589319229126 acc= 0.9225477576255798 auc= 0.9396802106389164\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8384444444444444\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.1025\n",
      "\n",
      "Test on train set: loss= 1.0970454216003418 acc= 0.9301553964614868 auc= 0.9451121209463536\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.699999988079071 auc= 0.834\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.7969\n",
      "\n",
      "Test on train set: loss= 0.8218627572059631 acc= 0.9477986693382263 auc= 0.9517320258886178\n",
      "Test on valid set: loss= 5.093587875366211 acc= 0.6800000667572021 auc= 0.8435555555555556\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.8159\n",
      "\n",
      "Test on train set: loss= 1.1504794359207153 acc= 0.9264324903488159 auc= 0.965047884019387\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8631111111111112\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 23.8120\n",
      "\n",
      "Test on train set: loss= 0.8358418941497803 acc= 0.946341872215271 auc= 0.9433751167839635\n",
      "Test on valid set: loss= 5.583389759063721 acc= 0.6401145458221436 auc= 0.8333333333333334\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.9372\n",
      "\n",
      "Test on train set: loss= 1.396452784538269 acc= 0.9105697870254517 auc= 0.9448798133810472\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.852\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 26.8244\n",
      "\n",
      "Test on train set: loss= 1.0071468353271484 acc= 0.9359015822410583 auc= 0.9116923833955524\n",
      "Test on valid set: loss= 6.214710712432861 acc= 0.6002240777015686 auc= 0.7953333333333333\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.0253\n",
      "\n",
      "Test on train set: loss= 0.9906298518180847 acc= 0.9356588125228882 auc= 0.9613255024474711\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7977777777777778\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 31.9688\n",
      "\n",
      "Test on train set: loss= 0.9754283428192139 acc= 0.9382486343383789 auc= 0.9382820361982305\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8102222222222222\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.4729\n",
      "\n",
      "Test on train set: loss= 0.9506368637084961 acc= 0.9394626021385193 auc= 0.9353299028713169\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8280000000000001\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 36.1508\n",
      "\n",
      "Test on train set: loss= 0.8810757994651794 acc= 0.9431855082511902 auc= 0.9636887622594775\n",
      "Test on valid set: loss= 4.194227695465088 acc= 0.7367696166038513 auc= 0.8742222222222222\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.2632\n",
      "\n",
      "Test on train set: loss= 0.8358391523361206 acc= 0.9467465281486511 auc= 0.9305753673980082\n",
      "Test on valid set: loss= 4.7048468589782715 acc= 0.7000013589859009 auc= 0.8528888888888888\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.9938\n",
      "\n",
      "Test on train set: loss= 0.8758818507194519 acc= 0.9441567063331604 auc= 0.9387331870827023\n",
      "Test on valid set: loss= 5.668648719787598 acc= 0.640001654624939 auc= 0.8411111111111111\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.8261\n",
      "\n",
      "Test on train set: loss= 1.0437439680099487 acc= 0.9337164163589478 auc= 0.9481134328423035\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8197777777777778\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 24.5806\n",
      "\n",
      "Test on train set: loss= 0.9210274815559387 acc= 0.9415668249130249 auc= 0.9469992379155597\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8320000000000001\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 28.0371\n",
      "\n",
      "Test on train set: loss= 1.196354627609253 acc= 0.9236807823181152 auc= 0.9517278886838871\n",
      "Test on valid set: loss= 4.835428714752197 acc= 0.7000000476837158 auc= 0.844\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 29.1017\n",
      "\n",
      "Test on train set: loss= 0.8833180665969849 acc= 0.9440757632255554 auc= 0.9390598036205559\n",
      "Test on valid set: loss= 6.469130992889404 acc= 0.573821485042572 auc= 0.7851111111111111\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.1240\n",
      "\n",
      "Test on train set: loss= 1.0225706100463867 acc= 0.9354969263076782 auc= 0.9393062497322283\n",
      "Test on valid set: loss= 5.4803056716918945 acc= 0.6598473787307739 auc= 0.8211111111111112\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 37.4260\n",
      "\n",
      "Test on train set: loss= 1.0665459632873535 acc= 0.9321786761283875 auc= 0.9328818665999297\n",
      "Test on valid set: loss= 5.8514628410339355 acc= 0.6217302083969116 auc= 0.7986666666666667\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 34.9921\n",
      "\n",
      "Test on train set: loss= 0.9533571004867554 acc= 0.938815176486969 auc= 0.9439823254551726\n",
      "Test on valid set: loss= 5.101156234741211 acc= 0.6800000071525574 auc= 0.8408888888888889\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 35.8302\n",
      "\n",
      "Test on train set: loss= 0.9971678256988525 acc= 0.9367108941078186 auc= 0.9341957378045282\n",
      "Test on valid set: loss= 4.30633544921875 acc= 0.7192166447639465 auc= 0.8522222222222222\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.7814\n",
      "\n",
      "Test on train set: loss= 0.8919787406921387 acc= 0.9427808523178101 auc= 0.9503157455981608\n",
      "Test on valid set: loss= 5.863363265991211 acc= 0.6209543943405151 auc= 0.8093333333333333\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.3949\n",
      "\n",
      "Test on train set: loss= 0.9228626489639282 acc= 0.9412431120872498 auc= 0.9573542153288352\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 34.5572\n",
      "\n",
      "Test on train set: loss= 0.9851941466331482 acc= 0.9375202059745789 auc= 0.925267152290296\n",
      "Test on valid set: loss= 6.769599437713623 acc= 0.5800000429153442 auc= 0.7857777777777779\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 28.1267\n",
      "\n",
      "Test on train set: loss= 0.7707422375679016 acc= 0.9507931470870972 auc= 0.9623442159143755\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.64000004529953 auc= 0.8011111111111111\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.2295\n",
      "\n",
      "Test on train set: loss= 0.8677077889442444 acc= 0.9448850750923157 auc= 0.9638704723617553\n",
      "Test on valid set: loss= 4.1907057762146 acc= 0.7399994134902954 auc= 0.8555555555555557\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.8926\n",
      "\n",
      "Test on train set: loss= 0.7404983639717102 acc= 0.9528164267539978 auc= 0.9553558713715015\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8411111111111111\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 33.0462\n",
      "\n",
      "Test on train set: loss= 1.1197534799575806 acc= 0.9281320571899414 auc= 0.938120781819347\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7966666666666666\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 35.2997\n",
      "\n",
      "Test on train set: loss= 1.1892086267471313 acc= 0.9246519804000854 auc= 0.941980718912766\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.7982222222222223\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 31.5336\n",
      "\n",
      "Test on train set: loss= 0.9716139435768127 acc= 0.9383295774459839 auc= 0.934130093543477\n",
      "Test on valid set: loss= 4.513828277587891 acc= 0.7192531228065491 auc= 0.8446666666666667\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 35.5371\n",
      "\n",
      "Test on train set: loss= 1.4058576822280884 acc= 0.9112981557846069 auc= 0.9277833846735618\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8317777777777777\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 27.5630\n",
      "\n",
      "Test on train set: loss= 0.7030212879180908 acc= 0.9553253650665283 auc= 0.9567382972767604\n",
      "Test on valid set: loss= 4.1907057762146 acc= 0.7399995923042297 auc= 0.8744444444444446\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.0838\n",
      "\n",
      "Test on train set: loss= 0.6321806907653809 acc= 0.9596956968307495 auc= 0.9620356285381021\n",
      "Test on valid set: loss= 5.266391754150391 acc= 0.6600876450538635 auc= 0.8217777777777778\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 24.3396\n",
      "\n",
      "Test on train set: loss= 0.7387699484825134 acc= 0.9531401991844177 auc= 0.9626078657023733\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.8777777777777779\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 25.0608\n",
      "\n",
      "Test on train set: loss= 0.736609935760498 acc= 0.9528164267539978 auc= 0.9662511985294044\n",
      "Test on valid set: loss= 4.726451396942139 acc= 0.6982325315475464 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 20.6049\n",
      "\n",
      "Test on train set: loss= 0.577971339225769 acc= 0.9625283479690552 auc= 0.9767196161864069\n",
      "Test on valid set: loss= 3.546196699142456 acc= 0.7797855138778687 auc= 0.8873333333333333\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 23.8649\n",
      "\n",
      "Test on train set: loss= 0.6849332451820374 acc= 0.9563774466514587 auc= 0.962138605206609\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.7860\n",
      "\n",
      "Test on train set: loss= 0.7145382761955261 acc= 0.9545969367027283 auc= 0.9544537134365993\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 21.9182\n",
      "\n",
      "Test on train set: loss= 0.8186991214752197 acc= 0.9479604959487915 auc= 0.9590366598912599\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000905990601 auc= 0.8879999999999999\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 20.4181\n",
      "\n",
      "Test on train set: loss= 0.5412931442260742 acc= 0.9649562835693359 auc= 0.9710721580369925\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8764444444444445\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 23.5134\n",
      "\n",
      "Test on train set: loss= 0.8191985487937927 acc= 0.9478795528411865 auc= 0.9593316625498021\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.8317777777777777\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 20.5537\n",
      "\n",
      "Test on train set: loss= 0.8017968535423279 acc= 0.9485270380973816 auc= 0.9635259475273539\n",
      "Test on valid set: loss= 5.4801530838012695 acc= 0.6599990129470825 auc= 0.8497777777777777\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 24.2474\n",
      "\n",
      "Test on train set: loss= 0.6800687909126282 acc= 0.9562156200408936 auc= 0.9738723159414322\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.888888888888889\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: 24.0413\n",
      "\n",
      "Test on train set: loss= 0.7688339948654175 acc= 0.9507931470870972 auc= 0.9650722553832924\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8217777777777778\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 17.9433\n",
      "\n",
      "Test on train set: loss= 0.6413114666938782 acc= 0.9588863849639893 auc= 0.9671799449970682\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6800000071525574 auc= 0.8226666666666667\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 19.5249\n",
      "\n",
      "Test on train set: loss= 0.6421534419059753 acc= 0.9588054418563843 auc= 0.9635625572614337\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.699999988079071 auc= 0.861111111111111\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.9751\n",
      "\n",
      "Test on train set: loss= 0.6867421269416809 acc= 0.9563774466514587 auc= 0.9567223046832505\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8535555555555556\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.0194\n",
      "\n",
      "Test on train set: loss= 0.798162043094635 acc= 0.9489316940307617 auc= 0.9405361406027207\n",
      "Test on valid set: loss= 5.480152606964111 acc= 0.6599999666213989 auc= 0.8406666666666667\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 22.0395\n",
      "\n",
      "Test on train set: loss= 0.607470691204071 acc= 0.9609096646308899 auc= 0.9769293508147502\n",
      "Test on valid set: loss= 5.897075176239014 acc= 0.6201769113540649 auc= 0.8173333333333332\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 30.6797\n",
      "\n",
      "Test on train set: loss= 1.0247819423675537 acc= 0.9350922703742981 auc= 0.950527802248842\n",
      "Test on valid set: loss= 4.835428237915039 acc= 0.6999999284744263 auc= 0.8428888888888888\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: 29.0341\n",
      "\n",
      "Test on train set: loss= 0.9611596465110779 acc= 0.9393816590309143 auc= 0.9633341236723127\n",
      "Test on valid set: loss= 3.3845374584198 acc= 0.780006468296051 auc= 0.8988888888888888\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 28.7865\n",
      "\n",
      "Test on train set: loss= 0.4382590651512146 acc= 0.9719974398612976 auc= 0.9764468069368426\n",
      "Test on valid set: loss= 2.9012575149536133 acc= 0.8199996948242188 auc= 0.9091111111111111\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 15.5698\n",
      "\n",
      "Test on train set: loss= 0.3371627926826477 acc= 0.9784719944000244 auc= 0.9868733958321638\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.9175555555555555\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 8.8801\n",
      "\n",
      "Test on train set: loss= 0.28917011618614197 acc= 0.9812237024307251 auc= 0.9866759141269845\n",
      "Test on valid set: loss= 2.901257276535034 acc= 0.8199999928474426 auc= 0.9091111111111111\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 9.1713\n",
      "\n",
      "Test on train set: loss= 0.27486732602119446 acc= 0.9822758436203003 auc= 0.9893226330077886\n",
      "Test on valid set: loss= 3.162368059158325 acc= 0.800000011920929 auc= 0.9091111111111111\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 9.7224\n",
      "\n",
      "Test on train set: loss= 0.24143004417419434 acc= 0.984218180179596 auc= 0.9906480034134806\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.8882222222222221\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.6989\n",
      "\n",
      "Test on train set: loss= 0.23384509980678558 acc= 0.984703779220581 auc= 0.9918033388621627\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.8988888888888888\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 8.2200\n",
      "\n",
      "Test on train set: loss= 0.2266974300146103 acc= 0.9855130910873413 auc= 0.9918053360076332\n",
      "Test on valid set: loss= 2.901257276535034 acc= 0.8199999928474426 auc= 0.9\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 7.9543\n",
      "\n",
      "Test on train set: loss= 0.21753428876399994 acc= 0.9859986901283264 auc= 0.9924096678540891\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.8984444444444444\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.2764\n",
      "\n",
      "Test on train set: loss= 0.21032410860061646 acc= 0.9864033460617065 auc= 0.992513486461968\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.9\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 5.4655\n",
      "\n",
      "Test on train set: loss= 0.21318179368972778 acc= 0.9859177470207214 auc= 0.9906969382129205\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.888888888888889\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.3833\n",
      "\n",
      "Test on train set: loss= 0.21546447277069092 acc= 0.9858368635177612 auc= 0.9929862418005557\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7600000500679016 auc= 0.8871111111111112\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.6238\n",
      "\n",
      "Test on train set: loss= 0.18698622286319733 acc= 0.9878601431846619 auc= 0.9937265627022566\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.8908888888888888\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.9548\n",
      "\n",
      "Test on train set: loss= 0.1868634670972824 acc= 0.9879410862922668 auc= 0.9931074065967783\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.8988888888888888\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 5.6450\n",
      "\n",
      "Test on train set: loss= 0.18750007450580597 acc= 0.9879410862922668 auc= 0.9940494763168282\n",
      "Test on valid set: loss= 2.906614303588867 acc= 0.8153002858161926 auc= 0.9102222222222223\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.9600\n",
      "\n",
      "Test on train set: loss= 0.20381715893745422 acc= 0.9867271184921265 auc= 0.9942533057446793\n",
      "Test on valid set: loss= 3.5459845066070557 acc= 0.7799964547157288 auc= 0.8873333333333333\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.0211\n",
      "\n",
      "Test on train set: loss= 0.16978712379932404 acc= 0.9889122843742371 auc= 0.9950280658954384\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8757777777777779\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.1920\n",
      "\n",
      "Test on train set: loss= 0.1731448769569397 acc= 0.9889122843742371 auc= 0.9939110994188447\n",
      "Test on valid set: loss= 3.2236196994781494 acc= 0.7999995350837708 auc= 0.8891111111111112\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.0621\n",
      "\n",
      "Test on train set: loss= 0.15787017345428467 acc= 0.9898025393486023 auc= 0.9942076287340325\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.8777777777777779\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.3886\n",
      "\n",
      "Test on train set: loss= 0.1735893040895462 acc= 0.9887503981590271 auc= 0.9957314460164396\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.888888888888889\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.4975\n",
      "\n",
      "Test on train set: loss= 0.1661699414253235 acc= 0.9889122843742371 auc= 0.9959258399916491\n",
      "Test on valid set: loss= 3.628230094909668 acc= 0.7603273391723633 auc= 0.8982222222222221\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.1525\n",
      "\n",
      "Test on train set: loss= 0.1535133421421051 acc= 0.9902071952819824 auc= 0.9965627733522124\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.888888888888889\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.6197\n",
      "\n",
      "Test on train set: loss= 0.16941960155963898 acc= 0.9890741109848022 auc= 0.9954483104951356\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.8886666666666667\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.8717\n",
      "\n",
      "Test on train set: loss= 0.15980805456638336 acc= 0.9896406531333923 auc= 0.9963020856068503\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8851111111111111\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.8547\n",
      "\n",
      "Test on train set: loss= 0.14414484798908234 acc= 0.9905309081077576 auc= 0.9967614568826407\n",
      "Test on valid set: loss= 3.6362359523773193 acc= 0.7507176399230957 auc= 0.8764444444444445\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.1303\n",
      "\n",
      "Test on train set: loss= 0.16349269449710846 acc= 0.9891550540924072 auc= 0.9957884703064359\n",
      "Test on valid set: loss= 3.8781888484954834 acc= 0.7522244453430176 auc= 0.8655555555555556\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.3285\n",
      "\n",
      "Test on train set: loss= 0.14485496282577515 acc= 0.9906118512153625 auc= 0.9951881456617647\n",
      "Test on valid set: loss= 3.223618984222412 acc= 0.800000011920929 auc= 0.8897777777777778\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.8450\n",
      "\n",
      "Test on train set: loss= 0.14574949443340302 acc= 0.9905309081077576 auc= 0.9945108962769582\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.8791111111111111\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.9465\n",
      "\n",
      "Test on train set: loss= 0.13432613015174866 acc= 0.9914211630821228 auc= 0.9950775474810379\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.8866666666666667\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.1900\n",
      "\n",
      "Test on train set: loss= 0.12294379621744156 acc= 0.9919877052307129 auc= 0.9968041833771902\n",
      "Test on valid set: loss= 3.8340182304382324 acc= 0.7600000500679016 auc= 0.8886666666666667\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.1481\n",
      "\n",
      "Test on train set: loss= 0.13560059666633606 acc= 0.9911783933639526 auc= 0.9951596981437841\n",
      "Test on valid set: loss= 3.545980930328369 acc= 0.7800000309944153 auc= 0.8882222222222224\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.9797\n",
      "\n",
      "Test on train set: loss= 0.11530987173318863 acc= 0.992554247379303 auc= 0.995808624750469\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8746666666666666\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.4128\n",
      "\n",
      "Test on train set: loss= 0.1181163638830185 acc= 0.992311418056488 auc= 0.9943571688703848\n",
      "Test on valid set: loss= 4.0651326179504395 acc= 0.7400010824203491 auc= 0.874\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.5787\n",
      "\n",
      "Test on train set: loss= 0.11641085892915726 acc= 0.992392361164093 auc= 0.9956952003252477\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8866666666666667\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.4396\n",
      "\n",
      "Test on train set: loss= 0.12000774592161179 acc= 0.9919877052307129 auc= 0.9954011200208338\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.4528\n",
      "\n",
      "Test on train set: loss= 0.11174160242080688 acc= 0.9926351308822632 auc= 0.9949557701056466\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8833333333333332\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.0223\n",
      "\n",
      "Test on train set: loss= 0.10353285819292068 acc= 0.9928779602050781 auc= 0.9954545908458915\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.0125\n",
      "\n",
      "Test on train set: loss= 0.10936891287565231 acc= 0.9930398464202881 auc= 0.9955133756185944\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8642222222222223\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 5.0917\n",
      "\n",
      "Test on train set: loss= 0.10968738049268723 acc= 0.9926351308822632 auc= 0.9967487218681124\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 5.0676\n",
      "\n",
      "Test on train set: loss= 0.12041563540697098 acc= 0.9921495914459229 auc= 0.9971985951396636\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 5.0584\n",
      "\n",
      "Test on train set: loss= 0.11377710103988647 acc= 0.992473304271698 auc= 0.9959607405965635\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8744444444444446\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.4705\n",
      "\n",
      "Test on train set: loss= 0.10910090804100037 acc= 0.9926351308822632 auc= 0.9968746414188926\n",
      "Test on valid set: loss= 4.148456573486328 acc= 0.7400000095367432 auc= 0.876\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 4.7779\n",
      "\n",
      "Test on train set: loss= 0.12526191771030426 acc= 0.9919067621231079 auc= 0.997608501016687\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.853111111111111\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.6910\n",
      "\n",
      "Test on train set: loss= 0.115931436419487 acc= 0.992392361164093 auc= 0.9961395075800535\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7200000286102295 auc= 0.8631111111111112\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.9776\n",
      "\n",
      "Test on train set: loss= 0.11936233937740326 acc= 0.992392361164093 auc= 0.9959348824700968\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 6.0980\n",
      "\n",
      "Test on train set: loss= 0.10982760787010193 acc= 0.9929589033126831 auc= 0.9959167400706936\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.0244\n",
      "\n",
      "Test on train set: loss= 0.11611895263195038 acc= 0.992554247379303 auc= 0.9960840328750962\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8435555555555556\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.8143\n",
      "\n",
      "Test on train set: loss= 0.11818648874759674 acc= 0.992311418056488 auc= 0.9953629221079812\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8455555555555556\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 4.2346\n",
      "\n",
      "Test on train set: loss= 0.110496886074543 acc= 0.9927160739898682 auc= 0.9972932549128318\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8455555555555556\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: 3.5911\n",
      "\n",
      "Test on train set: loss= 0.11535456031560898 acc= 0.992554247379303 auc= 0.9962726798122155\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8751111111111112\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: 3.8804\n",
      "\n",
      "Test on train set: loss= 0.11175574362277985 acc= 0.9927160739898682 auc= 0.9972410238375039\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 6.2802\n",
      "\n",
      "Test on train set: loss= 0.11936014890670776 acc= 0.992311418056488 auc= 0.9957182092729318\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.2137\n",
      "\n",
      "Test on train set: loss= 0.11134964972734451 acc= 0.9927970170974731 auc= 0.9946665774346732\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8673333333333334\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.0976\n",
      "\n",
      "Test on train set: loss= 0.11853857338428497 acc= 0.9922304749488831 auc= 0.9943513893567619\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8673333333333334\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.3617\n",
      "\n",
      "Test on train set: loss= 0.1009967178106308 acc= 0.9933635592460632 auc= 0.996723439160531\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.8100\n",
      "\n",
      "Test on train set: loss= 0.10731672495603561 acc= 0.9930398464202881 auc= 0.9969856623917257\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.2747\n",
      "\n",
      "Test on train set: loss= 0.11130387336015701 acc= 0.992554247379303 auc= 0.9972980610894906\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8571111111111112\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.1282\n",
      "\n",
      "Test on train set: loss= 0.10670960694551468 acc= 0.9927970170974731 auc= 0.9970831989907258\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.0804\n",
      "\n",
      "Test on train set: loss= 0.11425787210464478 acc= 0.992554247379303 auc= 0.9960287338016395\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.9792\n",
      "\n",
      "Test on train set: loss= 0.10233307629823685 acc= 0.9936063289642334 auc= 0.9951996537487199\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.9502\n",
      "\n",
      "Test on train set: loss= 0.10058984905481339 acc= 0.9933635592460632 auc= 0.9975476764807777\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.1494\n",
      "\n",
      "Test on train set: loss= 0.10037562251091003 acc= 0.9932016730308533 auc= 0.9971306870009876\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.5036\n",
      "\n",
      "Test on train set: loss= 0.09958896040916443 acc= 0.9932826161384583 auc= 0.9967069604534619\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.0060\n",
      "\n",
      "Test on train set: loss= 0.10232247412204742 acc= 0.9932826161384583 auc= 0.9966843046039482\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.5813\n",
      "\n",
      "Test on train set: loss= 0.11759835481643677 acc= 0.992473304271698 auc= 0.9975667030820518\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8644444444444446\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.5023\n",
      "\n",
      "Test on train set: loss= 0.09878837317228317 acc= 0.9935253858566284 auc= 0.997463050150907\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.7361\n",
      "\n",
      "Test on train set: loss= 0.10692892968654633 acc= 0.9927970170974731 auc= 0.9969639467574922\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.5084\n",
      "\n",
      "Test on train set: loss= 0.10217158496379852 acc= 0.9932826161384583 auc= 0.9970972070550248\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.3903\n",
      "\n",
      "Test on train set: loss= 0.10443481802940369 acc= 0.9927970170974731 auc= 0.996682082600822\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.7774\n",
      "\n",
      "Test on train set: loss= 0.10338195413351059 acc= 0.9932826161384583 auc= 0.995835163204006\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.3394\n",
      "\n",
      "Test on train set: loss= 0.10666655004024506 acc= 0.9927970170974731 auc= 0.9970132268731003\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 5.5561\n",
      "\n",
      "Test on train set: loss= 0.09866806119680405 acc= 0.9932826161384583 auc= 0.9963032642807921\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.7398\n",
      "\n",
      "Test on train set: loss= 0.10643243044614792 acc= 0.9931207299232483 auc= 0.9950865022508303\n",
      "Test on valid set: loss= 3.868342876434326 acc= 0.7599999904632568 auc= 0.8671111111111112\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.4645\n",
      "\n",
      "Test on train set: loss= 0.10793675482273102 acc= 0.9929589033126831 auc= 0.9967371522112989\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8644444444444446\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.2367\n",
      "\n",
      "Test on train set: loss= 0.0920943170785904 acc= 0.9940109848976135 auc= 0.9971518918726124\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9984\n",
      "\n",
      "Test on train set: loss= 0.0979304313659668 acc= 0.9932016730308533 auc= 0.9960714203741687\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.8494\n",
      "\n",
      "Test on train set: loss= 0.10638251900672913 acc= 0.9927970170974731 auc= 0.9968149750683525\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8653333333333334\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.5540\n",
      "\n",
      "Test on train set: loss= 0.08546087890863419 acc= 0.9940109848976135 auc= 0.9975665857168389\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.9360\n",
      "\n",
      "Test on train set: loss= 0.09804003685712814 acc= 0.9936872720718384 auc= 0.9972162973048782\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.7365\n",
      "\n",
      "Test on train set: loss= 0.0907774269580841 acc= 0.9940109848976135 auc= 0.9973188210450724\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.5041\n",
      "\n",
      "Test on train set: loss= 0.098444364964962 acc= 0.9935253858566284 auc= 0.9965006974274239\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.1479\n",
      "\n",
      "Test on train set: loss= 0.08846481889486313 acc= 0.9940919280052185 auc= 0.9973521270448387\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8640000000000001\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.5480\n",
      "\n",
      "Test on train set: loss= 0.09777573496103287 acc= 0.9936872720718384 auc= 0.9965664405237904\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.5365\n",
      "\n",
      "Test on train set: loss= 0.09651846438646317 acc= 0.9936063289642334 auc= 0.9972052293251116\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.0216\n",
      "\n",
      "Test on train set: loss= 0.09069614857435226 acc= 0.9940919280052185 auc= 0.9975689480643274\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.0746\n",
      "\n",
      "Test on train set: loss= 0.09261924773454666 acc= 0.9938491582870483 auc= 0.9954771409945147\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.3915\n",
      "\n",
      "Test on train set: loss= 0.08839849382638931 acc= 0.9939301013946533 auc= 0.9973073183510298\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 4.0689\n",
      "\n",
      "Test on train set: loss= 0.10503577440977097 acc= 0.9931207299232483 auc= 0.9980775477515316\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.4613\n",
      "\n",
      "Test on train set: loss= 0.08747247606515884 acc= 0.9940919280052185 auc= 0.997874640432939\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.6780\n",
      "\n",
      "Test on train set: loss= 0.08431310951709747 acc= 0.9942538142204285 auc= 0.9969508863879966\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.6265\n",
      "\n",
      "Test on train set: loss= 0.08902806788682938 acc= 0.9941728711128235 auc= 0.9972947723955798\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.1778\n",
      "\n",
      "Test on train set: loss= 0.08449047803878784 acc= 0.9942538142204285 auc= 0.9974744724368152\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.5031\n",
      "\n",
      "Test on train set: loss= 0.10209599137306213 acc= 0.9932016730308533 auc= 0.9964382009708185\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8564444444444446\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 4.4581\n",
      "\n",
      "Test on train set: loss= 0.09952886402606964 acc= 0.9934445023536682 auc= 0.9968012656015445\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8555555555555555\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.3047\n",
      "\n",
      "Test on train set: loss= 0.09926626086235046 acc= 0.9936063289642334 auc= 0.9970163512752439\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8655555555555556\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 2.6881\n",
      "\n",
      "Test on train set: loss= 0.10464753955602646 acc= 0.9932016730308533 auc= 0.9962962790659562\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.3104\n",
      "\n",
      "Test on train set: loss= 0.1009647473692894 acc= 0.9932016730308533 auc= 0.9961445577906798\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 3.9129\n",
      "\n",
      "Test on train set: loss= 0.10520724207162857 acc= 0.9928779602050781 auc= 0.9976766605410357\n",
      "Test on valid set: loss= 4.51306676864624 acc= 0.7199999690055847 auc= 0.8451111111111113\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.4570\n",
      "\n",
      "Test on train set: loss= 0.09980453550815582 acc= 0.9932826161384583 auc= 0.996989598133671\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8553333333333335\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: 2.9581\n",
      "\n",
      "Test on train set: loss= 0.0894695296883583 acc= 0.9940919280052185 auc= 0.9961926323793243\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "Reweight\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: 3.5545\n",
      "\n",
      "Test on train set: loss= 0.09023651480674744 acc= 0.9939301013946533 auc= 0.9976710277166363\n",
      "Test on valid set: loss= 4.190704822540283 acc= 0.7400000095367432 auc= 0.8646666666666667\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "metric_idx = 2\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_Reweight(beginEncodingEpoch, endEncodingEpoch,  \n",
    "                                               codingEnhancementRate, mu,\n",
    "                                               train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'Reweight')\n",
    "                        \n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "reweightWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoSen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.016666668>\n",
      "97/97 - 5s - loss: 1.5530\n",
      "\n",
      "Test on train set: loss= 1.3484238386154175 acc= 0.5787471532821655 auc= 0.7724083859897146\n",
      "Test on valid set: loss= 2.777120590209961 acc= 0.19578734040260315 auc= 0.6982222222222222\n",
      "\n",
      "Epoch 2/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.033333335>\n",
      "97/97 - 3s - loss: -1.0854e+00\n",
      "\n",
      "Test on train set: loss= 1.0714298486709595 acc= 0.6390417814254761 auc= 0.8135939240623478\n",
      "Test on valid set: loss= 3.0231456756591797 acc= 0.24761533737182617 auc= 0.7542222222222221\n",
      "\n",
      "Epoch 3/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.05>\n",
      "97/97 - 2s - loss: -1.2396e+00\n",
      "\n",
      "Test on train set: loss= 1.131277322769165 acc= 0.6381515264511108 auc= 0.7689231862655844\n",
      "Test on valid set: loss= 3.423051118850708 acc= 0.2613378167152405 auc= 0.6804444444444445\n",
      "\n",
      "Epoch 4/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.06666667>\n",
      "97/97 - 2s - loss: -1.1430e+00\n",
      "\n",
      "Test on train set: loss= 1.2326762676239014 acc= 0.6332146525382996 auc= 0.7820145691047778\n",
      "Test on valid set: loss= 3.137291193008423 acc= 0.21208766102790833 auc= 0.6764444444444445\n",
      "\n",
      "Epoch 5/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.083333336>\n",
      "97/97 - 2s - loss: -1.0954e+00\n",
      "\n",
      "Test on train set: loss= 1.2451815605163574 acc= 0.6387989521026611 auc= 0.7935327728596666\n",
      "Test on valid set: loss= 2.7080471515655518 acc= 0.27142614126205444 auc= 0.7106666666666667\n",
      "\n",
      "Epoch 6/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.7092e+00\n",
      "\n",
      "Test on train set: loss= 1.3224838972091675 acc= 0.676837146282196 auc= 0.7901189270760847\n",
      "Test on valid set: loss= 2.46390962600708 acc= 0.26761820912361145 auc= 0.7226666666666667\n",
      "\n",
      "Epoch 7/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.9180e+00\n",
      "\n",
      "Test on train set: loss= 2.909635305404663 acc= 0.5675784945487976 auc= 0.7852889515154927\n",
      "Test on valid set: loss= 2.4760751724243164 acc= 0.26558077335357666 auc= 0.7471111111111111\n",
      "\n",
      "Epoch 8/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.5942e+00\n",
      "\n",
      "Test on train set: loss= 0.8665105104446411 acc= 0.7635157108306885 auc= 0.8559073398611279\n",
      "Test on valid set: loss= 2.204160451889038 acc= 0.30236852169036865 auc= 0.7915555555555556\n",
      "\n",
      "Epoch 9/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3254e+00\n",
      "\n",
      "Test on train set: loss= 1.5157277584075928 acc= 0.7058109641075134 auc= 0.8318975468595531\n",
      "Test on valid set: loss= 3.695385456085205 acc= 0.25354552268981934 auc= 0.7382222222222222\n",
      "\n",
      "Epoch 10/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.2524e+00\n",
      "\n",
      "Test on train set: loss= 0.9801411628723145 acc= 0.7487050890922546 auc= 0.8505741261645898\n",
      "Test on valid set: loss= 2.491183280944824 acc= 0.31379371881484985 auc= 0.7577777777777778\n",
      "\n",
      "Epoch 11/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3526e+00\n",
      "\n",
      "Test on train set: loss= 1.3247371912002563 acc= 0.6720621585845947 auc= 0.8501205519945371\n",
      "Test on valid set: loss= 2.2201240062713623 acc= 0.30120012164115906 auc= 0.7835555555555556\n",
      "\n",
      "Epoch 12/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.8803e+00\n",
      "\n",
      "Test on train set: loss= 1.5001187324523926 acc= 0.6927808523178101 auc= 0.8209060934701637\n",
      "Test on valid set: loss= 2.6836931705474854 acc= 0.32931309938430786 auc= 0.728\n",
      "\n",
      "Epoch 13/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.4134e+00\n",
      "\n",
      "Test on train set: loss= 1.3767707347869873 acc= 0.7119618058204651 auc= 0.8037743179120547\n",
      "Test on valid set: loss= 3.2016611099243164 acc= 0.3130747973918915 auc= 0.7582222222222222\n",
      "\n",
      "Epoch 14/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.4847e+00\n",
      "\n",
      "Test on train set: loss= 2.027273654937744 acc= 0.6719002723693848 auc= 0.7987645567028815\n",
      "Test on valid set: loss= 2.806699275970459 acc= 0.33591264486312866 auc= 0.7657777777777778\n",
      "\n",
      "Epoch 15/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.5344e+00\n",
      "\n",
      "Test on train set: loss= 0.8955262899398804 acc= 0.7673195004463196 auc= 0.8567173077097907\n",
      "Test on valid set: loss= 2.5358033180236816 acc= 0.3320639729499817 auc= 0.7582222222222222\n",
      "\n",
      "Epoch 16/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3499e+00\n",
      "\n",
      "Test on train set: loss= 1.2431823015213013 acc= 0.7546131610870361 auc= 0.8360177296974873\n",
      "Test on valid set: loss= 3.383803606033325 acc= 0.387132465839386 auc= 0.7471111111111111\n",
      "\n",
      "Epoch 17/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.5925e+00\n",
      "\n",
      "Test on train set: loss= 1.1472538709640503 acc= 0.7593881487846375 auc= 0.8773710892627775\n",
      "Test on valid set: loss= 3.0169899463653564 acc= 0.40759044885635376 auc= 0.788\n",
      "\n",
      "Epoch 18/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9097e+00\n",
      "\n",
      "Test on train set: loss= 1.1949273347854614 acc= 0.7385885119438171 auc= 0.8576233536457496\n",
      "Test on valid set: loss= 3.9774341583251953 acc= 0.2956545054912567 auc= 0.7435555555555555\n",
      "\n",
      "Epoch 19/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.5683e+00\n",
      "\n",
      "Test on train set: loss= 1.7299493551254272 acc= 0.7364842891693115 auc= 0.8152968038700219\n",
      "Test on valid set: loss= 4.478280067443848 acc= 0.328865647315979 auc= 0.7075555555555555\n",
      "\n",
      "Epoch 20/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.9593e+00\n",
      "\n",
      "Test on train set: loss= 1.1536887884140015 acc= 0.7461961507797241 auc= 0.8673012923978941\n",
      "Test on valid set: loss= 2.9463727474212646 acc= 0.34627264738082886 auc= 0.8177777777777777\n",
      "\n",
      "Epoch 21/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.2356e+00\n",
      "\n",
      "Test on train set: loss= 1.4983617067337036 acc= 0.7596309781074524 auc= 0.8462613074753531\n",
      "Test on valid set: loss= 4.403487682342529 acc= 0.3381674587726593 auc= 0.7622222222222221\n",
      "\n",
      "Epoch 22/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.4756e+00\n",
      "\n",
      "Test on train set: loss= 1.484339714050293 acc= 0.7419877052307129 auc= 0.8393513218263028\n",
      "Test on valid set: loss= 3.066370964050293 acc= 0.3943740427494049 auc= 0.8137777777777778\n",
      "\n",
      "Epoch 23/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.2508e+00\n",
      "\n",
      "Test on train set: loss= 1.8045657873153687 acc= 0.7517805099487305 auc= 0.8451331942445229\n",
      "Test on valid set: loss= 3.956677198410034 acc= 0.36914536356925964 auc= 0.7573333333333333\n",
      "\n",
      "Epoch 24/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.5232e+00\n",
      "\n",
      "Test on train set: loss= 1.4499790668487549 acc= 0.7701521515846252 auc= 0.8499648540377223\n",
      "Test on valid set: loss= 3.6371467113494873 acc= 0.3872006833553314 auc= 0.8075555555555557\n",
      "\n",
      "Epoch 25/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.7866e+00\n",
      "\n",
      "Test on train set: loss= 2.2383644580841064 acc= 0.6884914040565491 auc= 0.8440348127856027\n",
      "Test on valid set: loss= 4.5897135734558105 acc= 0.29377931356430054 auc= 0.7724444444444443\n",
      "\n",
      "Epoch 26/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.6706e+00\n",
      "\n",
      "Test on train set: loss= 1.3449513912200928 acc= 0.7818873524665833 auc= 0.8636836449474761\n",
      "Test on valid set: loss= 3.9782543182373047 acc= 0.381156325340271 auc= 0.7466666666666668\n",
      "\n",
      "Epoch 27/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.2713e+00\n",
      "\n",
      "Test on train set: loss= 1.3817957639694214 acc= 0.6905147433280945 auc= 0.8783129799768089\n",
      "Test on valid set: loss= 3.440864324569702 acc= 0.25238659977912903 auc= 0.7506666666666667\n",
      "\n",
      "Epoch 28/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.4426e+00\n",
      "\n",
      "Test on train set: loss= 2.159271240234375 acc= 0.6905147433280945 auc= 0.8410984222449687\n",
      "Test on valid set: loss= 4.206456661224365 acc= 0.3065119683742523 auc= 0.734\n",
      "\n",
      "Epoch 29/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.6543e+00\n",
      "\n",
      "Test on train set: loss= 1.4349626302719116 acc= 0.7391550540924072 auc= 0.860115632104973\n",
      "Test on valid set: loss= 3.1405632495880127 acc= 0.32772138714790344 auc= 0.7942222222222223\n",
      "\n",
      "Epoch 30/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.0105e+00\n",
      "\n",
      "Test on train set: loss= 1.608742594718933 acc= 0.7357559204101562 auc= 0.8455196270649934\n",
      "Test on valid set: loss= 4.164577484130859 acc= 0.282169908285141 auc= 0.7355555555555556\n",
      "\n",
      "Epoch 31/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.6398e+00\n",
      "\n",
      "Test on train set: loss= 1.899267315864563 acc= 0.7334898114204407 auc= 0.783363897999109\n",
      "Test on valid set: loss= 5.442005157470703 acc= 0.29682913422584534 auc= 0.6924444444444444\n",
      "\n",
      "Epoch 32/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.5800e+00\n",
      "\n",
      "Test on train set: loss= 1.4507286548614502 acc= 0.7351084351539612 auc= 0.8705688737520051\n",
      "Test on valid set: loss= 3.9486887454986572 acc= 0.36355483531951904 auc= 0.7742222222222223\n",
      "\n",
      "Epoch 33/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1729e+01\n",
      "\n",
      "Test on train set: loss= 1.0909225940704346 acc= 0.7867432832717896 auc= 0.8729883732064684\n",
      "Test on valid set: loss= 2.529193639755249 acc= 0.41624394059181213 auc= 0.8124444444444443\n",
      "\n",
      "Epoch 34/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.2574e+01\n",
      "\n",
      "Test on train set: loss= 1.559557318687439 acc= 0.7573648691177368 auc= 0.8899942039721249\n",
      "Test on valid set: loss= 3.387117862701416 acc= 0.38666781783103943 auc= 0.7957777777777778\n",
      "\n",
      "Epoch 35/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.1367e+00\n",
      "\n",
      "Test on train set: loss= 1.2975376844406128 acc= 0.7598737478256226 auc= 0.875497717465007\n",
      "Test on valid set: loss= 3.855252742767334 acc= 0.40353959798812866 auc= 0.792888888888889\n",
      "\n",
      "Epoch 36/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0073e+01\n",
      "\n",
      "Test on train set: loss= 1.5343222618103027 acc= 0.734865665435791 auc= 0.8369968556778483\n",
      "Test on valid set: loss= 4.4827561378479 acc= 0.35997167229652405 auc= 0.736\n",
      "\n",
      "Epoch 37/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0603e+01\n",
      "\n",
      "Test on train set: loss= 2.1987171173095703 acc= 0.7099384665489197 auc= 0.8524238991540039\n",
      "Test on valid set: loss= 3.828540563583374 acc= 0.45264822244644165 auc= 0.8046666666666666\n",
      "\n",
      "Epoch 38/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3039e+01\n",
      "\n",
      "Test on train set: loss= 1.8216454982757568 acc= 0.7359986901283264 auc= 0.8686533870025611\n",
      "Test on valid set: loss= 5.47120475769043 acc= 0.2892896831035614 auc= 0.7355555555555556\n",
      "\n",
      "Epoch 39/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.5713e+00\n",
      "\n",
      "Test on train set: loss= 1.1961801052093506 acc= 0.791922926902771 auc= 0.8867628143827563\n",
      "Test on valid set: loss= 3.669102668762207 acc= 0.3990930914878845 auc= 0.7777777777777778\n",
      "\n",
      "Epoch 40/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4267e+01\n",
      "\n",
      "Test on train set: loss= 2.5497868061065674 acc= 0.71147620677948 auc= 0.8097787419345854\n",
      "Test on valid set: loss= 4.797360897064209 acc= 0.3769388794898987 auc= 0.7706666666666667\n",
      "\n",
      "Epoch 41/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.5281e+01\n",
      "\n",
      "Test on train set: loss= 1.4285625219345093 acc= 0.7570410966873169 auc= 0.9040516701304199\n",
      "Test on valid set: loss= 2.7357189655303955 acc= 0.4941551685333252 auc= 0.8302222222222222\n",
      "\n",
      "Epoch 42/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0817e+01\n",
      "\n",
      "Test on train set: loss= 1.0651274919509888 acc= 0.807704746723175 auc= 0.8944867476419114\n",
      "Test on valid set: loss= 4.125676155090332 acc= 0.4135074019432068 auc= 0.7684444444444445\n",
      "\n",
      "Epoch 43/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.0444e+00\n",
      "\n",
      "Test on train set: loss= 1.116697907447815 acc= 0.8157979846000671 auc= 0.8872034068326311\n",
      "Test on valid set: loss= 3.6944336891174316 acc= 0.40177884697914124 auc= 0.788888888888889\n",
      "\n",
      "Epoch 44/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.0533e+00\n",
      "\n",
      "Test on train set: loss= 1.5203711986541748 acc= 0.7975882291793823 auc= 0.8335144168613885\n",
      "Test on valid set: loss= 4.596782684326172 acc= 0.39345452189445496 auc= 0.7577777777777778\n",
      "\n",
      "Epoch 45/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.9281e+00\n",
      "\n",
      "Test on train set: loss= 1.305979609489441 acc= 0.7798640131950378 auc= 0.8755407384407586\n",
      "Test on valid set: loss= 3.6109349727630615 acc= 0.48559847474098206 auc= 0.8235555555555555\n",
      "\n",
      "Epoch 46/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1399e+01\n",
      "\n",
      "Test on train set: loss= 1.4001778364181519 acc= 0.7932178974151611 auc= 0.8445215053181485\n",
      "Test on valid set: loss= 4.647552013397217 acc= 0.4131588935852051 auc= 0.7417777777777779\n",
      "\n",
      "Epoch 47/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.1473e+00\n",
      "\n",
      "Test on train set: loss= 1.202615737915039 acc= 0.8075429201126099 auc= 0.9177143466106898\n",
      "Test on valid set: loss= 4.004759311676025 acc= 0.36743447184562683 auc= 0.8324444444444443\n",
      "\n",
      "Epoch 48/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.7511e+00\n",
      "\n",
      "Test on train set: loss= 1.3954676389694214 acc= 0.7862576842308044 auc= 0.8837768546960385\n",
      "Test on valid set: loss= 4.495456695556641 acc= 0.4033412039279938 auc= 0.7631111111111111\n",
      "\n",
      "Epoch 49/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.6853e+01\n",
      "\n",
      "Test on train set: loss= 2.3892672061920166 acc= 0.7179508209228516 auc= 0.891919270881506\n",
      "Test on valid set: loss= 3.858701467514038 acc= 0.43659478425979614 auc= 0.7951111111111112\n",
      "\n",
      "Epoch 50/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.8200e+01\n",
      "\n",
      "Test on train set: loss= 1.0329954624176025 acc= 0.8286662101745605 auc= 0.9055610039741542\n",
      "Test on valid set: loss= 4.266392230987549 acc= 0.4132999777793884 auc= 0.7715555555555556\n",
      "\n",
      "Epoch 51/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.7005e+01\n",
      "\n",
      "Test on train set: loss= 2.4991209506988525 acc= 0.703949511051178 auc= 0.8623014506011624\n",
      "Test on valid set: loss= 7.309418201446533 acc= 0.3317181468009949 auc= 0.7895555555555556\n",
      "\n",
      "Epoch 52/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.3988e+01\n",
      "\n",
      "Test on train set: loss= 2.0127503871917725 acc= 0.7567983269691467 auc= 0.8972137249521015\n",
      "Test on valid set: loss= 5.146059513092041 acc= 0.4403161406517029 auc= 0.8\n",
      "\n",
      "Epoch 53/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.0361e+01\n",
      "\n",
      "Test on train set: loss= 1.9484487771987915 acc= 0.7192457318305969 auc= 0.8950572376416124\n",
      "Test on valid set: loss= 5.013272762298584 acc= 0.3550589680671692 auc= 0.796\n",
      "\n",
      "Epoch 54/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.6855e+01\n",
      "\n",
      "Test on train set: loss= 1.5668381452560425 acc= 0.7556652426719666 auc= 0.8941577672407988\n",
      "Test on valid set: loss= 3.3475513458251953 acc= 0.3922725021839142 auc= 0.8319999999999999\n",
      "\n",
      "Epoch 55/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.5342e+01\n",
      "\n",
      "Test on train set: loss= 2.1475696563720703 acc= 0.7372936010360718 auc= 0.8589907027602915\n",
      "Test on valid set: loss= 5.195083141326904 acc= 0.32875126600265503 auc= 0.772\n",
      "\n",
      "Epoch 56/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.8560e+01\n",
      "\n",
      "Test on train set: loss= 1.590993881225586 acc= 0.8028488159179688 auc= 0.8597735926410666\n",
      "Test on valid set: loss= 5.5502471923828125 acc= 0.4431251883506775 auc= 0.7731111111111112\n",
      "\n",
      "Epoch 57/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.9030e+01\n",
      "\n",
      "Test on train set: loss= 1.8696963787078857 acc= 0.7668339014053345 auc= 0.8891302549977469\n",
      "Test on valid set: loss= 3.9753615856170654 acc= 0.4265472888946533 auc= 0.8124444444444444\n",
      "\n",
      "Epoch 58/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.5149e+01\n",
      "\n",
      "Test on train set: loss= 1.970353364944458 acc= 0.7704758644104004 auc= 0.8549018643447471\n",
      "Test on valid set: loss= 4.9052276611328125 acc= 0.3333825469017029 auc= 0.7871111111111111\n",
      "\n",
      "Epoch 59/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.2766e+01\n",
      "\n",
      "Test on train set: loss= 1.801938533782959 acc= 0.7621398568153381 auc= 0.8963395085428065\n",
      "Test on valid set: loss= 5.206210136413574 acc= 0.3693141043186188 auc= 0.8235555555555555\n",
      "\n",
      "Epoch 60/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.1959e+01\n",
      "\n",
      "Test on train set: loss= 1.9129109382629395 acc= 0.754208505153656 auc= 0.8317849856700266\n",
      "Test on valid set: loss= 6.156160831451416 acc= 0.3318292200565338 auc= 0.7606666666666666\n",
      "\n",
      "Epoch 61/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.9416e+01\n",
      "\n",
      "Test on train set: loss= 1.8791686296463013 acc= 0.7703140377998352 auc= 0.8962639561165607\n",
      "Test on valid set: loss= 5.220586776733398 acc= 0.34867987036705017 auc= 0.7708888888888888\n",
      "\n",
      "Epoch 62/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.9829e+01\n",
      "\n",
      "Test on train set: loss= 1.616486668586731 acc= 0.7840725183486938 auc= 0.8716294971261511\n",
      "Test on valid set: loss= 4.877779006958008 acc= 0.44913774728775024 auc= 0.8111111111111111\n",
      "\n",
      "Epoch 63/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.7212e+01\n",
      "\n",
      "Test on train set: loss= 2.231743097305298 acc= 0.7680479288101196 auc= 0.8954247034836692\n",
      "Test on valid set: loss= 5.0398759841918945 acc= 0.48776668310165405 auc= 0.8297777777777778\n",
      "\n",
      "Epoch 64/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.7187e+01\n",
      "\n",
      "Test on train set: loss= 2.129514455795288 acc= 0.7771932482719421 auc= 0.8880078215868854\n",
      "Test on valid set: loss= 5.572884559631348 acc= 0.4261043071746826 auc= 0.8208888888888888\n",
      "\n",
      "Epoch 65/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.6567e+01\n",
      "\n",
      "Test on train set: loss= 2.3195364475250244 acc= 0.7546941041946411 auc= 0.876620301504003\n",
      "Test on valid set: loss= 7.007040023803711 acc= 0.3409181535243988 auc= 0.784888888888889\n",
      "\n",
      "Epoch 66/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.8435e+01\n",
      "\n",
      "Test on train set: loss= 2.0824615955352783 acc= 0.7791356444358826 auc= 0.883074647689946\n",
      "Test on valid set: loss= 4.312499523162842 acc= 0.45269790291786194 auc= 0.82\n",
      "\n",
      "Epoch 67/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.5761e+01\n",
      "\n",
      "Test on train set: loss= 1.989702820777893 acc= 0.7787309885025024 auc= 0.8866686473424845\n",
      "Test on valid set: loss= 5.431394100189209 acc= 0.4540706276893616 auc= 0.7615555555555555\n",
      "\n",
      "Epoch 68/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.5918e+01\n",
      "\n",
      "Test on train set: loss= 2.262094020843506 acc= 0.7442538142204285 auc= 0.8789973584545765\n",
      "Test on valid set: loss= 4.679595947265625 acc= 0.4456750452518463 auc= 0.8182222222222222\n",
      "\n",
      "Epoch 69/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.5144e+01\n",
      "\n",
      "Test on train set: loss= 1.846077561378479 acc= 0.8086759448051453 auc= 0.8460208329080878\n",
      "Test on valid set: loss= 4.612182140350342 acc= 0.4232356548309326 auc= 0.7904444444444445\n",
      "\n",
      "Epoch 70/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1835e+01\n",
      "\n",
      "Test on train set: loss= 1.6225255727767944 acc= 0.7793784141540527 auc= 0.8870106343891757\n",
      "Test on valid set: loss= 4.964247703552246 acc= 0.4049805998802185 auc= 0.7906666666666666\n",
      "\n",
      "Epoch 71/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3270e+01\n",
      "\n",
      "Test on train set: loss= 1.228980541229248 acc= 0.8447717428207397 auc= 0.9175861660345482\n",
      "Test on valid set: loss= 4.5808796882629395 acc= 0.4731614589691162 auc= 0.8004444444444445\n",
      "\n",
      "Epoch 72/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.1339e+01\n",
      "\n",
      "Test on train set: loss= 1.3818457126617432 acc= 0.7948365211486816 auc= 0.9006793627839007\n",
      "Test on valid set: loss= 4.086958408355713 acc= 0.4351215958595276 auc= 0.8124444444444444\n",
      "\n",
      "Epoch 73/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.6925e+01\n",
      "\n",
      "Test on train set: loss= 1.135101079940796 acc= 0.8543217778205872 auc= 0.9285953691892231\n",
      "Test on valid set: loss= 4.643674373626709 acc= 0.47877129912376404 auc= 0.8191111111111111\n",
      "\n",
      "Epoch 74/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.1499e+01\n",
      "\n",
      "Test on train set: loss= 1.470555067062378 acc= 0.8208158016204834 auc= 0.8978110484427428\n",
      "Test on valid set: loss= 6.289999485015869 acc= 0.39637377858161926 auc= 0.7937777777777779\n",
      "\n",
      "Epoch 75/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.4415e+01\n",
      "\n",
      "Test on train set: loss= 1.0891807079315186 acc= 0.8297183513641357 auc= 0.9295061441038746\n",
      "Test on valid set: loss= 3.6194846630096436 acc= 0.5304211378097534 auc= 0.8695555555555554\n",
      "\n",
      "Epoch 76/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.8676e+01\n",
      "\n",
      "Test on train set: loss= 1.690795660018921 acc= 0.8034153580665588 auc= 0.897928254629216\n",
      "Test on valid set: loss= 5.2962117195129395 acc= 0.4428088665008545 auc= 0.8228888888888889\n",
      "\n",
      "Epoch 77/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.8382e+01\n",
      "\n",
      "Test on train set: loss= 1.4715715646743774 acc= 0.8050339818000793 auc= 0.9203917218364033\n",
      "Test on valid set: loss= 5.629773139953613 acc= 0.44041794538497925 auc= 0.8428888888888888\n",
      "\n",
      "Epoch 78/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.3829e+01\n",
      "\n",
      "Test on train set: loss= 1.1316685676574707 acc= 0.8522175550460815 auc= 0.9429985527386867\n",
      "Test on valid set: loss= 4.311684608459473 acc= 0.41541415452957153 auc= 0.8611111111111113\n",
      "\n",
      "Epoch 79/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.4395e+01\n",
      "\n",
      "Test on train set: loss= 1.3244200944900513 acc= 0.8365166783332825 auc= 0.9258159030891944\n",
      "Test on valid set: loss= 3.2897393703460693 acc= 0.5456386804580688 auc= 0.8837777777777778\n",
      "\n",
      "Epoch 80/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.2884e+01\n",
      "\n",
      "Test on train set: loss= 1.6454441547393799 acc= 0.8277759552001953 auc= 0.9019186616748485\n",
      "Test on valid set: loss= 4.743710041046143 acc= 0.47770771384239197 auc= 0.8384444444444444\n",
      "\n",
      "Epoch 81/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -2.6130e+01\n",
      "\n",
      "Test on train set: loss= 2.0915606021881104 acc= 0.7790547013282776 auc= 0.9076117326977776\n",
      "Test on valid set: loss= 4.681339263916016 acc= 0.44747692346572876 auc= 0.8355555555555556\n",
      "\n",
      "Epoch 82/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.2854e+01\n",
      "\n",
      "Test on train set: loss= 2.0392889976501465 acc= 0.7871479392051697 auc= 0.9133720671361752\n",
      "Test on valid set: loss= 5.552862644195557 acc= 0.4320080280303955 auc= 0.8548888888888889\n",
      "\n",
      "Epoch 83/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.6643e+01\n",
      "\n",
      "Test on train set: loss= 1.8167824745178223 acc= 0.8005017638206482 auc= 0.89260475270314\n",
      "Test on valid set: loss= 4.19993257522583 acc= 0.47363191843032837 auc= 0.8493333333333334\n",
      "\n",
      "Epoch 84/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3225e+01\n",
      "\n",
      "Test on train set: loss= 1.4404319524765015 acc= 0.80794757604599 auc= 0.9281965316813807\n",
      "Test on valid set: loss= 3.9833319187164307 acc= 0.44066470861434937 auc= 0.820888888888889\n",
      "\n",
      "Epoch 85/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.6946e+01\n",
      "\n",
      "Test on train set: loss= 1.2603832483291626 acc= 0.8579637408256531 auc= 0.9234251362593252\n",
      "Test on valid set: loss= 5.287064075469971 acc= 0.4980984330177307 auc= 0.8046666666666666\n",
      "\n",
      "Epoch 86/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.0831e+01\n",
      "\n",
      "Test on train set: loss= 1.4862098693847656 acc= 0.8511654138565063 auc= 0.91690065628853\n",
      "Test on valid set: loss= 4.310671329498291 acc= 0.555884838104248 auc= 0.847111111111111\n",
      "\n",
      "Epoch 87/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.9309e+01\n",
      "\n",
      "Test on train set: loss= 1.8640333414077759 acc= 0.8244577646255493 auc= 0.8937360195797194\n",
      "Test on valid set: loss= 6.231755256652832 acc= 0.46933263540267944 auc= 0.8128888888888888\n",
      "\n",
      "Epoch 88/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.4539e+01\n",
      "\n",
      "Test on train set: loss= 2.096325397491455 acc= 0.8104564547538757 auc= 0.8632396335881423\n",
      "Test on valid set: loss= 5.826797962188721 acc= 0.46716243028640747 auc= 0.8133333333333332\n",
      "\n",
      "Epoch 89/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.4135e+01\n",
      "\n",
      "Test on train set: loss= 1.533658742904663 acc= 0.8374069333076477 auc= 0.907479477720601\n",
      "Test on valid set: loss= 4.734911918640137 acc= 0.5086213946342468 auc= 0.866888888888889\n",
      "\n",
      "Epoch 90/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.7345e+01\n",
      "\n",
      "Test on train set: loss= 1.6895157098770142 acc= 0.8399158120155334 auc= 0.8831716863974182\n",
      "Test on valid set: loss= 7.222925186157227 acc= 0.42486435174942017 auc= 0.8231111111111111\n",
      "\n",
      "Epoch 91/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.2865e+01\n",
      "\n",
      "Test on train set: loss= 1.9393789768218994 acc= 0.7584978938102722 auc= 0.9107783498453559\n",
      "Test on valid set: loss= 5.620671272277832 acc= 0.4254828989505768 auc= 0.8222222222222222\n",
      "\n",
      "Epoch 92/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.2236e+01\n",
      "\n",
      "Test on train set: loss= 1.5598914623260498 acc= 0.8389446139335632 auc= 0.8916623246004838\n",
      "Test on valid set: loss= 5.206697940826416 acc= 0.4567248225212097 auc= 0.8535555555555556\n",
      "\n",
      "Epoch 93/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -3.9496e+01\n",
      "\n",
      "Test on train set: loss= 1.7792863845825195 acc= 0.8366785645484924 auc= 0.8832354081198996\n",
      "Test on valid set: loss= 5.765917778015137 acc= 0.4633997976779938 auc= 0.7771111111111112\n",
      "\n",
      "Epoch 94/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.2275e+01\n",
      "\n",
      "Test on train set: loss= 2.067028045654297 acc= 0.8087568879127502 auc= 0.8856120390958401\n",
      "Test on valid set: loss= 5.575546741485596 acc= 0.40252965688705444 auc= 0.8477777777777777\n",
      "\n",
      "Epoch 95/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.5701e+01\n",
      "\n",
      "Test on train set: loss= 1.842525839805603 acc= 0.8220297694206238 auc= 0.9243727982614525\n",
      "Test on valid set: loss= 4.530523777008057 acc= 0.5285002589225769 auc= 0.861111111111111\n",
      "\n",
      "Epoch 96/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.0723e+01\n",
      "\n",
      "Test on train set: loss= 1.7656856775283813 acc= 0.8406442403793335 auc= 0.8831108128583767\n",
      "Test on valid set: loss= 5.12470817565918 acc= 0.5393315553665161 auc= 0.8475555555555555\n",
      "\n",
      "Epoch 97/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.6433e+01\n",
      "\n",
      "Test on train set: loss= 1.5080980062484741 acc= 0.8488993048667908 auc= 0.8961465380895411\n",
      "Test on valid set: loss= 5.6269659996032715 acc= 0.5146273374557495 auc= 0.8148888888888889\n",
      "\n",
      "Epoch 98/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.6082e+01\n",
      "\n",
      "Test on train set: loss= 2.5888872146606445 acc= 0.7657008767127991 auc= 0.8983958381601529\n",
      "Test on valid set: loss= 4.936083793640137 acc= 0.523255467414856 auc= 0.8553333333333333\n",
      "\n",
      "Epoch 99/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.2286e+01\n",
      "\n",
      "Test on train set: loss= 1.692090392112732 acc= 0.8356264233589172 auc= 0.8758386503549215\n",
      "Test on valid set: loss= 6.009776592254639 acc= 0.4690369963645935 auc= 0.8175555555555556\n",
      "\n",
      "Epoch 100/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.5779e+01\n",
      "\n",
      "Test on train set: loss= 1.684940218925476 acc= 0.8131272196769714 auc= 0.9255842578899344\n",
      "Test on valid set: loss= 5.085023403167725 acc= 0.5406469106674194 auc= 0.8513333333333334\n",
      "\n",
      "Epoch 101/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.1892e+01\n",
      "\n",
      "Test on train set: loss= 2.026017189025879 acc= 0.8121560215950012 auc= 0.8758832897544876\n",
      "Test on valid set: loss= 5.038546562194824 acc= 0.5298940539360046 auc= 0.8484444444444443\n",
      "\n",
      "Epoch 102/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.3116e+01\n",
      "\n",
      "Test on train set: loss= 1.4648674726486206 acc= 0.8355454802513123 auc= 0.922977694448817\n",
      "Test on valid set: loss= 4.4244384765625 acc= 0.5804396271705627 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 103/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.9889e+01\n",
      "\n",
      "Test on train set: loss= 1.9998691082000732 acc= 0.8085950016975403 auc= 0.8682944744678448\n",
      "Test on valid set: loss= 5.669266223907471 acc= 0.4778386652469635 auc= 0.8526666666666667\n",
      "\n",
      "Epoch 104/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.1271e+01\n",
      "\n",
      "Test on train set: loss= 1.7749377489089966 acc= 0.819601833820343 auc= 0.9153279814474965\n",
      "Test on valid set: loss= 7.206042289733887 acc= 0.4362613260746002 auc= 0.8273333333333334\n",
      "\n",
      "Epoch 105/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.8415e+01\n",
      "\n",
      "Test on train set: loss= 1.920166254043579 acc= 0.8164454698562622 auc= 0.8510802566690325\n",
      "Test on valid set: loss= 7.813243389129639 acc= 0.4178287982940674 auc= 0.7168888888888889\n",
      "\n",
      "Epoch 106/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.8116e+01\n",
      "\n",
      "Test on train set: loss= 1.299523115158081 acc= 0.838135302066803 auc= 0.9305096530771653\n",
      "Test on valid set: loss= 4.668304920196533 acc= 0.5723917484283447 auc= 0.8480000000000001\n",
      "\n",
      "Epoch 107/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.4893e+01\n",
      "\n",
      "Test on train set: loss= 1.7851139307022095 acc= 0.8275331854820251 auc= 0.9036053314338961\n",
      "Test on valid set: loss= 6.766131401062012 acc= 0.4825119972229004 auc= 0.7959999999999999\n",
      "\n",
      "Epoch 108/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -2.9550e+01\n",
      "\n",
      "Test on train set: loss= 1.3935251235961914 acc= 0.8478472232818604 auc= 0.9008590364808835\n",
      "Test on valid set: loss= 6.613918304443359 acc= 0.4577772617340088 auc= 0.8015555555555556\n",
      "\n",
      "Epoch 109/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.7832e+01\n",
      "\n",
      "Test on train set: loss= 2.6152255535125732 acc= 0.7713661193847656 auc= 0.8335300520515367\n",
      "Test on valid set: loss= 8.260252952575684 acc= 0.3857995569705963 auc= 0.782888888888889\n",
      "\n",
      "Epoch 110/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -3.7232e+01\n",
      "\n",
      "Test on train set: loss= 2.2397537231445312 acc= 0.8005017638206482 auc= 0.8947381984180105\n",
      "Test on valid set: loss= 7.506953716278076 acc= 0.46123963594436646 auc= 0.8017777777777779\n",
      "\n",
      "Epoch 111/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.1772e+01\n",
      "\n",
      "Test on train set: loss= 3.0960447788238525 acc= 0.7378601431846619 auc= 0.8128892652301115\n",
      "Test on valid set: loss= 7.703676700592041 acc= 0.35334378480911255 auc= 0.7682222222222223\n",
      "\n",
      "Epoch 112/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -4.3759e+01\n",
      "\n",
      "Test on train set: loss= 2.002439260482788 acc= 0.8206539154052734 auc= 0.9204865739742084\n",
      "Test on valid set: loss= 5.379488468170166 acc= 0.5309532284736633 auc= 0.8666666666666666\n",
      "\n",
      "Epoch 113/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.5235e+01\n",
      "\n",
      "Test on train set: loss= 2.167214870452881 acc= 0.8051149249076843 auc= 0.878386071089199\n",
      "Test on valid set: loss= 6.709997653961182 acc= 0.5113476514816284 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 114/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.4688e+01\n",
      "\n",
      "Test on train set: loss= 1.6354997158050537 acc= 0.8480899930000305 auc= 0.9420344738214617\n",
      "Test on valid set: loss= 5.116267681121826 acc= 0.5908534526824951 auc= 0.8817777777777778\n",
      "\n",
      "Epoch 115/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.4983e+01\n",
      "\n",
      "Test on train set: loss= 2.5372467041015625 acc= 0.811346709728241 auc= 0.8677959610529549\n",
      "Test on valid set: loss= 7.752291202545166 acc= 0.4828709661960602 auc= 0.8255555555555556\n",
      "\n",
      "Epoch 116/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.8248e+01\n",
      "\n",
      "Test on train set: loss= 1.66144859790802 acc= 0.8475234508514404 auc= 0.9306463862043468\n",
      "Test on valid set: loss= 4.970763206481934 acc= 0.5705240368843079 auc= 0.8626666666666667\n",
      "\n",
      "Epoch 117/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.1195e+01\n",
      "\n",
      "Test on train set: loss= 2.209972381591797 acc= 0.8326319456100464 auc= 0.9209875259067092\n",
      "Test on valid set: loss= 6.890396595001221 acc= 0.5193406343460083 auc= 0.8511111111111112\n",
      "\n",
      "Epoch 118/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.3354e+01\n",
      "\n",
      "Test on train set: loss= 2.1993839740753174 acc= 0.8313369750976562 auc= 0.8755409945089475\n",
      "Test on valid set: loss= 7.9688639640808105 acc= 0.4687100946903229 auc= 0.7826666666666668\n",
      "\n",
      "Epoch 119/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.4214e+01\n",
      "\n",
      "Test on train set: loss= 1.7476717233657837 acc= 0.8545646071434021 auc= 0.8887376910719059\n",
      "Test on valid set: loss= 6.724605560302734 acc= 0.5120571851730347 auc= 0.8111111111111111\n",
      "\n",
      "Epoch 120/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.2359e+01\n",
      "\n",
      "Test on train set: loss= 1.645127773284912 acc= 0.861524760723114 auc= 0.9311150424467065\n",
      "Test on valid set: loss= 5.179896354675293 acc= 0.6068952083587646 auc= 0.8571111111111112\n",
      "\n",
      "Epoch 121/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.8941e+01\n",
      "\n",
      "Test on train set: loss= 1.4173550605773926 acc= 0.8755260705947876 auc= 0.9492446800198918\n",
      "Test on valid set: loss= 4.663948059082031 acc= 0.61961430311203 auc= 0.8940000000000001\n",
      "\n",
      "Epoch 122/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.4639e+01\n",
      "\n",
      "Test on train set: loss= 1.9870128631591797 acc= 0.8376497030258179 auc= 0.9394802047182761\n",
      "Test on valid set: loss= 6.233505725860596 acc= 0.47252926230430603 auc= 0.8606666666666667\n",
      "\n",
      "Epoch 123/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.6769e+01\n",
      "\n",
      "Test on train set: loss= 1.3704692125320435 acc= 0.8759307265281677 auc= 0.9445455818394226\n",
      "Test on valid set: loss= 5.089768409729004 acc= 0.6224314570426941 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 124/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -7.9101e+01\n",
      "\n",
      "Test on train set: loss= 1.6363986730575562 acc= 0.8623340725898743 auc= 0.9375907049229779\n",
      "Test on valid set: loss= 5.495593070983887 acc= 0.5323795080184937 auc= 0.8700000000000001\n",
      "\n",
      "Epoch 125/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.0350e+01\n",
      "\n",
      "Test on train set: loss= 1.7006785869598389 acc= 0.8544836640357971 auc= 0.9210379527914174\n",
      "Test on valid set: loss= 5.827993392944336 acc= 0.537032961845398 auc= 0.8522222222222222\n",
      "\n",
      "Epoch 126/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.4021e+01\n",
      "\n",
      "Test on train set: loss= 1.4283339977264404 acc= 0.8640336394309998 auc= 0.9337021149500465\n",
      "Test on valid set: loss= 5.445196151733398 acc= 0.5484017133712769 auc= 0.8693333333333333\n",
      "\n",
      "Epoch 127/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -4.4813e+01\n",
      "\n",
      "Test on train set: loss= 1.3408887386322021 acc= 0.880786657333374 auc= 0.9540417608092635\n",
      "Test on valid set: loss= 5.798244476318359 acc= 0.5481294393539429 auc= 0.874\n",
      "\n",
      "Epoch 128/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.2178e+01\n",
      "\n",
      "Test on train set: loss= 1.386715292930603 acc= 0.8647620677947998 auc= 0.9377756022125473\n",
      "Test on valid set: loss= 4.520036220550537 acc= 0.6032375693321228 auc= 0.8975555555555556\n",
      "\n",
      "Epoch 129/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.4890e+01\n",
      "\n",
      "Test on train set: loss= 1.9032108783721924 acc= 0.8299611806869507 auc= 0.9148306211321025\n",
      "Test on valid set: loss= 6.766400337219238 acc= 0.46480733156204224 auc= 0.8722222222222221\n",
      "\n",
      "Epoch 130/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.7360e+01\n",
      "\n",
      "Test on train set: loss= 1.7065459489822388 acc= 0.8626578450202942 auc= 0.932504296316756\n",
      "Test on valid set: loss= 7.242220878601074 acc= 0.48174434900283813 auc= 0.8624444444444445\n",
      "\n",
      "Epoch 131/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1826e+02\n",
      "\n",
      "Test on train set: loss= 2.0350916385650635 acc= 0.8311751484870911 auc= 0.9002880944013449\n",
      "Test on valid set: loss= 4.91837739944458 acc= 0.6077308058738708 auc= 0.8804444444444444\n",
      "\n",
      "Epoch 132/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.0981e+01\n",
      "\n",
      "Test on train set: loss= 1.809403419494629 acc= 0.8514082431793213 auc= 0.8858960832469405\n",
      "Test on valid set: loss= 6.339815616607666 acc= 0.537458598613739 auc= 0.8204444444444444\n",
      "\n",
      "Epoch 133/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.2382e+01\n",
      "\n",
      "Test on train set: loss= 1.8108489513397217 acc= 0.8435577750205994 auc= 0.930821553486578\n",
      "Test on valid set: loss= 6.501418590545654 acc= 0.5031778812408447 auc= 0.8493333333333334\n",
      "\n",
      "Epoch 134/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.3292e+01\n",
      "\n",
      "Test on train set: loss= 1.6711686849594116 acc= 0.8530268669128418 auc= 0.9310016597346499\n",
      "Test on valid set: loss= 6.1449079513549805 acc= 0.551278293132782 auc= 0.894888888888889\n",
      "\n",
      "Epoch 135/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.6540e+01\n",
      "\n",
      "Test on train set: loss= 2.043184757232666 acc= 0.8283424973487854 auc= 0.9263783957493397\n",
      "Test on valid set: loss= 6.899481296539307 acc= 0.4791548252105713 auc= 0.8691111111111111\n",
      "\n",
      "Epoch 136/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.2915e+01\n",
      "\n",
      "Test on train set: loss= 1.6136701107025146 acc= 0.8621722459793091 auc= 0.940638154966962\n",
      "Test on valid set: loss= 5.425865650177002 acc= 0.5606374740600586 auc= 0.8853333333333332\n",
      "\n",
      "Epoch 137/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -6.6863e+01\n",
      "\n",
      "Test on train set: loss= 1.506314992904663 acc= 0.8592586517333984 auc= 0.942137448848549\n",
      "Test on valid set: loss= 6.158175468444824 acc= 0.5466997027397156 auc= 0.8764444444444444\n",
      "\n",
      "Epoch 138/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.0180e+01\n",
      "\n",
      "Test on train set: loss= 1.5729478597640991 acc= 0.8708319664001465 auc= 0.9321500423838129\n",
      "Test on valid set: loss= 5.689056873321533 acc= 0.5907551050186157 auc= 0.8462222222222223\n",
      "\n",
      "Epoch 139/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.1378e+01\n",
      "\n",
      "Test on train set: loss= 1.8248757123947144 acc= 0.8430721759796143 auc= 0.9378310027611109\n",
      "Test on valid set: loss= 5.5602707862854 acc= 0.5789538025856018 auc= 0.8977777777777778\n",
      "\n",
      "Epoch 140/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.8562e+01\n",
      "\n",
      "Test on train set: loss= 2.9324734210968018 acc= 0.7603593468666077 auc= 0.9087617557131658\n",
      "Test on valid set: loss= 7.342713832855225 acc= 0.5003450512886047 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 141/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.4929e+01\n",
      "\n",
      "Test on train set: loss= 1.5135691165924072 acc= 0.8594205379486084 auc= 0.9562253670871602\n",
      "Test on valid set: loss= 5.468543529510498 acc= 0.5798131227493286 auc= 0.8940000000000001\n",
      "\n",
      "Epoch 142/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.5761e+01\n",
      "\n",
      "Test on train set: loss= 1.5475447177886963 acc= 0.8599870800971985 auc= 0.9393283209360677\n",
      "Test on valid set: loss= 5.353614330291748 acc= 0.541246235370636 auc= 0.8455555555555556\n",
      "\n",
      "Epoch 143/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -5.4442e+01\n",
      "\n",
      "Test on train set: loss= 1.4515204429626465 acc= 0.8754451274871826 auc= 0.9510057117168331\n",
      "Test on valid set: loss= 6.03071403503418 acc= 0.5530514717102051 auc= 0.8726666666666667\n",
      "\n",
      "Epoch 144/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -5.2729e+01\n",
      "\n",
      "Test on train set: loss= 2.0829391479492188 acc= 0.8319844603538513 auc= 0.8930896304065673\n",
      "Test on valid set: loss= 6.170076847076416 acc= 0.553929328918457 auc= 0.844888888888889\n",
      "\n",
      "Epoch 145/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -6.2522e+01\n",
      "\n",
      "Test on train set: loss= 1.823270320892334 acc= 0.8533505797386169 auc= 0.9247343045867373\n",
      "Test on valid set: loss= 7.659341812133789 acc= 0.4656980633735657 auc= 0.8191111111111111\n",
      "\n",
      "Epoch 146/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.4780e+01\n",
      "\n",
      "Test on train set: loss= 1.8912004232406616 acc= 0.8488993048667908 auc= 0.9426654485122026\n",
      "Test on valid set: loss= 5.101591110229492 acc= 0.543375551700592 auc= 0.8591111111111112\n",
      "\n",
      "Epoch 147/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0745e+02\n",
      "\n",
      "Test on train set: loss= 2.2120361328125 acc= 0.8200874328613281 auc= 0.9262070515526684\n",
      "Test on valid set: loss= 5.366457462310791 acc= 0.6176380515098572 auc= 0.881111111111111\n",
      "\n",
      "Epoch 148/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.6174e+01\n",
      "\n",
      "Test on train set: loss= 1.642124891281128 acc= 0.8632243275642395 auc= 0.9573162014275823\n",
      "Test on valid set: loss= 6.37516975402832 acc= 0.5386820435523987 auc= 0.8642222222222223\n",
      "\n",
      "Epoch 149/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.3778e+01\n",
      "\n",
      "Test on train set: loss= 1.2638866901397705 acc= 0.8900129199028015 auc= 0.9578994683766068\n",
      "Test on valid set: loss= 5.458470344543457 acc= 0.5536476969718933 auc= 0.8764444444444444\n",
      "\n",
      "Epoch 150/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3377e+02\n",
      "\n",
      "Test on train set: loss= 1.4921002388000488 acc= 0.8813531994819641 auc= 0.8792683368034588\n",
      "Test on valid set: loss= 6.354481220245361 acc= 0.5993093252182007 auc= 0.8391111111111111\n",
      "\n",
      "Epoch 151/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.3496e+02\n",
      "\n",
      "Test on train set: loss= 1.9565508365631104 acc= 0.8493039608001709 auc= 0.9152488630936111\n",
      "Test on valid set: loss= 7.8866729736328125 acc= 0.4792718291282654 auc= 0.8537777777777779\n",
      "\n",
      "Epoch 152/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1846e+02\n",
      "\n",
      "Test on train set: loss= 1.7513047456741333 acc= 0.8582065105438232 auc= 0.9370883630088118\n",
      "Test on valid set: loss= 7.115055084228516 acc= 0.4925309121608734 auc= 0.8448888888888888\n",
      "\n",
      "Epoch 153/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1213e+02\n",
      "\n",
      "Test on train set: loss= 1.165425181388855 acc= 0.903609573841095 auc= 0.9642843051658637\n",
      "Test on valid set: loss= 6.286677360534668 acc= 0.49659261107444763 auc= 0.8386666666666667\n",
      "\n",
      "Epoch 154/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.4071e+02\n",
      "\n",
      "Test on train set: loss= 1.9201855659484863 acc= 0.8567497730255127 auc= 0.9593995686243411\n",
      "Test on valid set: loss= 6.083261013031006 acc= 0.5814984440803528 auc= 0.9017777777777777\n",
      "\n",
      "Epoch 155/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.4160e+02\n",
      "\n",
      "Test on train set: loss= 1.358738899230957 acc= 0.8952735662460327 auc= 0.945103535584248\n",
      "Test on valid set: loss= 5.887869358062744 acc= 0.573966920375824 auc= 0.8851111111111111\n",
      "\n",
      "Epoch 156/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.1819e+01\n",
      "\n",
      "Test on train set: loss= 2.0123863220214844 acc= 0.8434768319129944 auc= 0.9265195802886337\n",
      "Test on valid set: loss= 8.813443183898926 acc= 0.42576712369918823 auc= 0.7986666666666667\n",
      "\n",
      "Epoch 157/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.6419e+01\n",
      "\n",
      "Test on train set: loss= 2.355224132537842 acc= 0.8221107125282288 auc= 0.9089582684317736\n",
      "Test on valid set: loss= 7.536375522613525 acc= 0.4803738594055176 auc= 0.8713333333333333\n",
      "\n",
      "Epoch 158/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0592e+02\n",
      "\n",
      "Test on train set: loss= 2.5368571281433105 acc= 0.8215441703796387 auc= 0.8771616309058936\n",
      "Test on valid set: loss= 7.916347503662109 acc= 0.4896424412727356 auc= 0.8171111111111111\n",
      "\n",
      "Epoch 159/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.4644e+01\n",
      "\n",
      "Test on train set: loss= 1.5552037954330444 acc= 0.8696989417076111 auc= 0.9388884531043222\n",
      "Test on valid set: loss= 5.152331352233887 acc= 0.6046639680862427 auc= 0.8846666666666666\n",
      "\n",
      "Epoch 160/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1533e+02\n",
      "\n",
      "Test on train set: loss= 1.6014540195465088 acc= 0.8756878972053528 auc= 0.9472127684246043\n",
      "Test on valid set: loss= 4.369434833526611 acc= 0.6428143978118896 auc= 0.8942222222222223\n",
      "\n",
      "Epoch 161/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0330e+02\n",
      "\n",
      "Test on train set: loss= 1.5281364917755127 acc= 0.8834574222564697 auc= 0.9344355630470769\n",
      "Test on valid set: loss= 4.997677803039551 acc= 0.6417937278747559 auc= 0.8844444444444445\n",
      "\n",
      "Epoch 162/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -9.4920e+01\n",
      "\n",
      "Test on train set: loss= 1.4359989166259766 acc= 0.8871803283691406 auc= 0.9430617872576021\n",
      "Test on valid set: loss= 7.1796770095825195 acc= 0.5195954442024231 auc= 0.8866666666666667\n",
      "\n",
      "Epoch 163/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.9443e+01\n",
      "\n",
      "Test on train set: loss= 1.3361960649490356 acc= 0.8919553160667419 auc= 0.9454513595351907\n",
      "Test on valid set: loss= 5.602380275726318 acc= 0.5703468322753906 auc= 0.9124444444444444\n",
      "\n",
      "Epoch 164/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.6685e+01\n",
      "\n",
      "Test on train set: loss= 1.926248550415039 acc= 0.8566688299179077 auc= 0.8932872293299535\n",
      "Test on valid set: loss= 7.545453071594238 acc= 0.5022447109222412 auc= 0.8193333333333334\n",
      "\n",
      "Epoch 165/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.2021e+01\n",
      "\n",
      "Test on train set: loss= 1.4790629148483276 acc= 0.8814341425895691 auc= 0.9487884278234852\n",
      "Test on valid set: loss= 5.426752090454102 acc= 0.5789883136749268 auc= 0.8895555555555557\n",
      "\n",
      "Epoch 166/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.5747e+01\n",
      "\n",
      "Test on train set: loss= 2.893890857696533 acc= 0.7947555780410767 auc= 0.8719176744161855\n",
      "Test on valid set: loss= 7.931805610656738 acc= 0.4507189691066742 auc= 0.8313333333333333\n",
      "\n",
      "Epoch 167/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -7.5005e+01\n",
      "\n",
      "Test on train set: loss= 1.451978325843811 acc= 0.8823243975639343 auc= 0.9451475574547095\n",
      "Test on valid set: loss= 6.828979969024658 acc= 0.4919314980506897 auc= 0.8397777777777777\n",
      "\n",
      "Epoch 168/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0498e+02\n",
      "\n",
      "Test on train set: loss= 1.1299176216125488 acc= 0.9079799056053162 auc= 0.9656345489403136\n",
      "Test on valid set: loss= 5.2410197257995605 acc= 0.6086469888687134 auc= 0.8955555555555554\n",
      "\n",
      "Epoch 169/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0237e+02\n",
      "\n",
      "Test on train set: loss= 2.3831865787506104 acc= 0.818792462348938 auc= 0.9053956377150113\n",
      "Test on valid set: loss= 7.46723747253418 acc= 0.5168537497520447 auc= 0.7944444444444445\n",
      "\n",
      "Epoch 170/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.2673e+02\n",
      "\n",
      "Test on train set: loss= 1.8743789196014404 acc= 0.8637908697128296 auc= 0.9103834271457316\n",
      "Test on valid set: loss= 7.436957359313965 acc= 0.5190562009811401 auc= 0.8668888888888888\n",
      "\n",
      "Epoch 171/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.4428e+02\n",
      "\n",
      "Test on train set: loss= 2.561521291732788 acc= 0.8064907789230347 auc= 0.9298517965090396\n",
      "Test on valid set: loss= 7.057031631469727 acc= 0.5399724245071411 auc= 0.8620000000000001\n",
      "\n",
      "Epoch 172/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3783e+02\n",
      "\n",
      "Test on train set: loss= 1.7505232095718384 acc= 0.8740692734718323 auc= 0.9250025834610174\n",
      "Test on valid set: loss= 5.423223972320557 acc= 0.6271551847457886 auc= 0.8504444444444446\n",
      "\n",
      "Epoch 173/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.8561e+02\n",
      "\n",
      "Test on train set: loss= 1.4797574281692505 acc= 0.8928455710411072 auc= 0.9203402381400647\n",
      "Test on valid set: loss= 5.174090385437012 acc= 0.6277147531509399 auc= 0.8657777777777778\n",
      "\n",
      "Epoch 174/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.7835e+02\n",
      "\n",
      "Test on train set: loss= 1.555268406867981 acc= 0.8904176354408264 auc= 0.9224828437239644\n",
      "Test on valid set: loss= 5.467006206512451 acc= 0.6369229555130005 auc= 0.8731111111111112\n",
      "\n",
      "Epoch 175/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1290e+02\n",
      "\n",
      "Test on train set: loss= 1.1038973331451416 acc= 0.9158303737640381 auc= 0.9682819428772097\n",
      "Test on valid set: loss= 4.663970947265625 acc= 0.6619809865951538 auc= 0.8866666666666665\n",
      "\n",
      "Epoch 176/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.2229e+02\n",
      "\n",
      "Test on train set: loss= 1.0860494375228882 acc= 0.9176918268203735 auc= 0.9679196193589487\n",
      "Test on valid set: loss= 5.06417179107666 acc= 0.5928462743759155 auc= 0.8973333333333333\n",
      "\n",
      "Epoch 177/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0538e+02\n",
      "\n",
      "Test on train set: loss= 1.5163110494613647 acc= 0.8862900733947754 auc= 0.9577750596074098\n",
      "Test on valid set: loss= 5.156839370727539 acc= 0.6530207395553589 auc= 0.8979999999999999\n",
      "\n",
      "Epoch 178/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0442e+02\n",
      "\n",
      "Test on train set: loss= 1.552210807800293 acc= 0.8774684071540833 auc= 0.9524438270927874\n",
      "Test on valid set: loss= 5.669501304626465 acc= 0.5995588898658752 auc= 0.8602222222222224\n",
      "\n",
      "Epoch 179/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3170e+02\n",
      "\n",
      "Test on train set: loss= 1.5370362997055054 acc= 0.8833764791488647 auc= 0.9354342831115471\n",
      "Test on valid set: loss= 5.829074859619141 acc= 0.6252995133399963 auc= 0.8671111111111112\n",
      "\n",
      "Epoch 180/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.2796e+02\n",
      "\n",
      "Test on train set: loss= 1.3068606853485107 acc= 0.9062803387641907 auc= 0.9408095970178934\n",
      "Test on valid set: loss= 6.294450759887695 acc= 0.5767889022827148 auc= 0.8473333333333335\n",
      "\n",
      "Epoch 181/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1603e+02\n",
      "\n",
      "Test on train set: loss= 1.7534760236740112 acc= 0.8757688403129578 auc= 0.9293549421810546\n",
      "Test on valid set: loss= 5.865099906921387 acc= 0.601498007774353 auc= 0.861111111111111\n",
      "\n",
      "Epoch 182/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.8639e+01\n",
      "\n",
      "Test on train set: loss= 1.561455249786377 acc= 0.8866137862205505 auc= 0.9406395039349629\n",
      "Test on valid set: loss= 5.60823917388916 acc= 0.6203043460845947 auc= 0.8764444444444444\n",
      "\n",
      "Epoch 183/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.6150e+01\n",
      "\n",
      "Test on train set: loss= 1.416772484779358 acc= 0.892602801322937 auc= 0.9052248919216808\n",
      "Test on valid set: loss= 5.334987640380859 acc= 0.640701174736023 auc= 0.828888888888889\n",
      "\n",
      "Epoch 184/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.0005e+02\n",
      "\n",
      "Test on train set: loss= 1.049564003944397 acc= 0.9155875444412231 auc= 0.9617738214477315\n",
      "Test on valid set: loss= 5.976339340209961 acc= 0.5632909536361694 auc= 0.8786666666666667\n",
      "\n",
      "Epoch 185/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0849e+02\n",
      "\n",
      "Test on train set: loss= 1.4381338357925415 acc= 0.8823243975639343 auc= 0.9526206769750905\n",
      "Test on valid set: loss= 6.563772678375244 acc= 0.5607882142066956 auc= 0.852\n",
      "\n",
      "Epoch 186/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.2268e+02\n",
      "\n",
      "Test on train set: loss= 1.3651728630065918 acc= 0.8966494202613831 auc= 0.9613115734805493\n",
      "Test on valid set: loss= 7.736688137054443 acc= 0.5199974775314331 auc= 0.845111111111111\n",
      "\n",
      "Epoch 187/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1637e+02\n",
      "\n",
      "Test on train set: loss= 1.2338143587112427 acc= 0.9033668041229248 auc= 0.9659823321181931\n",
      "Test on valid set: loss= 6.242430210113525 acc= 0.579192042350769 auc= 0.8788888888888888\n",
      "\n",
      "Epoch 188/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.1503e+02\n",
      "\n",
      "Test on train set: loss= 1.8647431135177612 acc= 0.8594205379486084 auc= 0.9194676759534595\n",
      "Test on valid set: loss= 6.436395645141602 acc= 0.5658930540084839 auc= 0.8444444444444444\n",
      "\n",
      "Epoch 189/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.0611e+02\n",
      "\n",
      "Test on train set: loss= 1.655466914176941 acc= 0.8709938526153564 auc= 0.9442510780985837\n",
      "Test on valid set: loss= 6.150022029876709 acc= 0.5791906118392944 auc= 0.882\n",
      "\n",
      "Epoch 190/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.8546e+01\n",
      "\n",
      "Test on train set: loss= 1.805098533630371 acc= 0.8612819910049438 auc= 0.9462094362858939\n",
      "Test on valid set: loss= 5.218364238739014 acc= 0.6494772434234619 auc= 0.8826666666666666\n",
      "\n",
      "Epoch 191/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.9696e+01\n",
      "\n",
      "Test on train set: loss= 1.6064963340759277 acc= 0.876659095287323 auc= 0.9571643442345223\n",
      "Test on valid set: loss= 5.837825775146484 acc= 0.6000155210494995 auc= 0.8939999999999999\n",
      "\n",
      "Epoch 192/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.0629e+01\n",
      "\n",
      "Test on train set: loss= 1.8871434926986694 acc= 0.849708616733551 auc= 0.9229108847793736\n",
      "Test on valid set: loss= 6.858132839202881 acc= 0.4906846582889557 auc= 0.8337777777777777\n",
      "\n",
      "Epoch 193/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.7636e+01\n",
      "\n",
      "Test on train set: loss= 1.4373759031295776 acc= 0.8812722563743591 auc= 0.9531774913763659\n",
      "Test on valid set: loss= 5.6446213722229 acc= 0.570662796497345 auc= 0.9037777777777778\n",
      "\n",
      "Epoch 194/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -8.6486e+01\n",
      "\n",
      "Test on train set: loss= 1.206733226776123 acc= 0.9054710268974304 auc= 0.9623082729319336\n",
      "Test on valid set: loss= 5.4016876220703125 acc= 0.6117480993270874 auc= 0.8766666666666666\n",
      "\n",
      "Epoch 195/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -9.9437e+01\n",
      "\n",
      "Test on train set: loss= 1.1908437013626099 acc= 0.9104888439178467 auc= 0.9468069360179626\n",
      "Test on valid set: loss= 6.181423187255859 acc= 0.5955402255058289 auc= 0.8620000000000001\n",
      "\n",
      "Epoch 196/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 3s - loss: -1.1957e+02\n",
      "\n",
      "Test on train set: loss= 1.2760244607925415 acc= 0.9015862941741943 auc= 0.9508321858806411\n",
      "Test on valid set: loss= 6.101690769195557 acc= 0.5918982625007629 auc= 0.8993333333333334\n",
      "\n",
      "Epoch 197/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.2976e+02\n",
      "\n",
      "Test on train set: loss= 1.4473422765731812 acc= 0.8955972790718079 auc= 0.9471496092312999\n",
      "Test on valid set: loss= 6.306325912475586 acc= 0.5916353464126587 auc= 0.8948888888888888\n",
      "\n",
      "Epoch 198/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.6055e+02\n",
      "\n",
      "Test on train set: loss= 1.2128914594650269 acc= 0.9157494306564331 auc= 0.9466418616319947\n",
      "Test on valid set: loss= 8.066366195678711 acc= 0.4935499131679535 auc= 0.8551111111111112\n",
      "\n",
      "Epoch 199/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.3283e+02\n",
      "\n",
      "Test on train set: loss= 1.4092354774475098 acc= 0.8974587321281433 auc= 0.9477544630412466\n",
      "Test on valid set: loss= 6.814695358276367 acc= 0.539973795413971 auc= 0.8617777777777779\n",
      "\n",
      "Epoch 200/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.1>\n",
      "97/97 - 2s - loss: -1.6806e+02\n",
      "\n",
      "Test on train set: loss= 1.8619112968444824 acc= 0.8607963919639587 auc= 0.9388341049303588\n",
      "Test on valid set: loss= 6.47222900390625 acc= 0.5606099963188171 auc= 0.8855555555555557\n",
      "\n",
      "Epoch 201/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.6624e+02\n",
      "\n",
      "Test on train set: loss= 1.2673577070236206 acc= 0.8998866677284241 auc= 0.958000724643636\n",
      "Test on valid set: loss= 5.916113376617432 acc= 0.595801591873169 auc= 0.892\n",
      "\n",
      "Epoch 202/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.4353e+02\n",
      "\n",
      "Test on train set: loss= 1.0395375490188599 acc= 0.9210909605026245 auc= 0.9681450222520958\n",
      "Test on valid set: loss= 6.096026420593262 acc= 0.6000010371208191 auc= 0.866888888888889\n",
      "\n",
      "Epoch 203/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.2758e+02\n",
      "\n",
      "Test on train set: loss= 0.9515454173088074 acc= 0.9247329235076904 auc= 0.9724121608811848\n",
      "Test on valid set: loss= 5.312196254730225 acc= 0.6599911451339722 auc= 0.8975555555555556\n",
      "\n",
      "Epoch 204/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.2604e+02\n",
      "\n",
      "Test on train set: loss= 0.9328896999359131 acc= 0.9258659482002258 auc= 0.9668160907022738\n",
      "Test on valid set: loss= 5.274664402008057 acc= 0.6403378248214722 auc= 0.8871111111111112\n",
      "\n",
      "Epoch 205/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.2804e+02\n",
      "\n",
      "Test on train set: loss= 0.9046295881271362 acc= 0.9265943765640259 auc= 0.9636429356613485\n",
      "Test on valid set: loss= 5.509373664855957 acc= 0.6400390863418579 auc= 0.8768888888888888\n",
      "\n",
      "Epoch 206/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.2474e+02\n",
      "\n",
      "Test on train set: loss= 0.8913391828536987 acc= 0.9265943765640259 auc= 0.9665336154073778\n",
      "Test on valid set: loss= 5.481931209564209 acc= 0.6430589556694031 auc= 0.8895555555555557\n",
      "\n",
      "Epoch 207/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.2641e+02\n",
      "\n",
      "Test on train set: loss= 0.8986796736717224 acc= 0.9287795424461365 auc= 0.9684594559780189\n",
      "Test on valid set: loss= 4.976207256317139 acc= 0.6800175309181213 auc= 0.892\n",
      "\n",
      "Epoch 208/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.2049e+02\n",
      "\n",
      "Test on train set: loss= 0.8170137405395508 acc= 0.9371964931488037 auc= 0.9730884325454383\n",
      "Test on valid set: loss= 4.845407009124756 acc= 0.6614316701889038 auc= 0.8951111111111111\n",
      "\n",
      "Epoch 209/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.1909e+02\n",
      "\n",
      "Test on train set: loss= 0.7905845642089844 acc= 0.9394626021385193 auc= 0.970806872849489\n",
      "Test on valid set: loss= 5.549279689788818 acc= 0.620907187461853 auc= 0.8671111111111112\n",
      "\n",
      "Epoch 210/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.2972e+02\n",
      "\n",
      "Test on train set: loss= 0.7651057243347168 acc= 0.9410812854766846 auc= 0.964123440081025\n",
      "Test on valid set: loss= 5.406397819519043 acc= 0.6223289966583252 auc= 0.8633333333333333\n",
      "\n",
      "Epoch 211/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.4559e+02\n",
      "\n",
      "Test on train set: loss= 0.8119116425514221 acc= 0.9367918372154236 auc= 0.9648662472627189\n",
      "Test on valid set: loss= 4.9787516593933105 acc= 0.6479542851448059 auc= 0.8495555555555555\n",
      "\n",
      "Epoch 212/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.5717e+02\n",
      "\n",
      "Test on train set: loss= 0.813726544380188 acc= 0.9396244883537292 auc= 0.9481234031630251\n",
      "Test on valid set: loss= 5.633211612701416 acc= 0.6192753911018372 auc= 0.8404444444444445\n",
      "\n",
      "Epoch 213/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.4307e+02\n",
      "\n",
      "Test on train set: loss= 0.7946198582649231 acc= 0.9403528571128845 auc= 0.961362725479936\n",
      "Test on valid set: loss= 5.6047492027282715 acc= 0.6290136575698853 auc= 0.8384444444444445\n",
      "\n",
      "Epoch 214/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.4333e+02\n",
      "\n",
      "Test on train set: loss= 0.7017459869384766 acc= 0.9449660181999207 auc= 0.9663252635534413\n",
      "Test on valid set: loss= 5.700295448303223 acc= 0.6399990320205688 auc= 0.8457777777777779\n",
      "\n",
      "Epoch 215/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.4998e+02\n",
      "\n",
      "Test on train set: loss= 0.731273889541626 acc= 0.9429426789283752 auc= 0.9647924641813006\n",
      "Test on valid set: loss= 5.707068920135498 acc= 0.6362638473510742 auc= 0.8351111111111111\n",
      "\n",
      "Epoch 216/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.5222e+02\n",
      "\n",
      "Test on train set: loss= 0.7207736372947693 acc= 0.9469892978668213 auc= 0.9623456201190796\n",
      "Test on valid set: loss= 6.050875663757324 acc= 0.6122233271598816 auc= 0.8366666666666667\n",
      "\n",
      "Epoch 217/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.5178e+02\n",
      "\n",
      "Test on train set: loss= 0.678866982460022 acc= 0.9486889243125916 auc= 0.9639817436619562\n",
      "Test on valid set: loss= 5.575003623962402 acc= 0.6399945020675659 auc= 0.833777777777778\n",
      "\n",
      "Epoch 218/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.3595e+02\n",
      "\n",
      "Test on train set: loss= 0.716708779335022 acc= 0.9460180997848511 auc= 0.9633923241399254\n",
      "Test on valid set: loss= 5.9296488761901855 acc= 0.6200003623962402 auc= 0.8273333333333334\n",
      "\n",
      "Epoch 219/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.3667e+02\n",
      "\n",
      "Test on train set: loss= 0.6788090467453003 acc= 0.9485270380973816 auc= 0.9724504067220945\n",
      "Test on valid set: loss= 6.450303554534912 acc= 0.5798707604408264 auc= 0.8226666666666667\n",
      "\n",
      "Epoch 220/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.5482e+02\n",
      "\n",
      "Test on train set: loss= 0.6971640586853027 acc= 0.9471511840820312 auc= 0.9656924272086489\n",
      "Test on valid set: loss= 7.674631118774414 acc= 0.5199999213218689 auc= 0.8171111111111111\n",
      "\n",
      "Epoch 221/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.9287e+02\n",
      "\n",
      "Test on train set: loss= 0.699426531791687 acc= 0.9477177262306213 auc= 0.9529038568102983\n",
      "Test on valid set: loss= 6.776505947113037 acc= 0.5741601586341858 auc= 0.8306666666666667\n",
      "\n",
      "Epoch 222/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.2617e+02\n",
      "\n",
      "Test on train set: loss= 0.7657957673072815 acc= 0.9465846419334412 auc= 0.9699966331845248\n",
      "Test on valid set: loss= 7.540939807891846 acc= 0.5198890566825867 auc= 0.8075555555555555\n",
      "\n",
      "Epoch 223/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.3677e+02\n",
      "\n",
      "Test on train set: loss= 0.7497743368148804 acc= 0.9458562731742859 auc= 0.9642993137584677\n",
      "Test on valid set: loss= 6.163461208343506 acc= 0.6029049754142761 auc= 0.8271111111111111\n",
      "\n",
      "Epoch 224/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.2359e+02\n",
      "\n",
      "Test on train set: loss= 0.6870750784873962 acc= 0.9516024589538574 auc= 0.960430660962243\n",
      "Test on valid set: loss= 6.046304702758789 acc= 0.6200000643730164 auc= 0.8475555555555555\n",
      "\n",
      "Epoch 225/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.0698e+02\n",
      "\n",
      "Test on train set: loss= 0.6541365385055542 acc= 0.9533829689025879 auc= 0.9629994019590923\n",
      "Test on valid set: loss= 5.531347751617432 acc= 0.6415433883666992 auc= 0.8404444444444443\n",
      "\n",
      "Epoch 226/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.8304e+02\n",
      "\n",
      "Test on train set: loss= 0.6445285081863403 acc= 0.9520071148872375 auc= 0.9692905290332214\n",
      "Test on valid set: loss= 5.677545547485352 acc= 0.6083059310913086 auc= 0.8357777777777778\n",
      "\n",
      "Epoch 227/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.6514e+02\n",
      "\n",
      "Test on train set: loss= 0.7304303646087646 acc= 0.9443185329437256 auc= 0.9666962531152329\n",
      "Test on valid set: loss= 5.882175922393799 acc= 0.6148689985275269 auc= 0.8717777777777778\n",
      "\n",
      "Epoch 228/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.4797e+02\n",
      "\n",
      "Test on train set: loss= 0.6959965229034424 acc= 0.9475558400154114 auc= 0.9760837636443226\n",
      "Test on valid set: loss= 5.061132431030273 acc= 0.6517322063446045 auc= 0.8768888888888888\n",
      "\n",
      "Epoch 229/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.7434e+02\n",
      "\n",
      "Test on train set: loss= 0.6330551505088806 acc= 0.9529783129692078 auc= 0.9665463472155732\n",
      "Test on valid set: loss= 6.4136061668396 acc= 0.5646275877952576 auc= 0.8491111111111111\n",
      "\n",
      "Epoch 230/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.5671e+02\n",
      "\n",
      "Test on train set: loss= 0.688895583152771 acc= 0.9499838352203369 auc= 0.9684525134933477\n",
      "Test on valid set: loss= 6.273295879364014 acc= 0.600011944770813 auc= 0.8546666666666667\n",
      "\n",
      "Epoch 231/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.1044e+02\n",
      "\n",
      "Test on train set: loss= 0.6339967250823975 acc= 0.954030454158783 auc= 0.9603540834308282\n",
      "Test on valid set: loss= 5.696049690246582 acc= 0.6202713847160339 auc= 0.8566666666666668\n",
      "\n",
      "Epoch 232/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.2730e+02\n",
      "\n",
      "Test on train set: loss= 0.6514270305633545 acc= 0.9527354836463928 auc= 0.9582226569181284\n",
      "Test on valid set: loss= 6.630944728851318 acc= 0.5800020098686218 auc= 0.8135555555555556\n",
      "\n",
      "Epoch 233/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.9623e+02\n",
      "\n",
      "Test on train set: loss= 0.6305318474769592 acc= 0.9545160531997681 auc= 0.964901430436664\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8426666666666668\n",
      "\n",
      "Epoch 234/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.0651e+02\n",
      "\n",
      "Test on train set: loss= 0.6372867226600647 acc= 0.953949511051178 auc= 0.9601251409965696\n",
      "Test on valid set: loss= 5.868566989898682 acc= 0.6035650968551636 auc= 0.8324444444444445\n",
      "\n",
      "Epoch 235/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.1175e+02\n",
      "\n",
      "Test on train set: loss= 0.6185702085494995 acc= 0.9552444219589233 auc= 0.9685354950334919\n",
      "Test on valid set: loss= 6.447238922119141 acc= 0.6000000238418579 auc= 0.8177777777777777\n",
      "\n",
      "Epoch 236/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.9279e+02\n",
      "\n",
      "Test on train set: loss= 0.5780891180038452 acc= 0.957996129989624 auc= 0.9730822795908999\n",
      "Test on valid set: loss= 6.527275562286377 acc= 0.5800009965896606 auc= 0.8457777777777779\n",
      "\n",
      "Epoch 237/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.2470e+02\n",
      "\n",
      "Test on train set: loss= 0.5651496648788452 acc= 0.9601812958717346 auc= 0.9706569997675369\n",
      "Test on valid set: loss= 6.125045299530029 acc= 0.6198315620422363 auc= 0.8173333333333332\n",
      "\n",
      "Epoch 238/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.2058e+02\n",
      "\n",
      "Test on train set: loss= 0.5411403775215149 acc= 0.9609096646308899 auc= 0.9767064152596785\n",
      "Test on valid set: loss= 5.791316986083984 acc= 0.636949896812439 auc= 0.8446666666666666\n",
      "\n",
      "Epoch 239/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.1678e+02\n",
      "\n",
      "Test on train set: loss= 0.5770228505134583 acc= 0.9598575830459595 auc= 0.9750257474398643\n",
      "Test on valid set: loss= 6.406527042388916 acc= 0.6000000238418579 auc= 0.836888888888889\n",
      "\n",
      "Epoch 240/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.9431e+02\n",
      "\n",
      "Test on train set: loss= 0.5926692485809326 acc= 0.9571868181228638 auc= 0.9781570516304493\n",
      "Test on valid set: loss= 5.802514553070068 acc= 0.6399999856948853 auc= 0.8573333333333334\n",
      "\n",
      "Epoch 241/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -2.0304e+02\n",
      "\n",
      "Test on train set: loss= 0.6212913990020752 acc= 0.9541113376617432 auc= 0.9721056805310597\n",
      "Test on valid set: loss= 6.044671535491943 acc= 0.589096188545227 auc= 0.8031111111111111\n",
      "\n",
      "Epoch 242/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.8813e+02\n",
      "\n",
      "Test on train set: loss= 0.5554368495941162 acc= 0.9602622389793396 auc= 0.9793081181003629\n",
      "Test on valid set: loss= 5.251415252685547 acc= 0.6601842045783997 auc= 0.852\n",
      "\n",
      "Epoch 243/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.7717e+02\n",
      "\n",
      "Test on train set: loss= 0.5204310417175293 acc= 0.9619618058204651 auc= 0.9814394345299652\n",
      "Test on valid set: loss= 5.118575572967529 acc= 0.6799999475479126 auc= 0.8562222222222223\n",
      "\n",
      "Epoch 244/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.6324e+02\n",
      "\n",
      "Test on train set: loss= 0.5045840740203857 acc= 0.9608287215232849 auc= 0.9818832145141178\n",
      "Test on valid set: loss= 5.423238754272461 acc= 0.6412413120269775 auc= 0.8806666666666667\n",
      "\n",
      "Epoch 245/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.5895e+02\n",
      "\n",
      "Test on train set: loss= 0.49028900265693665 acc= 0.964875340461731 auc= 0.9822964204879483\n",
      "Test on valid set: loss= 5.948736190795898 acc= 0.6198015213012695 auc= 0.8539999999999999\n",
      "\n",
      "Epoch 246/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 3s - loss: -1.7157e+02\n",
      "\n",
      "Test on train set: loss= 0.5328618288040161 acc= 0.9590482115745544 auc= 0.9852484208230358\n",
      "Test on valid set: loss= 4.99179744720459 acc= 0.6735939979553223 auc= 0.8675555555555556\n",
      "\n",
      "Epoch 247/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.8354e+02\n",
      "\n",
      "Test on train set: loss= 0.5056623220443726 acc= 0.9616380929946899 auc= 0.9799402397898801\n",
      "Test on valid set: loss= 5.204825401306152 acc= 0.661903977394104 auc= 0.8593333333333334\n",
      "\n",
      "Epoch 248/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.9311e+02\n",
      "\n",
      "Test on train set: loss= 0.5183842182159424 acc= 0.961557149887085 auc= 0.9762020773408094\n",
      "Test on valid set: loss= 5.157790660858154 acc= 0.6799999475479126 auc= 0.8437777777777778\n",
      "\n",
      "Epoch 249/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.8192e+02\n",
      "\n",
      "Test on train set: loss= 0.4542791545391083 acc= 0.9660893678665161 auc= 0.9817099132017303\n",
      "Test on valid set: loss= 5.461277961730957 acc= 0.625478208065033 auc= 0.841777777777778\n",
      "\n",
      "Epoch 250/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n",
      "97/97 - 2s - loss: -1.7237e+02\n",
      "\n",
      "Test on train set: loss= 0.5252905488014221 acc= 0.9602622389793396 auc= 0.9808665450344309\n",
      "Test on valid set: loss= 6.124876499176025 acc= 0.6200000047683716 auc= 0.8557777777777777\n",
      "\n",
      "Epoch 251/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.6484e+02\n",
      "\n",
      "Test on train set: loss= 0.47608327865600586 acc= 0.9641469717025757 auc= 0.9804133074009662\n",
      "Test on valid set: loss= 5.832094669342041 acc= 0.6199995279312134 auc= 0.852\n",
      "\n",
      "Epoch 252/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.0201e+02\n",
      "\n",
      "Test on train set: loss= 0.4858705997467041 acc= 0.9651181697845459 auc= 0.9811114790713281\n",
      "Test on valid set: loss= 6.0773024559021 acc= 0.6199482679367065 auc= 0.853111111111111\n",
      "\n",
      "Epoch 253/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9879e+02\n",
      "\n",
      "Test on train set: loss= 0.49753546714782715 acc= 0.9632567167282104 auc= 0.9785869024215355\n",
      "Test on valid set: loss= 6.033940315246582 acc= 0.6199969053268433 auc= 0.868\n",
      "\n",
      "Epoch 254/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9792e+02\n",
      "\n",
      "Test on train set: loss= 0.5171527862548828 acc= 0.9625283479690552 auc= 0.9783821762453504\n",
      "Test on valid set: loss= 5.618243217468262 acc= 0.6167355179786682 auc= 0.8726666666666667\n",
      "\n",
      "Epoch 255/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9474e+02\n",
      "\n",
      "Test on train set: loss= 0.504288911819458 acc= 0.9631757736206055 auc= 0.9773496953474364\n",
      "Test on valid set: loss= 5.660937309265137 acc= 0.6199496984481812 auc= 0.8682222222222222\n",
      "\n",
      "Epoch 256/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9066e+02\n",
      "\n",
      "Test on train set: loss= 0.47997260093688965 acc= 0.9649562835693359 auc= 0.9798182104023174\n",
      "Test on valid set: loss= 5.193582534790039 acc= 0.6503700017929077 auc= 0.868\n",
      "\n",
      "Epoch 257/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.8861e+02\n",
      "\n",
      "Test on train set: loss= 0.5019478797912598 acc= 0.9634186029434204 auc= 0.9779001188688354\n",
      "Test on valid set: loss= 5.612779140472412 acc= 0.638772189617157 auc= 0.8651111111111112\n",
      "\n",
      "Epoch 258/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.0214e+02\n",
      "\n",
      "Test on train set: loss= 0.5155306458473206 acc= 0.9608287215232849 auc= 0.9727354507152131\n",
      "Test on valid set: loss= 5.952939510345459 acc= 0.593605637550354 auc= 0.8577777777777778\n",
      "\n",
      "Epoch 259/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9692e+02\n",
      "\n",
      "Test on train set: loss= 0.48460838198661804 acc= 0.9637423157691956 auc= 0.9769533193398857\n",
      "Test on valid set: loss= 6.031914234161377 acc= 0.6186627745628357 auc= 0.8784444444444445\n",
      "\n",
      "Epoch 260/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9901e+02\n",
      "\n",
      "Test on train set: loss= 0.5024488568305969 acc= 0.9617999196052551 auc= 0.9731912372986073\n",
      "Test on valid set: loss= 5.907315731048584 acc= 0.6200144290924072 auc= 0.8644444444444443\n",
      "\n",
      "Epoch 261/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.1757e+02\n",
      "\n",
      "Test on train set: loss= 0.6112475991249084 acc= 0.9516024589538574 auc= 0.9699971818361363\n",
      "Test on valid set: loss= 7.048030853271484 acc= 0.5400088429450989 auc= 0.8400000000000001\n",
      "\n",
      "Epoch 262/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.2538e+02\n",
      "\n",
      "Test on train set: loss= 0.646044909954071 acc= 0.9526546001434326 auc= 0.9690593146032136\n",
      "Test on valid set: loss= 6.769644737243652 acc= 0.5799555778503418 auc= 0.8433333333333334\n",
      "\n",
      "Epoch 263/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.3880e+02\n",
      "\n",
      "Test on train set: loss= 0.5108689069747925 acc= 0.961395263671875 auc= 0.9723288044937558\n",
      "Test on valid set: loss= 5.732740879058838 acc= 0.6215535998344421 auc= 0.8668888888888888\n",
      "\n",
      "Epoch 264/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.5535e+02\n",
      "\n",
      "Test on train set: loss= 0.5292080044746399 acc= 0.96147620677948 auc= 0.9684926570964212\n",
      "Test on valid set: loss= 6.124877452850342 acc= 0.6199990510940552 auc= 0.8460000000000001\n",
      "\n",
      "Epoch 265/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.2205e+02\n",
      "\n",
      "Test on train set: loss= 0.5150509476661682 acc= 0.9609906077384949 auc= 0.9718602453659665\n",
      "Test on valid set: loss= 5.950962066650391 acc= 0.6200035810470581 auc= 0.8642222222222223\n",
      "\n",
      "Epoch 266/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.2907e+02\n",
      "\n",
      "Test on train set: loss= 0.5670247077941895 acc= 0.9545160531997681 auc= 0.9681474770653615\n",
      "Test on valid set: loss= 6.120363712310791 acc= 0.5999703407287598 auc= 0.8597777777777779\n",
      "\n",
      "Epoch 267/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -2.1556e+02\n",
      "\n",
      "Test on train set: loss= 0.6205541491508484 acc= 0.9508740901947021 auc= 0.9629083183230758\n",
      "Test on valid set: loss= 6.430628776550293 acc= 0.5802536010742188 auc= 0.8302222222222222\n",
      "\n",
      "Epoch 268/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9872e+02\n",
      "\n",
      "Test on train set: loss= 0.5788226127624512 acc= 0.9549207091331482 auc= 0.9661964859742673\n",
      "Test on valid set: loss= 6.098884105682373 acc= 0.6196673512458801 auc= 0.8353333333333334\n",
      "\n",
      "Epoch 269/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.9144e+02\n",
      "\n",
      "Test on train set: loss= 0.5373019576072693 acc= 0.9602622389793396 auc= 0.9704356203388185\n",
      "Test on valid set: loss= 5.946278095245361 acc= 0.6119897365570068 auc= 0.8426666666666666\n",
      "\n",
      "Epoch 270/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8751e+02\n",
      "\n",
      "Test on train set: loss= 0.5313995480537415 acc= 0.9612334370613098 auc= 0.9639217665438096\n",
      "Test on valid set: loss= 5.84628438949585 acc= 0.5918478965759277 auc= 0.8197777777777778\n",
      "\n",
      "Epoch 271/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.8799e+02\n",
      "\n",
      "Test on train set: loss= 0.5223033428192139 acc= 0.9610715508460999 auc= 0.9682109715760069\n",
      "Test on valid set: loss= 5.846478462219238 acc= 0.6139279007911682 auc= 0.8224444444444444\n",
      "\n",
      "Epoch 272/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8229e+02\n",
      "\n",
      "Test on train set: loss= 0.4657275974750519 acc= 0.9633376598358154 auc= 0.9678410925963794\n",
      "Test on valid set: loss= 5.75663948059082 acc= 0.6399942636489868 auc= 0.8402222222222221\n",
      "\n",
      "Epoch 273/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.7098e+02\n",
      "\n",
      "Test on train set: loss= 0.49399009346961975 acc= 0.9631757736206055 auc= 0.9680637307718152\n",
      "Test on valid set: loss= 5.972429275512695 acc= 0.6194316148757935 auc= 0.8364444444444444\n",
      "\n",
      "Epoch 274/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.6448e+02\n",
      "\n",
      "Test on train set: loss= 0.47509798407554626 acc= 0.9633376598358154 auc= 0.9740925127449129\n",
      "Test on valid set: loss= 4.869582653045654 acc= 0.6835075616836548 auc= 0.8655555555555555\n",
      "\n",
      "Epoch 275/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8355e+02\n",
      "\n",
      "Test on train set: loss= 0.517691969871521 acc= 0.96131432056427 auc= 0.9747861436650345\n",
      "Test on valid set: loss= 5.4801926612854 acc= 0.6599593758583069 auc= 0.8397777777777777\n",
      "\n",
      "Epoch 276/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8740e+02\n",
      "\n",
      "Test on train set: loss= 0.4774829149246216 acc= 0.9647135138511658 auc= 0.9714344369851233\n",
      "Test on valid set: loss= 5.480536460876465 acc= 0.6596190929412842 auc= 0.8302222222222223\n",
      "\n",
      "Epoch 277/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8928e+02\n",
      "\n",
      "Test on train set: loss= 0.4723317623138428 acc= 0.9637423157691956 auc= 0.9709222187629093\n",
      "Test on valid set: loss= 4.933055877685547 acc= 0.6662635207176208 auc= 0.8315555555555555\n",
      "\n",
      "Epoch 278/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8558e+02\n",
      "\n",
      "Test on train set: loss= 0.47586289048194885 acc= 0.9639850854873657 auc= 0.9692758200366047\n",
      "Test on valid set: loss= 4.836676597595215 acc= 0.6814066767692566 auc= 0.8442222222222222\n",
      "\n",
      "Epoch 279/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.7707e+02\n",
      "\n",
      "Test on train set: loss= 0.5041512846946716 acc= 0.9635804295539856 auc= 0.9740642211703292\n",
      "Test on valid set: loss= 4.89897346496582 acc= 0.6711341738700867 auc= 0.8415555555555556\n",
      "\n",
      "Epoch 280/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8473e+02\n",
      "\n",
      "Test on train set: loss= 0.4782424867153168 acc= 0.9643897414207458 auc= 0.9685381482068071\n",
      "Test on valid set: loss= 5.022603511810303 acc= 0.6611913442611694 auc= 0.8422222222222222\n",
      "\n",
      "Epoch 281/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.7511e+02\n",
      "\n",
      "Test on train set: loss= 0.47680941224098206 acc= 0.9633376598358154 auc= 0.9674145891684978\n",
      "Test on valid set: loss= 5.256197929382324 acc= 0.6600006818771362 auc= 0.8460000000000001\n",
      "\n",
      "Epoch 282/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5660e+02\n",
      "\n",
      "Test on train set: loss= 0.4934810698032379 acc= 0.9636613726615906 auc= 0.9741076416906539\n",
      "Test on valid set: loss= 5.144912242889404 acc= 0.6641063094139099 auc= 0.836888888888889\n",
      "\n",
      "Epoch 283/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.7102e+02\n",
      "\n",
      "Test on train set: loss= 0.4649657905101776 acc= 0.9651991128921509 auc= 0.9734135368494744\n",
      "Test on valid set: loss= 4.79210090637207 acc= 0.6999965310096741 auc= 0.8537777777777779\n",
      "\n",
      "Epoch 284/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.6423e+02\n",
      "\n",
      "Test on train set: loss= 0.4788803458213806 acc= 0.9644706845283508 auc= 0.9733309210912324\n",
      "Test on valid set: loss= 4.813042163848877 acc= 0.6999934911727905 auc= 0.8644444444444446\n",
      "\n",
      "Epoch 285/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5583e+02\n",
      "\n",
      "Test on train set: loss= 0.4680585563182831 acc= 0.965684711933136 auc= 0.9761536555488011\n",
      "Test on valid set: loss= 4.837245464324951 acc= 0.6982631087303162 auc= 0.8693333333333335\n",
      "\n",
      "Epoch 286/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.5217e+02\n",
      "\n",
      "Test on train set: loss= 0.48302021622657776 acc= 0.9641469717025757 auc= 0.9778599083188272\n",
      "Test on valid set: loss= 4.835748672485352 acc= 0.6996824741363525 auc= 0.8584444444444446\n",
      "\n",
      "Epoch 287/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5752e+02\n",
      "\n",
      "Test on train set: loss= 0.5047218203544617 acc= 0.9628520607948303 auc= 0.9745868820204675\n",
      "Test on valid set: loss= 5.073873519897461 acc= 0.6800000667572021 auc= 0.860888888888889\n",
      "\n",
      "Epoch 288/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5539e+02\n",
      "\n",
      "Test on train set: loss= 0.45770519971847534 acc= 0.9662511944770813 auc= 0.9763925122914403\n",
      "Test on valid set: loss= 5.1458587646484375 acc= 0.6799999475479126 auc= 0.8473333333333335\n",
      "\n",
      "Epoch 289/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5669e+02\n",
      "\n",
      "Test on train set: loss= 0.48684272170066833 acc= 0.9637423157691956 auc= 0.976379020620891\n",
      "Test on valid set: loss= 5.071386337280273 acc= 0.6800001859664917 auc= 0.8664444444444446\n",
      "\n",
      "Epoch 290/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.7589e+02\n",
      "\n",
      "Test on train set: loss= 0.48598188161849976 acc= 0.9651991128921509 auc= 0.9758554347363795\n",
      "Test on valid set: loss= 4.914888381958008 acc= 0.6803763508796692 auc= 0.8662222222222222\n",
      "\n",
      "Epoch 291/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.8501e+02\n",
      "\n",
      "Test on train set: loss= 0.4936186671257019 acc= 0.9647944569587708 auc= 0.9713697544977447\n",
      "Test on valid set: loss= 5.0963521003723145 acc= 0.6528229713439941 auc= 0.8526666666666667\n",
      "\n",
      "Epoch 292/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.8630e+02\n",
      "\n",
      "Test on train set: loss= 0.4950616955757141 acc= 0.9643088579177856 auc= 0.9653941842789597\n",
      "Test on valid set: loss= 5.804293155670166 acc= 0.6200122833251953 auc= 0.8555555555555557\n",
      "\n",
      "Epoch 293/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5806e+02\n",
      "\n",
      "Test on train set: loss= 0.4841645658016205 acc= 0.9660084247589111 auc= 0.9706056629141372\n",
      "Test on valid set: loss= 4.934229850769043 acc= 0.6760355234146118 auc= 0.8719999999999999\n",
      "\n",
      "Epoch 294/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5751e+02\n",
      "\n",
      "Test on train set: loss= 0.44661837816238403 acc= 0.9685982465744019 auc= 0.9773635860394014\n",
      "Test on valid set: loss= 4.89356803894043 acc= 0.681092381477356 auc= 0.8582222222222222\n",
      "\n",
      "Epoch 295/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5515e+02\n",
      "\n",
      "Test on train set: loss= 0.4492499828338623 acc= 0.9674652218818665 auc= 0.9777040589137647\n",
      "Test on valid set: loss= 4.835435390472412 acc= 0.6999933123588562 auc= 0.8673333333333334\n",
      "\n",
      "Epoch 296/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5378e+02\n",
      "\n",
      "Test on train set: loss= 0.44347572326660156 acc= 0.9677079916000366 auc= 0.9784749291090321\n",
      "Test on valid set: loss= 4.840554714202881 acc= 0.695478081703186 auc= 0.8691111111111111\n",
      "\n",
      "Epoch 297/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5807e+02\n",
      "\n",
      "Test on train set: loss= 0.4442109167575836 acc= 0.9686791896820068 auc= 0.9767936316017833\n",
      "Test on valid set: loss= 4.891056537628174 acc= 0.6786717176437378 auc= 0.8597777777777778\n",
      "\n",
      "Epoch 298/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.6400e+02\n",
      "\n",
      "Test on train set: loss= 0.44955816864967346 acc= 0.9677889347076416 auc= 0.9751596619058563\n",
      "Test on valid set: loss= 5.1044087409973145 acc= 0.679998517036438 auc= 0.8597777777777779\n",
      "\n",
      "Epoch 299/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 3s - loss: -1.5846e+02\n",
      "\n",
      "Test on train set: loss= 0.44838884472846985 acc= 0.9673033356666565 auc= 0.975838467484697\n",
      "Test on valid set: loss= 4.840778827667236 acc= 0.6953054666519165 auc= 0.8577777777777778\n",
      "\n",
      "Epoch 300/300\n",
      "\n",
      "CoSen\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "97/97 - 2s - loss: -1.5753e+02\n",
      "\n",
      "Test on train set: loss= 0.4466595947742462 acc= 0.9678698778152466 auc= 0.974737420987444\n",
      "Test on valid set: loss= 4.8354291915893555 acc= 0.6999996900558472 auc= 0.8608888888888888\n",
      "\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "encoding_matrix = tf.Variable(tf.eye(10), trainable=False)\n",
    "trainGen = train_generator(epochs, batch_size, encoding_matrix)\n",
    "model.set_weights(pretrainedWeights)\n",
    "model.compile(optimizer=copy.deepcopy(optimizer), loss = loss_fn)\n",
    "\n",
    "\n",
    "metric_idx = 3\n",
    "trainMetrics = (softConfusionMatrix_train[metric_idx], \n",
    "                confusionMatrix_train[metric_idx],loss_train[metric_idx], \n",
    "                acc_train[metric_idx], auc_train[metric_idx])\n",
    "\n",
    "validMetrics = (softConfusionMatrix_valid[metric_idx], \n",
    "                loss_valid[metric_idx], acc_valid[metric_idx], auc_valid[metric_idx])\n",
    "\n",
    "adjust_lr = AdjustLR(\n",
    "      schedule = lr_schedule,\n",
    "      base_learning_rate = lr,\n",
    "  )\n",
    "\n",
    "uemgm = UpdateEncodingMatrixAndGetMetrics_CoSen(beginEncodingEpoch, endEncodingEpoch,  \n",
    "                                               codingEnhancementRate, mu,\n",
    "                                               train_x_no_valid, train_y_no_valid,\n",
    "                                               valid_x, valid_y, trainMetrics, \n",
    "                                               validMetrics, 'CoSen')\n",
    "_ = model.fit(trainGen, \n",
    "              initial_epoch = beginEncodingEpoch, \n",
    "              epochs = epochs, verbose = 2,\n",
    "              steps_per_epoch = steps, callbacks=[uemgm,\n",
    "                                             adjust_lr,\n",
    "                                            ])\n",
    "\n",
    "cosenWeights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print and Plot the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9Kf7zhM1xpuX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy of baseline: 0.6437000036239624\n",
      "minority accuracy of baseline: 0.46419999599456785\n",
      "\n",
      "test accuracy of enhencement encoding: 0.6942999958992004\n",
      "minority accuracy of enhencement encoding: 0.5388000011444092\n",
      "\n",
      "test accuracy of reweighting: 0.6883000135421753\n",
      "minority accuracy of reweighting: 0.519599997997284\n",
      "\n",
      "test accuracy of cost-sensitiveness: 0.6686999797821045\n",
      "minority accuracy of cost-sensitiveness: 0.5154000103473664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(labels, preds):\n",
    "    pred_classes = tf.one_hot(tf.argmax(preds, axis=1), 10)\n",
    "    cm=tf.matmul(labels, pred_classes, transpose_a=True)\n",
    "    cm=cm/tf.reduce_sum(cm, axis=1, keepdims=True)\n",
    "    return cm\n",
    "\n",
    "def soft_confusion_matrix(labels, preds):\n",
    "    scm=tf.matmul(labels, preds, transpose_a=True)\n",
    "    scm=scm/tf.reduce_sum(scm, axis=1, keepdims=True)\n",
    "    return scm\n",
    "\n",
    "softConfusionMatrix_test = np.zeros((4, n_class, n_class))\n",
    "confusionMatrix_test = np.zeros((4, n_class, n_class))\n",
    "loss_test=np.zeros((4,))\n",
    "acc_test=np.zeros((4,))\n",
    "auc_test=np.zeros((4,))\n",
    "minority_acc_test=np.zeros((4,))\n",
    "experiment_keywords=['baseline', 'enhencement encoding', 'reweighting', 'cost-sensitiveness']\n",
    "weight_list = [baselineWeights, enhancementWeights, reweightWeights, cosenWeights]\n",
    "\n",
    "n_minority_class = 5\n",
    "minority_class_beging_idx = n_class - n_minority_class\n",
    "\n",
    "\n",
    "for metric_idx in range(4):\n",
    "    model.set_weights(weight_list[metric_idx])\n",
    "    y_h = tf.constant(model.predict(test_x, batch_size=evaluation_batch_size, verbose=0))\n",
    "    y = test_y\n",
    "    softConfusionMatrix_test[metric_idx] = soft_confusion_matrix(y, y_h).numpy()\n",
    "    confusionMatrix_test[metric_idx] = confusion_matrix(y, y_h).numpy()\n",
    "    loss_test[metric_idx] = tf.reduce_mean(loss_fn(y, y_h)).numpy()\n",
    "    acc_test[metric_idx] = tf.reduce_mean(keras.metrics.categorical_accuracy(y, y_h)).numpy()\n",
    "    auc_test[metric_idx] = metrics.roc_auc_score(y.numpy(), y_h.numpy(), multi_class='ovr')\n",
    "    minority_acc_test[metric_idx] = confusionMatrix_test[metric_idx,\n",
    "                                                         minority_class_beging_idx:,\n",
    "                                                         minority_class_beging_idx:].diagonal().mean()\n",
    "\n",
    "\n",
    "for k,a,ma in zip(experiment_keywords,acc_test, minority_acc_test):\n",
    "    print('test accuracy of '+k+':', a)\n",
    "    print('minority accuracy of '+k+':', ma)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Soft-Confusion Matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [20,5])\n",
    "\n",
    "for scm_idx in range(4):\n",
    "    scm = softConfusionMatrix_test[scm_idx] * 100\n",
    "    plt.subplot(1,4,scm_idx+1)\n",
    "    plt.imshow(scm, cmap = 'Blues', vmax = 100, vmin = 0)\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            z = np.around(scm[i, j],2)\n",
    "            c = 'k' if z<50 else 'w'\n",
    "            text = plt.text(j, i, z,\n",
    "                           ha=\"center\", va=\"center\", size = 7.5,\n",
    "                           color = c\n",
    "                           )\n",
    "            \n",
    "    plt.ylabel('Index of True Class')\n",
    "    plt.xlabel('Index of Predicted Class')\n",
    "    plt.title(experiment_keywords[scm_idx])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/xxx/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x324 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [20,4.5])\n",
    "\n",
    "for tsne_idx in range(4):\n",
    "    model.set_weights(weight_list[tsne_idx])\n",
    "    y_h = model.predict(test_x, batch_size=evaluation_batch_size, verbose=0)\n",
    "    predictions = np.argmax(y_h, axis = 1)\n",
    "    ts = TSNE(init= 'pca', learning_rate = 'auto')\n",
    "    results = ts.fit_transform(y_h)\n",
    "    x_min, x_max = np.min(results[:,0]), np.max(results[:,0])\n",
    "    y_min, y_max = np.min(results[:,1]), np.max(results[:,1])\n",
    "    x = (results[:,0] - x_min) / (x_max - x_min)\n",
    "    y = (results[:,1] - y_min) / (y_max - y_min)\n",
    "    plt.subplot(1,4,tsne_idx+1)  \n",
    "    plt.scatter(x, y, c = ['C'+str(p) for p in predictions])\n",
    "    plt.xticks([])        \n",
    "    plt.yticks([])\n",
    "    plt.title(experiment_keywords[tsne_idx])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFA10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "xxx",
   "language": "python",
   "name": "xxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
